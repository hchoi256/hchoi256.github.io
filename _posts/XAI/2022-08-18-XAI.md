---
layout: single
title: "설명 가능한 AI(Explainable AI, XAI)... 왜 중요할까?"
categories: XAI
tag: [XAI]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/xai.png
sidebar:
    nav: "docs"
---

# 배경

딥러닝의 발전에 따라 다양한 분야에서 큰 발전을 거머쥘 수 있었다.

하지만, 세계가 점점 대용량 학습 데이터를 다루기 시작하면서 모델의 구조가 복잡해지고 이해하는 것이 불가능에 가까워졌다.

딥러닝은 **종단간 기계학습 (End-to-end)** 기반으로 입력에서 출력까지 '파이프라인 네트워크' 없이 한 번에 학습을 처리한다.

이러한 맥락에서, 대용량 학습 데이터를 다루기 시작하면서 점점 모델의 구조가 복잡해지고 이해하는 것이 불가능에 가까워졌다.

신경망 모델이 도출하는 결과에 대한 근거, 즉 출력값에 대한 설명의 부재가 딥러닝의 한계점으로 항상 부각되었다 (**Blackbox*).

> [Blackbox](https://github.com/hchoi256/ai-terms)

![image](https://user-images.githubusercontent.com/39285147/185485647-7933c156-635d-4abc-a04c-ce885fd5734f.png)

또한, 딥러닝은 편향성이 존재하는 모델로 진화할 가능성을 배제할 수 없으며, 치명적 오류를 범할 가능성이 있다.

이러한 편향성을 조사하기 위하여 설명 가능한 AI, XAI의 필요성에 세계가 집중하고 있다.

# 설명 가능한 AI(Explainable AI, XAI)
가령, 모델이 **'왜'** 인종과 관련하여 잘못된 예측 결과를 내었고, 이러한 평향성을 고치기 위해서 **'어떻게'** 해야 할지 등을 알아내기 위해 모델의 **결과를 설명할 수 있는** 설명 가능한 인공지능이다.

XAI가 활용되는 대표적인 예시들을 잠시 살펴보자.

## Reliability & Robustness
[*Pascal VOC 2007 classification*]

![image](https://user-images.githubusercontent.com/39285147/185486188-d0b2c5cd-eb74-4459-ad55-c168ea84e648.png)

'Pascal'은 이미지 분류 연구로 쓰인 기법으로, 이미지의 어느 부분을 보고 class를 판별하였는지 빨간색으로 표시한다.

XAI 기법은 Pascal 실험이 이미지에 포함된 '워터마크'로 class를 분류하는 모델/데이터셋의 오류를 색출한다.

자세히 보면, 워터마크 주변에 히트맵(Heatmap)이 강조가 되어있는데, 이것은 XAI 모델이** 분류 결과의 가장 지대한 근거가 된 부분**을 가르키면서 **'설명력'**을 높인다.

## Fairness COMPAS crime prediction (편향성)
[*COMPAS*] 

![image](https://user-images.githubusercontent.com/39285147/185486923-e0bcd4e3-207f-484d-a244-9f945f2431d9.png)

'COMPAS'는 어떤 범죄자를 풀어줬을 때, 그 사람이 다시 범죄를 저질러서 감옥으로 돌아올 확률를 도출해낸다.

XAI 기법은 COMPAS가 False Positive(흑인이 범죄를 저지르지 않았는데도 나중에 재범할 것이라고 예측) 혹은 False Negative(백인이 범죄를 저질렀는데도 나중에 범죄를 저지르지 않을 것이다)라는 잘못된 인식 오류가 발생하는 것을 발견한다.

## XAI in Critical Systems
Self-driving car
- **자동차 사고 원인 판단**하는 것이 중요하다


COVID19 classification 
- **왜 알고리즘이 그런 예측 결과를 냈는지 설명**하는 것은 **신뢰 여부**를 결정한다

# Taxonomy of XAI Methods

> **Interpretability**
>
>> 해석 가능성은 인간이 결정의 원인을 이해할 수 있는 정도, 즉 인간이 모델의 결과를 일관되게 예측할 수 있는 정도입니다. 

> **Explainability**
>
>> 사람이 모델을 쓸 때 그 동작을 이해하고 신뢰할 수 있게 해주는 기계 학습 기술(XAI)이다.

## Local vs. Global
Local: **주어진 특정 데이터**에 대한 예측 결과를 **개별적으로** 설명

Global: **전체 데이터셋**에서 모델의 **전반적인** 행동을 설명

## White box vs. Black box
White box: **모델의 내부 구조를 정확하게 알고 있는 상황**에서 설명을 시도

Black box: **모델의 내부 구조는 모른채**, 단순히 모델의 입력과 출력만 가지고 설명을 시도

## Intrinsic vs. Post hoc
Intrinsic: **모델의 복잡도 훈련 이전**부터, 설명에 용이한 제안을 한 뒤 학습을 시킨 모델을 가지고 설명

Post hoc: **임의 모델 훈련 이후** 이 방법을 적용해서 그 모델의 행동을 설명)

## Model specific vs. Model agnostic
Model specific: **특정 모델 구조**에만 적용 가능 (e.g., CAM requires global average pooling)

Model agnostic: 모델 구조와 무관하게 **어느 모델에도 항시 적용 가능**

> Linear model & Simple Decision Tree
>
>> Global, White box, Intrinsic, Model specific

> Grad CAM
>
>> Local, White box, Post hoc, Model agnostic

이제, 본격적으로 [다음 글](https://hchoi256.github.io/xai/XAI-saliency/)에서 XAI 기법들에 대하여 학습해보자.