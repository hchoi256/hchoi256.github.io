---
layout: single
title: "Meta, Transfer, Multi-task, Continual Learning ì°¨ì´"
categories: LightWeight
tag: [Meta Learning, Transfer Learning, Multi-task Learning, Continual Learning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/ml-thumbnail.jpg
sidebar:
    nav: "docs"
---

****
# Introduction ğŸ™Œ
ëŒ€ë¶€ë¶„ì˜ ì•±ë“¤ì€ ë†’ì€ ì§ˆì˜ Data ìˆ˜ì§‘ ë° ì´ë¥¼ Trainingí•  Computational Powerê°€ ë¶€ì¡±í•˜ì—¬, ì£¼ì–´ì§„ í™˜ê²½ì— ë§ê²Œ Re-Trainí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ë¶€ê°ë˜ê³  ìˆë‹¤.

í•˜ì—¬ ë‹¤ë¥¸ Task ìˆ˜í–‰ì„ ìœ„í•œ AI ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë¡œ ì ì€ Datasetì„ ê°€ì§€ëŠ” ë˜ ë‹¤ë¥¸ Taskë„ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì´ ëŒ€ë‘ë˜ê³  ìˆë‹¤.

í•´ë‹¹ ë°©ì‹ì€ Dataì˜ ì–‘ì´ ì ê³ , HW í•œê³„ë¥¼ íƒ€íŒŒí•˜ëŠ” ì¥ì ì„ ê°–ê³  ìˆë‹¤.

****
# Multi-task Learning âœ
- ê° task ë³„ ìµœì ì˜ í”¼ë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ëŠ” í•˜ë‚˜ì˜ ê±°ëŒ€ ëª¨ë¸
- ìƒˆë¡œìš´ datasetì´ ë“¤ì–´ì˜¤ë©´ ê°€ì¥ ì í•©í•œ task ì°¾ê¸° ìœ„í•œ í•™ìŠµ

****
# Meta Learning ğŸ˜œ
- Few-shot Learning
- *Learning-to-Learn*
- ê¸°í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ì „ì´í•™ìŠµë³´ë‹¤ ë” ì ì€ datasetì—ì„œ ë¹ ë¥´ê²Œ ìµœì í™”í•˜ëŠ” generalization ë°©ì‹
    - `Model-based Approach`
    - `Metric-based model`: ì €ì°¨ì› ê³µê°„ì— ìƒˆë¡œìš´ ë°ì´í„° ë§µí•‘í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ classë¡œ ë¶„ë¥˜
    - `Optimization-based Approach`: ë‹¤ìˆ˜ taskì˜ generalized model í”¼ë¼ë¯¸í„°ë¥¼ ìƒˆë¡œìš´ task ëª¨ë¸ì˜ ì´ˆê¸° í”¼ë¼ë¯¸í„° ê°’ìœ¼ë¡œ ì´ìš© $$\rightarrow$$ ìµœì  í”¼ë¼ë¯¸í„° ë” ë¹ ë¥´ê²Œ ê²€ìƒ‰.
        - `Model-Agnostic Meta-Learning (MAML)`: ë¶„ë¥˜ ì´ì™¸ ê°•í™”í•™ìŠµ ë“± ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ì ìš© ê°€ëŠ¥

## Model-Agnostic Meta-Learning (MAML)
$$Model-Agnostic (ëª¨ë¸ê³¼\ ìƒê´€ì—†ì´)$$

![image](https://user-images.githubusercontent.com/39285147/218943327-ec845b73-171e-47b4-8853-797429e38793.png)

![image](https://user-images.githubusercontent.com/39285147/218943056-ebad0c8d-ed9f-4cc1-bdba-30b93f9440b6.png)

ìƒê¸° ê·¸ë¦¼ì—ì„œ $$\theta$$ê°€ ê°€ë¦¬í‚¤ëŠ” pointëŠ” Task 1, 2, 3ì— ëŒ€í•œ ìµœì ì€ ì•„ë‹ˆë‹¤.

í•˜ì§€ë§Œ, Task 1, 2, 3 ì–´ëŠ ê³³ìœ¼ë¡œë“  ë¹ ë¥´ê²Œ ì´ë™í•  ìˆ˜ ìˆëŠ” pointì´ê¸° ë•Œë¬¸ì— $$\theta$$ëŠ” ìµœìƒì˜ ì´ˆê¸° ì‹œì‘ì ì¼ ê²ƒì´ë‹¤.

ê·¸ ì‹œì‘ì ì€ ë‹¤ìˆ˜ taskë¡œë¶€í„° í‰ê· ì ìœ¼ë¡œ 2íšŒì˜ Gradient Descentë§Œ ë°œìƒí•˜ë©´ ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ê±°ë¦¬ì— ìˆë„ë¡ ì„¤ì •í•œë‹¤.

ì´í›„, ìƒˆë¡œìš´ Taskì— ë§ëŠ” ìµœì ì˜ $$\theta^*$$ë¥¼ ì°¾ì•„ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ Gradient Descentë¥¼ ì§„í–‰í•œë‹¤.

ë”°ë¼ì„œ, ìƒˆë¡œìš´ taskê°€ ì ì€ datasetì— ëŒ€í•˜ì—¬ ìˆ˜í–‰ë˜ì–´ë„ ìš°ë¦¬ëŠ” ìµœì ì˜ ì‹œì‘ì  $$\theta^*$$ì—ì„œë¶€í„° ì‹œì‘í•˜ê¸° ë•Œë¬¸ì•  few-shot learningì— ì„±ê³µí•œë‹¤.

****
# Transfer Learning ğŸ¥°
- Few-shot Learning
- ë‹¤ë¥¸ task ê¸°í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ, ë‹¤ë¥¸ task ìˆ˜í–‰í•˜ëŠ” ì ì€ dataset ê¸°ë°˜ fine-tuning ì•Œê³ ë¦¬ì¦˜ ì ìš©

## Catastrophic Forgetting
ê¸°í•™ìŠµ ëª¨ë¸ì—ì„œ ì–»ì€ weightë“¤ì´ ì–´ë–¤ Correlationì´ ìˆëŠ”ì§€ ëª¨ë¥´ëŠ” Deep Learningì—ì„œ ìƒˆë¡œìš´ Taskë¥¼ ë°°ìš°ëŠ”ë° ì •í™•í•˜ê²Œ Fine Tuningì„ ìœ„í•´ weightë¥¼ ì„£ë¶€ë¥´ê²Œ ë°”ê¾¸ë©´, ê¸°ì¡´ Task ì •ë³´ì™€ ê´€ë ¨ëœ weight, ì¦‰ ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ëŠ” í˜„ìƒì´ë‹¤.

## Knowledge Distillation
- ê°™ì€ task ê¸°í•™ìŠµ ëª¨ë¸ì—ì„œ, ë” ì ì€ dataset ê³µìœ í•˜ëŠ” ì‘ì€ ëª¨ë¸ë¡œ ì§€ì‹(ê°€ì¤‘ì¹˜) ì „ë‹¬

****
# Continual Learning ğŸŒ·
ì „ì´í•™ìŠµì˜ ê³ ì§ˆë³‘ **Catastrophic forgetting (Semantic Draft)** ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‚˜ì˜¨ ì•Œê³ ë¦¬ì¦˜

## (1) Elastic Weight Consoliation (EWC)
Fine Tuningìœ¼ë¡œ weightë¥¼ ì„£ë¶€ë¥´ê²Œ ê±´ë“œë¦¬ë©´, ê¸°í•™ìŠµ ëª¨ë¸ì´ í•™ìŠµí•œ Task ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ê²Œ ëœë‹¤.

í•˜ì—¬ ìƒˆë¡œìš´ taskì— ëŒ€í•˜ì—¬ weightë¥¼ ì¡°ê¸ˆì”© ê±´ë“œë ¤ë³´ìëŠ” ë°©ì‹ì´ë‹¤.

ê¸°í•™ìŠµ ëª¨ë¸ì˜ weight ì¤‘ì— ì¤‘ìš”í•œ weightì—ëŠ” Regularization Termì„ ì¶”ê°€í•´ì„œ ì‚´ì§ë§Œ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/218947973-02ffeb4c-9930-4dbf-9983-d0c017b92d15.png)

![image](https://user-images.githubusercontent.com/39285147/218950098-5407ffd1-005f-4dc7-ae79-2fdae1c1223c.png)

$$F_i$$ë¼ëŠ” í•¨ìˆ˜ëŠ” Fisher Information Matrixë¡œ ì–´ë–¤ Random Variableì˜ ê´€ì¸¡ê°’ìœ¼ë¡œë¶€í„°, ë¶„í¬ì˜ parameterì— ëŒ€í•´ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì´ë‹¤.

`L2 Reularization`ê³¼ `No Penalty ì•Œê³ ë¦¬ì¦˜`ì—ì„œ, ê¸°ì¡´ Task A (no penalty, íŒŒë€ìƒ‰)ê°€ ì ì€ Errorë¥¼ ê°–ëŠ” êµ¬ê°„ì„ ë²—ì–´ë‚˜ë²„ë¦¬ì§€ë§Œ, EWCëŠ” ê·¸ ì¤‘ê°„ ì§€ì ì„ êµë¬˜í•˜ê²Œ ì˜ ì°¾ì•„ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ë‹¤ë¥´ê²Œ ë§í•˜ë©´, Task Aì™€ Bì— ëª¨ë‘ë¥¼ ê³ ë ¤í•œ ìµœì ì˜ weightë¥¼ ì°¾ëŠ” ë°©ì‹ì´ë‹¤.

## (2) Dynamically expandable networks (DEN)
![image](https://user-images.githubusercontent.com/39285147/218950462-633f040a-1713-4454-bddf-35a7ee8c8a33.png)

ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í’€ë ¤ë©´, Neural Networkì˜ Capacityê°€ ì¦ê°€í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, Node ìˆ˜ê°€ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ì‹ì— ëŒ€í•œ Appraochì´ë‹¤.

DENì€ ë™ì ìœ¼ë¡œ Node ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ, ìƒˆë¡œìš´ Taskì— ì ì‘í•´ë‚˜ê°€ëŠ” ë°©ì‹ì´ë‹¤.

í•˜ê¸° 3ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ processë¥¼ í†µí•´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.

> ê° Processì— ë”°ë¥¸ loss functionì€ (2)[https://arxiv.org/abs/1708.01547]ì— ì˜ ì •ë¦¬ë˜ì–´ ìˆë‹¤.

### (1) Selective Re-training
Re-trainingì„ í•  ì£¼ìš”í•œ weightë¥¼ ì„ ë³„í•´ì„œ updateí•˜ìëŠ” ë°©ì‹ì´ë‹¤.

Catastrophic forgetting í˜„ìƒì„ ë°©ì§€í•˜ê³ ì, ê¸°ì¡´ì˜ weightë“¤ì„ ì €ì¥í–ˆë‹¤ê°€ ì´í›„ ë‹¤ì‹œ ì¬í™œìš©í•œë‹¤ (Split and Duplication).

### (2) Dynamic Expansion
ì¤‘ìš”í•œ weightë§Œìœ¼ë¡œëŠ” Target Taskì— ëŒ€í•œ ì¶©ë¶„í•œ ì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.

Modelì˜ Capacityê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì—, ë…¸ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•˜ëŠ” ìƒí™©ì„ ë§í•œë‹¤.

### (3) Split and Duplication
Catastrophic forgetting í˜„ìƒ ë°©ì§€ ì°¨ì›ì—ì„œ ê¸°ì¡´ì˜ weightê°€ Threshold ì´ìƒìœ¼ë¡œ ë°”ë€Œë©´ ë³µì‚¬í•œ ê¸°ì¡´ì˜ weightë¥¼ ì˜†ì— ë¶™ì—¬ë„£ëŠ”ë‹¤.

1ë‹¨ê³„ì—ì„œ updateí•  weightë¥¼ ì ì ˆíˆ ë½‘ì§€ ì•Šì•˜ìœ¼ë©´, 3ë²ˆì—ì„œ ë³µì‚¬í•´ì„œ ì¶”ê°€ë  ë…¸ë“œê°€ ê³¼ë‹¤í•˜ì—¬ ë¹„íš¨ìœ¨ì ì´ê±°ë‚˜, í˜¹ì€ Catastrophic forgetting í˜„ìƒì´ ì¬ë°œìƒí•œë‹¤. 

****
# Reference ğŸ’•
[1] J. Kirkpatrick, et al., â€œOvercoming catastrophic forgetting in neural networks,â€ Proc. Nat. Acad. Sci., vol. 114, no. 13, pp. 3521â€“3526, 2017.

[2] Yoon, J., Yang, E., Lee, J., Hwang, S.J. "Lifelong learning with dynamically expandable networks", ICLR, 2017