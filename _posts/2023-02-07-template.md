---
layout: single
title: "[논문분석] "
categories: Others
tag: [Model Compression, Quantization, Pruning]
toc: true
toc_sticky: true
toc_label: "쭌스log"
author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[논문링크]()

<span style="color:blue"> ???? </span>

****
# 한줄요약 ✔

****
# Preliminaries 🍱

****
# Problem Definition ✏
                Given a large pre-trained language model

                Return a quantized model

                Such that it outperforms the performance of the original model in terms of inference time while retaining accuracy.

****
# Challenges and Main Idea💣
**C1)** <span style="color:orange"> </span>

**C2)** <span style="color:orange"> </span>

**C3)** <span style="color:orange"> </span>

**Idea)** <span style="color:lightgreen"> </span>

****
# Proposed Method 🧿

****
# Experiment 👀

****
# Open Reivew 💗

****
# Discussion 🍟

****
# Major Takeaways 😃

****
# Conclusion ✨
## Strength
## Weakness

****
# Reference