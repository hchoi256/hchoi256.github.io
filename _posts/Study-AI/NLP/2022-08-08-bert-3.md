---
layout: single
title: "ConvS2S, Self-Attention, Multi-head Attention"
categories: NLP
tag: [NLP, ConvS2S, Self-Attention, Multi-head Attention, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# Convolution Seq2Seq (ConvS2S)
![image](https://user-images.githubusercontent.com/39285147/187821144-8be0249a-3eac-47cd-a208-cbe533c0c7f6.png)

## Encoder
![image](https://user-images.githubusercontent.com/39285147/186263776-3dfac63c-5ea1-4736-8e19-52dbbd40809a.png)

- *Residual Connection*: 입력 정보가 가중치 합(weight sum) 연산에 적용되고자 지름길 route를 만든다. 다음 레이어에 전이시켜 최종 출력에 영향을 줄 수 있도록 하기 위함.

> [ResNet](https://github.com/hchoi256/ai-terms/blob/main/README.md) 알아보기.

기존 RNN의 '직렬화'라는 한계점을 타파하고자 **RNN 구조를 제거**하고 몇 개의 단어 토큰으로 'Convolution'을 적용한 경우이다.

> RNN 확장 모델 LSTM으로도 무한히 많이 쌓이는 데이터 처리에 대한 RNN의 근본적인 한계점을 해결하지는 못한다.

![image](https://user-images.githubusercontent.com/39285147/186263248-1acb5a60-ae13-4df1-a1ba-cbe0d47fe9cc.png)

- Convolution Filter 개수: 순환 신경망의 레이어 간 연결선을 제거하여 **이전 출력을 기다릴 필요없이**, 상기 사진에서는 가령 3개의 단어 토큰 단위로 새로운 단어 벡터를 인풋으로 활용한다.
- 빨간색 평행사변형은 'go', 'to', 'the school' 세 단어가 하나의 새로운 벡터로 변환되는 모습을 보여준다.

![image](https://user-images.githubusercontent.com/39285147/187803235-9c85f564-987d-489d-a40c-901600767c9c.png)

이러한 아키텍쳐를 구현함으로써 질적으로도 뛰어날 뿐 아니라, 계산의 **병렬화**를 가능하게 하여 **훈련 시간의 단축**이라는 쾌거를 이루어낼 수 있었다.
- 기존 직렬화 방식은 'I' --> 'go' --> 'to' --> 'the school' 식으로 학습하면서 최종적으로 이전 단어 맥락을 전부 고려하는 context vector를 만든다.

![image](https://user-images.githubusercontent.com/39285147/187803537-dee60d19-98e6-4caa-979f-f3a9be6d62e2.png)

상기 사진에서, ①은 Attention Weight를 구할 때 사용되는 부분으로, Softmax 이후 Attention Weights가 도출되는 모습이다.

또한, ②은 ①에서 얻은 Attention Weights에 각 단어 토큰에 대한 지름길 route를 통하여 가중합을 구할 때 사용된다.

## Decoder
하나의 time step에서 디코딩을 할 때, 이전 단계의 decoder에서 얹어준 hidden state vector, 생성 단어 토큰, 그리고 인코더에서 계산된 attention weights를 입력으로써 활용한다.

![image](https://user-images.githubusercontent.com/39285147/187806286-f4f722e6-4e15-4811-8ff4-d6c7de7a2212.png)

- Decoder들의 hiddens state vector에 인코더처럼 합성곱 연산을 취한다 ![image](https://user-images.githubusercontent.com/39285147/187806394-b73d772a-50bb-4c75-a3e8-2b1c11866f39.png)
- *노란색 박스*: 가중치가 가장 높은 단어 토큰을 의미한다 ![image](https://user-images.githubusercontent.com/39285147/187806363-3583876e-8617-47d7-ad40-6814263c501f.png)
- 인코더에서 계산된 출력값 (*encoding vectors*) ![image](https://user-images.githubusercontent.com/39285147/187806450-19811752-ff33-478e-8505-7ab676c389cd.png)
- 현 time step 디코더 hidden state vector 가중치 ![image](https://user-images.githubusercontent.com/39285147/187806528-3e392bcf-7d38-48d2-92ab-2b643c8fc96e.png)
- 현 time step 단어 토큰을 대표하는 hidden state vector ![image](https://user-images.githubusercontent.com/39285147/187806336-ac45f014-9b1d-4026-968a-cea3e6a5b355.png)
- 인코더처럼 Concat하는 것이 아니라 Sum한다 ![image](https://user-images.githubusercontent.com/39285147/187806574-bbe103a9-4435-4e34-a57e-edd488431423.png)

## 한계점
문장의 처음 혹은 끝에 나오는 하나의 단어 토큰을 전체 맥락 상에서 구별하지 못한다 (Convolution 그룹 단위로 관찰하기 때문이다).
- 반면에, RNN 기반은 정보의 소실은 있으나 어느 정도 전체 문장을 감안할 수 있다.

**단어 토큰의 순서를 부여하는 *Transformer-positional Encoding*** 방법으로 개선이 가능하다.

## *Transformer-positional Encoding*
![image](https://user-images.githubusercontent.com/39285147/187990527-8aa968f4-0a9c-4363-acb8-0784a3388fd7.png)
![image](https://user-images.githubusercontent.com/39285147/187994912-e6193a02-f614-4297-b5b2-768b5fc5c7b3.png)

RNN 구조가 아닌 ConvS2S 같은 모델의 Embedding에서 각 차원(단어 토큰)의 순서를 보장하는 방법론이다.

$$e=(w_1+p_1,...,w_m+p_m$$에서 $$p_m$$을 각 단어 토큰에 더함으로써, 단어의 순서 파악이 가능하게 된다.

![image](https://user-images.githubusercontent.com/39285147/187995132-589c6718-a315-4357-8652-f9e16bf7b7d2.png)
- BLEU (blue score): ![image](https://user-images.githubusercontent.com/39285147/188000011-bed8e2b6-1231-49ce-915f-d5895715a136.png)
- PPL (perplexity)

****
# Self-attention 모델
![image](https://user-images.githubusercontent.com/39285147/185520452-e27a130d-510f-4d3a-a12d-adea5378a164.png)

RNN의 시계열 학습에서 벗어나 Attention 구조만으로 학습에 임한다.

Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 **input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식**으로 동작한다.

![image](https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png)

**인코더 내부에서 인코더 한 토큰을 재구성**하는 개념이다.
- **Query, Key, Value가 동일한 시퀀스**를 가진다.

![image](https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png)

- Score = Query와 Key 내적

최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.

이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.

![image](https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png)

****
# Multi-head Attention 모델
하나의 Attention 모델은 각 단어 토큰의 가중치만을 도출하기에, 다른 단어와 어떻게 연관되어있는지 잘 훈련이 안될수도 있다.

따라서 여러개의 어텐션, 즉 Multi-head Attention을 이용하여 이를 하나로 이어붙여서 연관성을 탐구한다.

여러 번의 Self-attention을 **병렬처리**로 수행하여 **빠른 속도**를 끌어낸 것이 바로 **multi-head attention**이다.

****
# Transformer 모델
Multi-head attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행하며, 이것이 바로 BERT에 탑재된 기술인 Transformer이다.

seq2seq의 구조인 '**인코더-디코더**'를 따르면서도, 어텐션(Attention)만으로 구현한 모델이기 때문에 **RNN을 사용하지 않는다**.

또한, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여준다.

이제, [BERT와 KoBERT 실습](https://hchoi256.github.io/nlp/bert-4/)을 수행해보자.

# Reference
[Deep Learning for NLP - KAIST 주재걸 교수님](https://www.youtube.com/watch?v=JqkfT1s60cI&list=PLep-kTP3NkcOjOS1a30UNW-tH2FSoGYfg&index=1)