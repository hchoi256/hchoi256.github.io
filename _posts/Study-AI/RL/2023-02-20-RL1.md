---
layout: single
title: "[강화학습] Q-Learning 이해하기"
categories: RL
tag: [Reinforcement Learning, Q-Learning]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/rl.png
sidebar:
    nav: "docs"
---

****
# 한줄요약 ✔

****
# Introduction 🙌
![image](https://user-images.githubusercontent.com/39285147/220178882-8bf22966-234c-4860-a3cb-e57908aef2d5.png)

AlphaGo의 등장 이래 강화학습에 대한 관심이 기하급수적으로 증가했고, 2023년 현재 chatGPT의 등장으로 다양한 산업 분야 전반에서 그 영향력이 어마무시해지고 있다.

Dota라는 게임의 프로그래머를 강화학습 기반 로봇이 이긴 사례 또한 있었다.

![image](https://user-images.githubusercontent.com/39285147/220175123-39773555-453f-4b43-a7cd-b2e18c026ce3.png)

상기 이미지에서, 18분 정도 실패를 거듭하며 스스로 강화학습한 로봇이 어느 골반을 꺾어야 전방으로 나아갈 수 있는지 제대로 학습한 모습이다.

하여 강화학습의 응용 분야는 무궁무진하며, 필자의 관심 분야인 자율주행 연구에도 많이 사용된다.
- 테슬라는 아직 `모방 학습(imitation learning)` 기반으로 자율주행 기술을 연구하지만, 장차 강화학습으로 전환하려는 움직임이다.

## Deep Learning vs Reinforcement Learning
![image](https://user-images.githubusercontent.com/39285147/220175775-7e9cc013-92fa-4cb7-a494-5f758a48ee0b.png)

Deep learning과 강화학습을 뿌리부터 다르다.

Deep learning은 뉴럴 네트워크라는 신경망을 활용해서 주어진 downstream task를 스스로 학습하여 각 노드(특징)마다 최적의 가중치를 발견하는 과정이다.

강화학습은 뉴럴 네트워크 신경망을 구현하지 않고, 보통 `model-free`하게 각 stage별마다 최적의 선택을 하면서 학습하는 **greedy algorithm**이다.

> `Model-free`: 주어진 Environment를 모르는 상태.

그렇다면 RL는 과연 무엇이고, 어떠한 알고리즘들로 greedy 학습을 수행하는 것인지 살펴보자.

## Reinforcement Learning이란
![image](https://user-images.githubusercontent.com/39285147/220176311-13f809d1-edff-481f-909d-7b7944984c98.png)

강화학습의 목표는 환경(environment)과 상호작용하는 임의의 에이전트(agent)를 학습시키는 것이다.

일반적으로 강화학습은 **stage**, **reward**, **action**이라는 세 가지 구성 요소를 가지고 학습을 진행한다.

에이전트는 환경 속에서 상태(state)를 인식하여 행동(action)하며 학습해 나가며, agent가 취한 행동의 응답으로 환경은 양수 혹은 음수 또는 0을 보상(Reward)으로 갖는다.

에이전트의 목표는 초기 상태부터 종료 상태까지 받을 수 있는 보상을 최대화하는 것으로, agent가 좋은 행동을 했을 때는 큰 보상을 주어 그 행동을 강화하고 그렇지 않은 행동을 했을 때는 작은 보상 혹은 음의 보상을 준다.

****
# Q-Learning ✨
Q러닝은 강화학습 기법 가운데 하나로, 알파고 이전부터 존재했던 화석같은 알고리즘입니다.

[*모두를 위한 강화학습 - 김성훈*]

![image](https://user-images.githubusercontent.com/39285147/220176913-c784abe4-32bc-4a50-99ed-e12f910911c5.png)

Q러닝 이해를 위해 Frozen Lake라는 환경(environment)를 하나 가정하려 합니다.

Frozen lake 문제는 빙판 위의 에이전트가 시작 지점에서 출발하여 목적지까지 최적의 경로로 도착해야 하는 문제이다. 

빙판 곳곳에는 구멍(hole)이 있어 구멍을 피해 목적지에 도착해야 한다.

> Agent는 전체 판을 볼 수 없고 현재 상태만 알 수 있으며, 행동을 여러 번 반복하다가 목적지(G)에 도착하든가 구멍에 빠져서 게임이 끝나서야 에이전트는 행동이 잘한 건지 알 수 있다.

### Q-Value
![image](https://user-images.githubusercontent.com/39285147/220177518-2d7fe917-a4b5-4bd1-ae77-90fb8dea06c4.png)

상기 수식에서, agent는 현재 $$s$$라는 상태에서 $$a$$라는 액션을 취하여, $$s'$$ 상태로 이동한다.

$$a$$라는 액션에 대해 agent는 $$r$$ 보상을 얻는다.

![image](https://user-images.githubusercontent.com/39285147/220178912-b1cefe1b-8b1c-4471-aee0-1cc74253d1e6.png)

또한 제자리에서 위-아래 반복이동하여 보상값을 누적하는 무한루프 현상을 타파하고자, `Discount factor`라는 개념을 도입하여 특정 Action을 취했을 때, Episode가 종료되기까지 reward의 총합의 예측값을 계산한다.

> `Discount Factor`: 현재 얻는 보상이 미래에 얻는 보상보다 얼마나 더 중요한지를 의미하는 값.

$$Q(): S \times A \rightarrow R$$

![image](https://user-images.githubusercontent.com/39285147/220179142-41aa3fee-263a-43dd-b9e4-34ef65a7fcf5.png)

- $$t$$: 특정 시점.
- $$\gamma$$: `disconut factor`.

이는 어떤 시간 $$t$$에서 전략 $$\pi$$를 따라 행동 $$a$$를 할 때, 미래의 보상들의 총합의 기대값이다.

## Q Function
![image](https://user-images.githubusercontent.com/39285147/220177121-266bda7a-ac44-4233-9f2c-e8e2db7b6339.png)

Q Function은 **상태-행동 가치 함수**라는 명칭으로도 불리며, 상태(state)와 행동(action)을 주면 이런 상태에서 이런 행동을 하면 얼마큼의 보상을 받을 수 있는지 알려줍니다.

Agent는 그러한 보상값을 최대로 하는 어떤 정책(policy)를 취하는 행동을 하면 된다 (하기 수식 참조).

![image](https://user-images.githubusercontent.com/39285147/220177230-db3e2265-7d92-4ee7-b66d-14323b9cb0ac.png)

- $$\pi$$: 정책.
- $$\pi^*$$: 최적의 정책; Q를 최대롤 하는 action을 취하는 정책이 최적이다.

![image](https://user-images.githubusercontent.com/39285147/220179516-bf083f95-707c-484d-a762-d07601f5278c.png)

알고리즘이 시작되기 전에 Q 함수는 고정된 임의의 값을 갖고, 매 time-step($$t$$)마다 Agent는 행동$$a$$를 선택하게 되고, 보상$$r$$를 받으며 새로운 상태 $$s_{t+1}$$로 전이하면서 Q 값을 갱신한다.

이 알고리즘의 핵심은 이전의 값과 새로운 정보의 가중합(weighted sum)을 이용하는 **Value Iteration Update** 기법이다. 

****
# Reference
[모두를 위한 강화학습 - 김성훈](https://www.facebook.com/groups/TensorFlowKR/permalink/431952443812486/)