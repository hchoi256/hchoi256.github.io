---
layout: single
title: "BERT Language Model - Part 2"
categories: NLP
tag: [NLP, python, BERT, Language Model]
toc: true
toc_sticky: true
toc_label: "GITHUB BLOG JJUNS"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

https://wikidocs.net/24996

# 언어 모델 (Language Model, LM)
'자연어'의 법칙을 컴퓨터로 모사하는 모델로, 다음에 등장할 단어 예측을 수행한다(= '맥락'을 고려한다).

## Markov 확률 모델
![image](https://user-images.githubusercontent.com/39285147/183536291-32897298-797e-4fd8-aac9-4bcc9ef3459e.png)

이전 단어의 형태를 통하여 확률적으로 다음에 나올 단어를 예측한다

가령, 'like' 다음에 'rabbit'이라는 단어가 나타날 확률은 주어진 학습 데이터에 기반하여 **33%**로 나타난다.

## RNN (Recurrent Neural Network) 모델
![image](https://user-images.githubusercontent.com/39285147/183536890-b8d596a2-c3c0-4c90-8193-ac96f8b8cdb0.png)

Markov 체인 모델을 **딥러닝**에 접목하여 확장한 모델이 바로 RNN이다.

현재 state를 결정하기 위하여 이전 state의 hidden layer 정보를 인풋으로 받는다.

이를 통해 **앞선 문맥을 고려한** 최종 출력 vector(Context vector)를 만든다.

보다 자세한 내용은 [여기](https://github.com/hchoi256/ai-terms/blob/main/README.md)를 참조하자.

### Seq2Seq
![image](https://user-images.githubusercontent.com/39285147/183537292-5cfe7c3f-d380-4e0c-aa20-266341ae5d9a.png)

**Encoder layer**: Context vector 획득
**Decoder layer**: Context vecotr 해독/해석

**번역**에 자주 사용되어, 자연어를 인식(encoding)하고 알맞은 언어로 번역(decoding)하여 출력한다.

하지만 sequence가 길어진다면, **초기 단어에 대한 정보를 점차 소실**되는 구조적 한계점과 **중요하지 않은 token의 정보** 또한 최종 출력에 영향을 주는 단점이 존재한다.

## Attention 모델
![image](https://user-images.githubusercontent.com/39285147/183538147-9eb2a2cf-b06c-4994-9a3b-11a4013a6fc8.png)

**특정 중요 단어에 대한 집중도**를 부여해서 모든 token을 고려하는 Seq2Seq의 한계점을 보완한다.

**RNN의 셀 각각이 도출하는 output을 모두 활용**하여 denamic하게 context vector를 생성한다.
- 1. FCL(Fully connected layer)에서 RNN 셀들과 앞선 단어 문맥을 고려한 출력값에 대한 **score를** 계산한다
- 2. softmax 활성화 함수를 사용해서 **attention weight**를 도출한다
- 3. 각 셀에 대한 결과값과 attention weight을 곱한 합계로 context vector를 구한다.
- 4. Context vector를 decoder에 넣는다.
- 5. 최종 output의 분류 결과가 안 좋다면, FCL에서 score를 조정하여 **decoder가 해석하기에 용이한 attention weight를 재계산한다**.
  - Input과 output 사이의 연관성을 가늠해볼 수 있다.

> 흔히, 딥러닝 모델이 내놓은 결과의 원인을 설명할 수 없는([Blackbox](https://github.com/hchoi256/ai-terms/blob/main/README.md)) 경우가 많음에도 불구하고, Attention 모델은 attention weight 덕분에 해당 설명이 가능하다.

하지만, RNN 신경망 구조를 기반으로 하기 때문에 이전 state의 결과를 기다려야 한다는 점에서 **연산 속도가 느리다**는 단점이 존재한다.

## Self-attention 모델
RNN의 시계열 학습에서 벗어나 attention 구조만으로 학습에 임한다.

Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 **input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식**으로 동작한다.

![image](https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png)

- **Query**: 단어에 대한 가중치
- **Key**: 단어가 Query와의 연관성을 나타내는 가중치
- **Value**: 의미에 대한 가중치 (집중할 단어) 

![image](https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png)

최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.

이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.

![image](https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png)

> RNN보다 더 복잡한 상기 과정을 **병렬처리**로 수행하여 **빠른 속도**를 끌어낸 것이 바로 **multi-head attention**이다.

## Transformer 모델
Multi-head attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행하며, 이것이 바로 BERT에 탑재된 기술이다.

> self attention --> multi-head attention --> 하나의 encoder 
