---
layout: single
title: "BERT Language Model - Part 1"
categories: NLP
tag: [NLP, python, BERT, Language Model]
toc: true
toc_sticky: true
toc_label: "GITHUB BLOG JJUNS"
#author_profile: false
header:
    teaser: /assets/images/posts/nlp-thumbnail.jpg
sidebar:
    nav: "docs"
---

# 자연어 처리(Natural Language Processing, NLP)
## 자연어란?
![image](https://user-images.githubusercontent.com/39285147/183526289-0d7a43a8-f329-4ed1-9e0b-31259a32fe6f.png)

- 부호화(Encoding)
- 해독(Decoding)

자연어 처리는 상기 도표에서 컴퓨터가 **텍스트를 해독하는 과정**을 의미한다.

일상에서 사용하는 모든 인간의 언어로, 한국어, 영어와 같은 것들이 예시이다.

> 인공언어: 프로그래밍 언어, etc.

## 자연어 처리의 두 종류
1. **규칙 기반 접근법 (Symbolic approach)**

2. **확률 기반 접근법 (Statistical approach)**
- *TF-IDF*
  - TF(Term frequency): 단어가 문서에 등장한 개수 (TF ↑, 중요단어확률 ↑)
  - DF(Document frequency): 해당 단어가 등장한 문서의 개수 (DF ↑, 중요단어확률 ↓)

## 자연어 처리의 단계
1. **전처리**
- 개행문자, 특수문자, 공백, 중복 표현, 이메일 및 링크, 불용어, 조사 제거
- 띄어쓰기, 문장분리 보정
- 어간추출

2. **Tokenizing**
![image](https://user-images.githubusercontent.com/39285147/183527388-2369aaca-6791-42d0-821b-1e09460d713f.png)

- 어절 tokenizing
- 형태소 tokenizing
- n-gram tokenizing
- **WordPiece tokenizing** (BERT)

3. **Lexical analysis**
- 어휘, 형태소 분석
- 개체명 인식
- 상호 참조

4. **Syntactic analysis**
- 구문 분석

5. **Semantic analysis**
- 의미 분석

## NLP 활용 분야
- 의미 분석 (i.e., 최초의 컴퓨터는 무엇 --> '질문')
- 구문 분석 (i.e., 문법 구문 분석)
- 감성 분석
- 형태소 분석
- 개체명 인식 (i.e., 네이버는 어떤 회사 --> '기관')

## Word embedding
'자연어'에서 특징을 추출하는 방법이다.

### 1. **One-hot encoding** (sparse representation)
![image](https://user-images.githubusercontent.com/39285147/183528088-8343c972-2c27-4f45-812b-3259e39e2151.png)

- 좌표평면 위에 표현된 각 단어가 가지는 **의미 파악이 불가능하다**
- 단어 간 **유사성 비교가 불가능하다**.
- **차원의 저주**

### 2. Word2Vec (dense representation)
![image](https://user-images.githubusercontent.com/39285147/183533843-dba5b35a-e875-48c7-a654-a0bcbe489fc2.png)

- 자연어의 **의미를 벡터 공간에 임베딩**하여, one-hot encoding의 한계점을 해결한다.
  - i.e., *한국 - 서울 + 도쿄 = 일본*
- 한 단어의 **주변 단어들을 통해** 그 단어의 의미 파악이 가능하다 (유사성 有).
- **한정된 자원**으로 표현이 가능하다.
- 주변 단어를 예측하기 때문에 **비지도학습으로 단어의 의미 학습**이 가능하다.

One-hot 벡터들을 인풋으로 받고, 2개의 간단한 hidden layer를 가진 신경망으로 구성되있다.

주변부 단어를 예측하는 방식으로 학습하여 인풋 단어를 올바르게 유츄해낸다.

단어 벡터의 유사성과 semantic 혹은 syntactic analogy를 통하여 Word2Vec 성능을 검증한다.

하지만, 단어의 **subword information(i,e., 서울 vs 서울시)을 무시**하고 **학습에 사용될 충분한 vocabulary가 주어져야 한다**는 한계가 존재한다.

### 3. FastText
![image](https://user-images.githubusercontent.com/39285147/183534835-d42db067-905f-483d-a738-e437e4dc4e78.png)

subword information을 무시하는 Word2Vec 한계를 **n-gram 기법**을 통해 subword를 직접 학습하면서 극복한다 (i.e., assumption에 2-gram을 적용하면 as, ss, su,..).

FastText는 단어를 n-gram으로 분리한 후, 모든 n-gram vector를 합산한 평균을 통해 단어 벡터를 획득한다.

오탈자, OOV (Out of Vocabulary), 그리고 빈도수가 적은 단어에 대한 학습이 용이하다.

## Word embedding 한계점
**동형어, 다의어**(*account: 계좌, 차지하다, etc.*)와 같은 단어에 대한 embedding 성능이 좋지 않다.

주변 단어를 예측하는 방식으로 학습하기 때문에 **'문맥'을 고려할 수 없다**.