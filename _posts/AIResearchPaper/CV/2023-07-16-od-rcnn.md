---
layout: single
title: "[논문 분석] RCNN"
categories: AIPaperCV
tag: [Computer Vision, Object Detection, RCNN]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/od.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/abs/1311.2524)

****
# 한줄요약 ✔
- Region proposals에 CNN을 적용. 
- Labeled data가 부족할 때, 보조작업(auxiliary task)를 supervised pre-training과 뒤를 이은 domain-specific fine-tuning을 통해 상당한 성능 향상.

****
# Introduction 🙌
R-CNN는 Regions with CNN features의 약자로, CNN과 Region proposals를 결합한 모델이다.

****
# Proposed Method 🧿
## R-CNN 구조
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/c8f0b94b-e129-496e-8ee6-0f0bb245c2dd)

1. 인풋 이미지.
2. 약 2,000개의 region proposals를 추출 (**selective search**).
3. Warping the regions to $$227 \times 227$$ (후보영역을 CNN 입력으로 넣기 위한 크기로 조정).
4. CNN을 이용해 각각의 region proposal의 특징 추출.
5. Linear SVMs를 이용해 각각의 label 분류.

## Region Proposals
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/c9c214ab-019c-472e-b156-d99795a89df2)

**Selective search** algorithm을 사용해 각 이미지에서 2,000장의 후보 영역을 찾는다.

### Selective Search
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/5aef69c1-688c-468d-b45c-8860620dfd35)

1. 색상, 질감, 영역크기 등을 이용해 non-objective segmentation을 수행 (사진에서 좌측).
2. Bottom-up 방식으로 small segemented areas들을 합쳐 더 큰 segemented areas들을 만듬 (사진에서 우측).
3. (2)의 작업을 반복하여 최종적으로 2,000개의 region proposal을 생성한다.

## CNN
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/f181f1c4-bdda-4671-81b9-b556d6baa6d9)

- Input: $$227 \times 227$$ 크기의 Region proposals 2,000개 
- Output: 4,096-dimensional feature vector

R-CNN의 CNN 구조는 pre-trained AlexNet를 따릅니다.

이러한 R-CNN의 출력인 4096차원의 특징 벡터는 이미지의 내용을 잘 나타내지만, 이미지 내에 존재하는 객체의 클래스를 구분하는 데는 부족합니다.

이것을 이후 **Domain-specific fine-tuning**을 통해 CNN을 재학습하여 객체의 클래스를 구분하는 작업을 추가 수행합니다.

### Domain-specific fine-tuning
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/70d35e10-80e8-4444-bda2-24bc61de73e2)

1. 인풋 이미지.
2. 2,000장의 region proposals 생성
3. 각 region proposal이 ground-truth box의 IoU(Intersection of Union)와 비교하여 IoU가 0.5보다 큰 경우 positive samples, 0.5보다 작은 경우 negative samples로 나눔.
- Positive sample: 객체가 포함되어 있는 sample.
- Negative sample: 객체가 포함되지 않은 배경 sample.

> sample을 나누면, ground truth만 positive sample로 정의할때 보다 30배 많은 학습데이터를 얻을 수 있어서, 많은 데이터 활용을 통한 overfitting 방지가 가능.

4. 미니 배치 생성 (positive sample 32개 + negative sample 96개 = 128개의 이미지로 구성).
5. 생성한 미니 배치를 이용해 fine-tuning 진행.
- Pre-trained AlexNet의 마지막 softmax layer를 $$N+1$$-way classification 수행하도록 수정.
    - $$N$$: R-CNN 데이터셋에 담긴 객체 종류의 개수.
    - 1을 더하여 배경인지 여부 판단
    - Fine-tuning 단계에서만 $$N+1$$로 수정된 softmax layer 사용.
6. CNN 결과인 피처맵을 Cache에 따로 저장

> IoU
>
>> ![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/02a0a234-a97c-4abb-9891-7cdd8e6af496)

## Class and Box Prediction
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/3331bae2-e8fa-4684-99ac-ed77f50a4c7c)

### Linear SVMs
- Output: (class, confidence score)
    - Confidence score: 해당 region proposal이 객체를 포함할 확률

클래스 분류를 위한 SVM 모델을 학습하는 단계이다.

### Bounding-box Regressor
            Input : 4096 sized feature vector
            Process : Detailed localization by Bounding box regressor
            Output : (K+1) x 4 sized vector

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/dc8effd1-cca5-4147-b03d-ab186852dafe)

Selective search 알고리즘을 통해 얻은 객체의 위치는 부정확할 수 있기 때문에, Bounding box regressor를 사용하여 객체의 위치를 조절한다.

위에 <span style="color:red"> 빨간색 네모 박스 </span>는 예측된 box이고, 검정색은 ground-truth이다.

각 (x,y,w,h) 별로 대응되는 $$d_{(x,y,w,h)}(p)$$ 를 예측값에 각각 곱하여 ground-truth 값으로 **회귀**하도록 학습한다.

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/22dc2ad3-9ea0-4f8b-8bbc-378082304e45)

- $$G$$: ground-truth
- $$P$$: prediction
- $$x,y,w,h$$: x와 y는 영역의 center, w: width, 그리고 h: height.

## Non Maximum Supression
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/1faf4a1f-0c23-4643-abbc-67bb07ea8c8f)

R-CNN을 통해 얻게 되는 2000개의 bounding box를 전부 다 표시할 경우우 하나의 객체에 대해 지나치게 많은 bounding box가 겹친다.

하여 각 객체별 최적의 bounding box를 선택하는 **Non maximum supression** 알고리즘을 적용한다.

1. Linear SVM에서 bounding box별로 지정한 confidence scroe threshold 이하의 box를 우선 제거.
2. 나머지 bounding box를 confidence score에 따라 내림차순 정렬.
3. 살아남은 boxes 중 confidence score가 가장 높은 box를 기준으로 다른 box와의 IoU값을 조사하여, IoU threshold 이상인 box를 모두 제거.
4. 3의 과정을 반복하여 남아있는 box만 선택한다.

****
# Conclusion ✨
## Strength
- VOC 2012 데이터를 기준으로 기존의 방법들(OverFeat)보다 mAP(mean average precision)이 30%이상 향상된 보다 간단하고 확장 가능한 탐지 알고리즘.
- Labeled data가 부족할 때, supervised pre-training과 뒤를 이은 domain-specific fine-tuning(fine-tuned AlexNet)을 통해 상당한 성능 향상.

## Weakness
- 인풋 이미지 하나당 2,000개의 region proposal 추출하기 떄문에 학습 및 추론 속도가 느리다
- 한 네트워크에서 3가지의 모델(network, SVM, Box Regressor)을 사용하기 때문에 구조와 학습 과정이 복잡하고, end-to-end 학습 불가능.  
- Feature caching을 위한 disk storage 필요.

****
# Reference