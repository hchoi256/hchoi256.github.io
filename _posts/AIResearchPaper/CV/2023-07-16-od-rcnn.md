---
layout: single
title: "[논문 분석] RCNN"
categories: AIPaperCV
tag: [Computer Vision, Object Detection, RCNN]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/od.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/abs/1311.2524)

****
# 한줄요약 ✔
- Selective search로 **region proposals 생성 후 CNN 인풋**으로 사용. 
- Labeled data가 부족할 때, 보조작업을 **supervised pre-training**과 뒤를 이은 **domain-specific fine-tuning**을 통해 상당한 성능 향상.

****
# Introduction 🙌
R-CNN 이전에는 전체 이미지에서 특징을 추출하기 위해 sliding window 방식을 사용했다.

이는 객체가 존재하지 않는 불필요한 배경 영역들 또한 학습 대상으로 삼기 때문에 학습 속도가 매우 느린 단점이 존재했다.

하여 R-CNN는 Selective Search 방법으로 전체 이미지에서 **객체가 존재할만한 후보영역(region proposals)들만** 학습 대상으로 삼는다. 

하여 R-CNN은 Regions with CNN features의 약자로, CNN과 Region proposals를 결합한 모델이다.

****
# Proposed Method 🧿
## R-CNN 구조
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/c8f0b94b-e129-496e-8ee6-0f0bb245c2dd)

1. 인풋 이미지.
2. **Selective Search**: 약 2,000개의 region proposals를 추출.
3. **Warping:** 추출한 후보영역을 CNN의 동일한 입력 크기($$227 \times 227$$)로 조정.
4. **CNN:** CNN을 이용해 각각의 region proposal의 특징 추출.
5. **SVM**: Linear SVM에서 각 후보영역의 label 분류.

## Region Proposals
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/c9c214ab-019c-472e-b156-d99795a89df2)

**Selective search**을 사용해 각 이미지에서 2,000장의 후보 영역을 찾는다.
- Python에서는 관련 라이브러리만 import하여 함수를 호출하면 손쉽게 후보영역 추출이 가능하다.

### Selective Search
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/5aef69c1-688c-468d-b45c-8860620dfd35)

Efﬁcient GraphBased Image Segmentation 방법을 사용해서 가장 작은 픽셀 단위부터 초기 후보군을 정하여 주변 후보군과 유사도 비교를 통해 반복적인 병합 과정을 거친다.

1. 색상, 질감, 영역크기 등을 이용해 non-objective segmentation을 수행 (사진에서 좌측).
2. Bottom-up 방식(사진에서 좌측 하단 $$\rightarrow$$ 우측 상단)으로 small segemented areas들을 합치며 더 큰 segemented areas들을 만든다.
3. (2)의 작업을 반복하여 최종적으로 사전에 지정한 2,000개의 region proposal을 생성한다.

> 초기에는 픽셀 단위로 segmentation한 후, 색상 등을 기준으로 픽셀들 간 유사도가 높은 것들을 합친다. 가령, 픽셀별 색상 차이의 유클리드 거리(Euclidean distance)를 계산하고 사전에 지정한 임계값보다 크면 두 픽셀을 합치게 된다.

Selective search 기법은 CNN 네트워크의 학습과 독립적으로 적용되기 때문에, 하나의 네트워크에서 모든 학습이 발생하는 end-to-end 학습의 걸림돌이다.

하여 향후 모델에서는 후보영역 탐색 과정 자체를 학습 대상으로 삼아 네트워크에 내부로 포함시킨다.

## CNN
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/f181f1c4-bdda-4671-81b9-b556d6baa6d9)

- Input: $$227 \times 227$$ 크기의 Region proposals 2,000개 
- Output: 4,096-dimensional feature vector

R-CNN의 CNN 구조는 pre-trained AlexNet를 따릅니다.

본 논문에서는 **Domain-specific fine-tuning** 과정을 통해 CNN 네트워크를 특정 객체 탐지 작업에 더 적합하게 동작하도록 조정하도록 합니다.

이를 통해 CNN 네트워크는 해당 작업에 특화된 특징을 더 잘 추출하도록 파라미터를 조정합니다.

### Domain-specific fine-tuning
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/70d35e10-80e8-4444-bda2-24bc61de73e2)

1. 인풋 이미지.
2. 2,000장의 region proposals 생성
3. 각 region proposal이 ground-truth box의 IoU(Intersection of Union)와 비교하여 IoU가 0.5보다 큰 경우 positive samples, 0.5보다 작은 경우 negative samples로 나눔.
- Positive sample: 객체가 포함되어 있는 sample.
- Negative sample: 객체가 포함되지 않은 배경 sample.
4. 미니 배치 생성 (positive sample 32개 + negative sample 96개 = 128개의 이미지로 구성).
5. 생성한 미니 배치를 이용해 CNN 네트워크(AlexNet)를 fine-tuning.

- Pre-trained AlexNet의 마지막 softmax layer를 $$(N+1)$$-way classification 수행하도록 수정.
    - $$N$$: R-CNN 데이터셋에 담긴 객체 종류의 개수.
    - $$N+1$$: 여기서 $$1$$은 배경인지 여부 판단
    - Fine-tuning 단계에서만 $$N+1$$로 수정된 softmax layer 사용.

6. CNN 결과인 피처맵을 Cache에 따로 저장

> Sample을 나누면, ground truth만 positive sample로 정의할때 보다 30배 많은 학습데이터를 얻을 수 있어서, 많은 데이터 활용에 따른 overfitting 방지가 가능.

> IoU
>
>> ![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/02a0a234-a97c-4abb-9891-7cdd8e6af496)

## Class and Box Prediction
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/3331bae2-e8fa-4684-99ac-ed77f50a4c7c)

### Linear SVMs
- Output: (class, confidence score)
    - Confidence score: 해당 region proposal이 객체를 포함할 확률로 SVM 모델의 최종 sigmoid 출력값이다.

클래스 분류를 위한 SVM 모델을 학습하는 단계이다.

### Bounding-box Regressor
            Input : 4096 sized feature vector
            Process : Detailed localization by Bounding box regressor
            Output : (K+1) x 4 sized vector

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/dc8effd1-cca5-4147-b03d-ab186852dafe)

Selective search 알고리즘을 통해 얻은 객체의 위치는 부정확할 수 있기 때문에, Bounding box regressor를 사용하여 객체의 위치를 조절한다.

위에 <span style="color:red"> 빨간색 네모 박스 </span>는 예측된 box이고, 검정색은 ground-truth이다.

각 (x,y,w,h) 별로 대응되는 $$d_{(x,y,w,h)}(p)$$ 를 예측값에 각각 곱하여 ground-truth 값으로 **회귀**하도록 학습한다.

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/22dc2ad3-9ea0-4f8b-8bbc-378082304e45)

- $$G$$: ground-truth
- $$P$$: prediction
- $$x,y,w,h$$: x와 y는 영역의 center, w: width, 그리고 h: height.

## Non Maximum Supression
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/1faf4a1f-0c23-4643-abbc-67bb07ea8c8f)

R-CNN을 통해 얻게 되는 2000개의 bounding box를 전부 다 표시할 경우우 하나의 객체에 대해 지나치게 많은 bounding box가 겹친다.

하여 각 객체별 최적의 bounding box를 선택하는 **Non maximum supression** 알고리즘을 적용한다.

1. Linear SVM에서 bounding box별로 지정한 confidence scroe threshold 이하의 box를 우선 제거.
2. 나머지 bounding box를 confidence score에 따라 내림차순 정렬.
3. 살아남은 boxes 중 confidence score가 가장 높은 box를 기준으로 다른 box와의 IoU값을 조사하여, IoU threshold 이상인 box를 모두 제거.
4. 3의 과정을 반복하여 남아있는 box만 선택한다.

****
# Conclusion ✨
## Strength
- VOC 2012 데이터를 기준으로 기존의 방법들(OverFeat)보다 mAP(mean average precision)이 30%이상 향상된 보다 간단하고 확장 가능한 탐지 알고리즘.
- Labeled data가 부족할 때, supervised pre-training과 뒤를 이은 domain-specific fine-tuning(fine-tuned AlexNet)을 통해 상당한 성능 향상.

## Weakness
- 인풋 이미지 하나당 2,000개의 region proposal 추출하기 떄문에 학습 및 추론 속도가 느리다
- 한 네트워크에서 3가지의 모델(network, SVM, Box Regressor)을 사용하기 때문에 구조와 학습 과정이 복잡하고, end-to-end 학습 불가능.
    - CNN 파인 튜닝 1회, SVM 학습 1회, box regressor 학습 1회
- Feature caching을 위한 disk storage 필요.

****
# Reference