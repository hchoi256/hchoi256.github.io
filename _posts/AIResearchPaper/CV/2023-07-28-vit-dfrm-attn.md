---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] Vision Transformer With Deformable Attention (CVPR 2022)"
categories: AIPaperCV
tag: [Computer Vision, ViT, Transformer, Deformable Attention]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/vit.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/abs/2201.00520)

<!-- <span style="color:blue"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
- Deformable Convolutionì˜ Offset ê°œë…ì„ ViTì˜ Self-Attentionì— ì ìš©í•œ Deformable Attention Moduleì„ ì œì•ˆ.
- ê¸°ì¡´ Swin Transformer(ICCV`21) ë° Deformable DETR(CoRL`21)ê³¼ ë‹¬ë¦¬ **Query Agnostic**í•œ í˜•íƒœë¡œ Patch ê°œìˆ˜ë³´ë‹¤ ì ì€ Reference Pointsë¥¼ í†µí•´ Attention ì—°ì‚°.
    - Query Agnostic: ëª¨ë“  Queryê°€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ê³ ì •ëœ ê°œìˆ˜ì˜ Reference Pointsë¥¼ ê³µìœ í•˜ì—¬ Offsetì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. 

****
# Preliminaries ğŸ±
## Swin Transformer
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/swin-transformer/)

## Deformable Convolution
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/od-deformable-detr/#deformable-convolution)

## Deformable DETR
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/od-deformable-detr/)

## DeepViT
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/39c66335-4b4a-422c-97be-96b446f61445)

DeepViT(arXiv`21) ë…¼ë¬¸ì—ì„œ ê¸°ì¡´ ViT êµ¬ì¡°ëŠ” CNNê³¼ ë‹¬ë¦¬ ê¹Šì´ê°€ ê¹Šì–´ì ¸ë„ ì„±ëŠ¥ í–¥ìƒì´ ì–´ë µë‹¤ëŠ” ì ì„ ì§€ì í–ˆìŠµë‹ˆë‹¤.

ìƒê¸° ì´ë¯¸ì§€ì—ì„œ<span style="color:red"> ë¹¨ê°„ì </span>ì€ ê°ê¸° ë‹¤ë¥¸ Queryì˜ ìœ„ì¹˜ì´ë©°, ê¶ê·¹ì ìœ¼ë¡œ ê¹Šì´ê°€ ê¹Šì–´ì§€ë©´ ë‹¤ë¥¸ Queryì— ëŒ€í•œ Self-Attention Mapì´ ê±°ì§„ ë™ì¼í•˜ê²Œ ìƒì„±ë˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

ì´ê²ƒì€ Layerê°€ ê¹Šì–´ì ¸ë„ ê±°ì§„ ìœ ì‚¬í•œ Self-Attention Mapì´ í•™ìŠµë˜ëŠ” ë¬¸ì œì (**Attention Collapse**)ì„ ìœ ë°œí•©ë‹ˆë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/de2ef356-afd9-4780-9f32-7e74be5d62de)

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ **Re-Attention êµ¬ì¡°**ë¥¼ ì œì•ˆí•˜ì—¬ Learnable Matrixë¡œ Self-Attention Mapì„ ì„ì–´ì£¼ì–´ ê¹Šì´ê°€ ë” ê¹Šì–´ì ¸ë„ ë‹¤ë¥¸ Self-Attention Mapì´ ìƒì„±í•©ë‹ˆë‹¤.

Learnable MatrixëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¡œ, ì—¬ëŸ¬ Queryë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

ì´ë¡œ ì¸í•´ ê¹Šì´ê°€ ê¹Šì–´ì ¸ë„ ë‹¤ì–‘í•œ Self-Attention Mapì´ ìƒì„±ë˜ì–´ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  ì…ë ¥ ë°ì´í„°(Query)ì˜ ë‹¤ì–‘í•œ ê´€ì ì„ ê³ ë ¤í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŠ” **query-dependent**í•œ íŠ¹ì§• ì¶”ì¶œì„ ê°œì„ í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ê°€ë ¹, ìƒê¸° ì´ë¯¸ì§€ì—ì„œ Block 23 ê¹Šì´ì—ì„œ Self-Attention Mapì€ **Uniform**í•´ì§€ê¸° ì‹œì‘í•˜ì§€ë§Œ, Re-Attentionì—ì„œëŠ” Block 30 ê¹Šì´ ì •ë„ì—ì„œ Uniformí•´ì§€ê¸° ì‹œì‘í•©ë‹ˆë‹¤.

í•˜ë‚˜ DeepViT ë˜í•œ ì–´ëŠ ì •ë„ ê¹Šì´ì—ì„œëŠ” ë‹¤ì‹œê¸ˆ Attention Collapse í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> CNN ê¸°ë°˜ì˜ ê°ì²´íƒì§€ ëª¨ë¸ì€ ê³ ì •ëœ í¬ê¸°ì˜ Kernelë¥¼ ì‚¬ìš©í•œ í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ í° ê°ì²´íƒì§€ ì„±ëŠ¥ì´ ì €ì¡°í•©ë‹ˆë‹¤. </span>

**Idea 1)** <span style="color:lightgreen"> Deformable Attention Moduleì„ í†µí•´ ê°ì²´ì— íŠ¹í™”ëœ ìˆ˜ìš© ì˜ì—­ì„ í˜•ì„±í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤. </span>

**C2)** <span style="color:orange"> DeepViTì—ì„œ ì–¸ê¸‰ëœ Global Attention ê´€ì ì—ì„œ ê²°êµ­ ëª¨ë‘ ë™ì¼í•œ Self-Attention Mapìœ¼ë¡œ ìˆ˜ë ´í•˜ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ Deformable ê°ì²´íƒì§€ ê¸°ë²•ë“¤ì´ ê° Queryë§ˆë‹¤ ë…ë¦½ì ì¸ Reference Pointsë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ì—°ì‚° ì¸¡ë©´ì—ì„œ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. </span>

**Idea 2)** <span style="color:lightgreen"> ëª¨ë“  Queryê°€ ë™ì¼í•œ Reference Pointsë¥¼ í†µí•´ Attention ì—°ì‚°ì„ í•˜ë„ë¡ í•©ë‹ˆë‹¤. </span>

****
# Proposed Method ğŸ§¿

****
# Experiment ğŸ‘€

****
# Open Reivew ğŸ’—

****
# Discussion ğŸŸ

****
# Major Takeaways ğŸ˜ƒ

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference