---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] Vision Transformer With Deformable Attention (CVPR 2022)"
categories: AIPaperCV
tag: [Computer Vision, ViT, Transformer, Deformable Attention]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/vit.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/abs/2201.00520)

<!-- <span style="color:blue"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/eb28ad9a-8050-4057-8c5c-4ff82da474fd)

- Deformable Convolutionì˜ Offset ê°œë…ì„ ViTì˜ Self-Attentionì— ì ìš©í•œ Deformable Attention Moduleì„ ì œì•ˆ.
- ê¸°ì¡´ Swin Transformer(ICCV'21) ë° Deformable DETR(CoRL'21)ê³¼ ë‹¬ë¦¬ **Query Agnostic**í•œ í˜•íƒœë¡œ Patch ê°œìˆ˜ë³´ë‹¤ ì ì€ Reference Pointsë¥¼ í†µí•´ Attention ì—°ì‚°.
    - Query Agnostic: ëª¨ë“  Queryê°€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ê³ ì •ëœ ê°œìˆ˜ì˜ Reference Pointsë¥¼ ê³µìœ í•˜ì—¬ Offsetì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. 

****
# Preliminaries ğŸ±
## Swin Transformer
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/swin-transformer/)

## Deformable Convolution
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/od-deformable-detr/#deformable-convolution)

## Deformable DETR
[ì—¬ê¸°](https://hchoi256.github.io/aipapercv/od-deformable-detr/)

## DeepViT
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/39c66335-4b4a-422c-97be-96b446f61445)

DeepViT(arXiv`21) ë…¼ë¬¸ì—ì„œ ê¸°ì¡´ ViT êµ¬ì¡°ëŠ” CNNê³¼ ë‹¬ë¦¬ ê¹Šì´ê°€ ê¹Šì–´ì ¸ë„ ì„±ëŠ¥ í–¥ìƒì´ ì–´ë µë‹¤ëŠ” ì ì„ ì§€ì í–ˆìŠµë‹ˆë‹¤.

ìƒê¸° ì´ë¯¸ì§€ì—ì„œ<span style="color:red"> ë¹¨ê°„ì </span>ì€ ê°ê¸° ë‹¤ë¥¸ Queryì˜ ìœ„ì¹˜ì´ë©°, ê¶ê·¹ì ìœ¼ë¡œ ê¹Šì´ê°€ ê¹Šì–´ì§€ë©´ ë‹¤ë¥¸ Queryì— ëŒ€í•œ Self-Attention Mapì´ ê±°ì§„ ë™ì¼í•˜ê²Œ ìƒì„±ë˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

ì´ê²ƒì€ Layerê°€ ê¹Šì–´ì ¸ë„ ê±°ì§„ ìœ ì‚¬í•œ Self-Attention Mapì´ í•™ìŠµë˜ëŠ” ë¬¸ì œì (**Attention Collapse**)ì„ ìœ ë°œí•©ë‹ˆë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/de2ef356-afd9-4780-9f32-7e74be5d62de)

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ **Re-Attention êµ¬ì¡°**ë¥¼ ì œì•ˆí•˜ì—¬ Learnable Matrixë¡œ Self-Attention Mapì„ ì„ì–´ì£¼ì–´ ê¹Šì´ê°€ ë” ê¹Šì–´ì ¸ë„ ë‹¤ë¥¸ Self-Attention Mapì´ ìƒì„±í•©ë‹ˆë‹¤.

Learnable MatrixëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¡œ, ì—¬ëŸ¬ Queryë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ì¡°ì ˆí•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

ì´ë¡œ ì¸í•´ ê¹Šì´ê°€ ê¹Šì–´ì ¸ë„ ë‹¤ì–‘í•œ Self-Attention Mapì´ ìƒì„±ë˜ì–´ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  ì…ë ¥ ë°ì´í„°(Query)ì˜ ë‹¤ì–‘í•œ ê´€ì ì„ ê³ ë ¤í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŠ” **query-dependent**í•œ íŠ¹ì§• ì¶”ì¶œì„ ê°œì„ í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ê°€ë ¹, ìƒê¸° ì´ë¯¸ì§€ì—ì„œ Block 23 ê¹Šì´ì—ì„œ Self-Attention Mapì€ **Uniform**í•´ì§€ê¸° ì‹œì‘í•˜ì§€ë§Œ, Re-Attentionì—ì„œëŠ” Block 30 ê¹Šì´ ì •ë„ì—ì„œ Uniformí•´ì§€ê¸° ì‹œì‘í•©ë‹ˆë‹¤.

í•˜ë‚˜ DeepViT ë˜í•œ ì–´ëŠ ì •ë„ ê¹Šì´ì—ì„œëŠ” ë‹¤ì‹œê¸ˆ Attention Collapse í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> CNN ê¸°ë°˜ì˜ ê°ì²´íƒì§€ ëª¨ë¸ì€ ê³ ì •ëœ í¬ê¸°ì˜ Kernelë¥¼ ì‚¬ìš©í•œ í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ í° ê°ì²´íƒì§€ ì„±ëŠ¥ì´ ì €ì¡°í•©ë‹ˆë‹¤. </span>

**Idea 1)** <span style="color:lightgreen"> Deformable Attention Moduleì„ í†µí•´ ê°ì²´ì— íŠ¹í™”ëœ ìˆ˜ìš© ì˜ì—­ì„ í˜•ì„±í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤. </span>

**C2)** <span style="color:orange"> DeepViTì—ì„œ ì–¸ê¸‰ëœ Global Attention ê´€ì ì—ì„œ ê²°êµ­ ëª¨ë‘ ë™ì¼í•œ Self-Attention Mapìœ¼ë¡œ ìˆ˜ë ´í•˜ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ Deformable ê°ì²´íƒì§€ ê¸°ë²•ë“¤ì´ ê° Queryë§ˆë‹¤ ë…ë¦½ì ì¸ Reference Pointsë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ì—°ì‚° ì¸¡ë©´ì—ì„œ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤. </span>

**Idea 2)** <span style="color:lightgreen"> ëª¨ë“  Queryê°€ ë™ì¼í•œ Reference Pointsë¥¼ í†µí•´ Attention ì—°ì‚°ì„ í•˜ë„ë¡ í•©ë‹ˆë‹¤. </span>

****
# Proposed Method ğŸ§¿
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/349dadf7-40b8-4783-af91-ac95ceaf1730)

## Deformable Attention Module
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/11f29c0a-cfa4-46bb-9c8b-53b4d36f251c)

- $$x$$: Input Feature Map.
- $$q$$: Query.
- $$\tilde{k}, \tilde{v}$$: deformed key and value.
- $$\phi$$: sampling function.
- $$W_q,W_k,W_v$$: Query, Key, Value ê°€ì¤‘ì¹˜ í–‰ë ¬.
- Reference Points: í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ Patch ê°œìˆ˜ë³´ë‹¤ ì ë„ë¡ Input Feature Map ìƒì— Uniformí•˜ê²Œ ìƒì„±ë©ë‹ˆë‹¤.
- $$\theta_{offset}$$: offset network.

## ë™ì‘ ê³¼ì •
1) Input Feature Map $$x \in (B,C, H, W)$$ì„ ë§ˆì¹˜ MSHAì²˜ëŸ¼ Channel-Wiseí•˜ê²Œ Group $$G$$ ë‹¨ìœ„ë¡œ $$x \in (B,G \times C, H, W) \rightarrow (B \times G, C, H,W)$$ ë¶„í• í•˜ì—¬ **Deformed Pointsì˜ ë‹¤ì–‘ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤**.

- Headì˜ ê°œìˆ˜ $$M$$ì€ $$G$$ì˜ ë°°ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ê° Groupì´ ë‹¤ìˆ˜ì˜ attention headë¥¼ í†µí•´ ì—°ì‚°ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. 
- ê° Groupë§ˆë‹¤ shared subnetworkë¥¼ í†µí•´ offsetsë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

2) $$x \in (B \times G, C, H,W)$$ì— ëŒ€í•´ **Reference Points $$p \in (B \times G, H_G,W_G,2)$$ë¥¼ Uniform Grid $$((H,W) \rightarrow (H_G,W_G))$$ë¡œ ìƒì„±í•˜ê³  ê° ì¢Œí‘œë¥¼ $$[-1,1]$$ ë²”ìœ„ë¡œ Normalization í•©ë‹ˆë‹¤**.

- $$H_G={H \over r}, W_G={W \over r}$$.
    - $$r$$: a factor.
- Reference Points: $$\{(0,0),...,(H_G-1,W_G-1)\}$$ ì¢Œí‘œë“¤ì„ $$[-1,+1]$$ ë²”ìœ„ë¡œ Normalizationì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - Normalization ì´í›„, $$(-1,-1)$$ ì¢Œí‘œëŠ” top-left cornerì´ ë˜ê²Œ ë©ë‹ˆë‹¤.
    - ê° Reference Point ë§ˆë‹¤ $$2$$ê°œì˜ $$x$$ì¶•ê³¼ $$y$$ì¶• Sampling Offsetì„ ê°€ì§‘ë‹ˆë‹¤.

3) Input Feature Mapì„ $$W_q$$ê³¼ ê³±í•˜ì—¬ **ì¿¼ë¦¬ $$q$$ë¥¼ êµ¬í•©ë‹ˆë‹¤**.

<span style="color:yellow"> $$q=xW_q$$ </span>

4) í•´ë‹¹ ì¿¼ë¦¬ $$q$$ë¥¼ Offset Network $$\theta_{offset}$$ì— ë„£ì–´ì„œ Reference Pointsì— ëŒ€í•œ **Offset Vectorë¥¼ êµ¬í•©ë‹ˆë‹¤**.

<span style="color:yellow">$$\bigtriangleup \textbf{p}=\theta_{offset}(q)$$ </span>

5) í•´ë‹¹ Offset Vectorê³¼ Reference Pointsë“¤ì„ ë”í•˜ì—¬ **Deformed Points(Sampling Points)ë¥¼ êµ¬í•©ë‹ˆë‹¤**.

<span style="color:yellow">$$\tilde{x}=\phi(x;p+\bigtriangleup p)$$ </span>

6) í•´ë‹¹ Deformed Pointsë“¤ì€ ì‹¤ìˆ˜ê°’ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì— **Bilinear Interpolationì„ í†µí•´ ì•Œë§ì€ Sampled Featuresë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤**.

<span style="color:yellow">$$\phi(z;(p_x,p_y))=\Sigma_{r_x,r_y}g(p_x,r_x) \cdot g(p_y,r_y) \cdot z[r_y,r_x,:]$$ </span>

<span style="color:yellow"> $$g(a,b)=max(0,1-\left\vert a-b \right\vert)$$ </span>

- $$r_x,r_y$$: indexes on $$z \in \mathbb{R}^{H \times W \times C}$$.

7) í•´ë‹¹ Sampled Featuresë“¤ì„ ê°ê° $$W_k,W_v$$ì™€ ê³±í•˜ì—¬ Deformed Key $$\tilde{k}$$ì™€ Deformed Value $$\tilde{v}$$ë¥¼ êµ¬í•©ë‹ˆë‹¤.

<span style="color:yellow">$$\tilde{k}=\tilde{x}W_k,\tilde{v}=\tilde{x}W_v$$ </span>

8) Deformed Pointsë¡œ ë¶€í„° Swin Transformerì—ì„œ ì œì‹œëœ **Relative Position Bias Offsetsì„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ êµ¬í•˜ê³ , ìµœì¢… Attention ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤**.

<span style="color:yellow">$$z^m=\sigma({q^m (\tilde{k}^{m})^T \over \sqrt{d}}+\phi(\hat{B};R)W)\tilde{v}^m$$ </span>

- $$z^m$$: $$m$$ë²ˆì§¸ headì˜ Attention ê²°ê³¼.
- $$\phi(\hat{B};R) \in \mathbb{R}^{HW \times H_G W_G}$$: Relative Position Bias Offset.
    - $$G$$: Group.
    - $$\hat{B}$$: Relative Position Bias.

## Offset Network
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/43ce0b3d-86d8-4638-8063-416863e17e6e)

****
# Experiment ğŸ‘€

****
# Open Reivew ğŸ’—

****
# Discussion ğŸŸ

****
# Major Takeaways ğŸ˜ƒ

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference