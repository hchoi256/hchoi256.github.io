---
layout: single
title: "[논문 분석] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
categories: AIPaperCV
tag: [Computer Vision, ViT, Vision Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/vit.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/pdf/2010.11929.pdf)

****
# Summary 📌
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/8204eabe-c472-4f9b-b289-f2c22c8f41b3)

- 이미지를 여러 개의 **flattened patches**으로 나누고, 이를 Transformer 인코더의 인풋으로 활용합니다.
- 기존 CNN 기반 SOTA 모델보다 더 성능이 우수합니다.
- **파라미터 한계 없음**: Transformer의 구조적인 특징으로 더 많은 데이터 및 파라미터를 사용하면 더 좋은 성능을 보여줍니다.
    - <span style="color:orange"> Transformer는 공간 특징을 상대적으로 덜 반영하여 CNN에 비해 inductive biases가 낮으며, 이는 일반화 능력을 저하시켜 적은 데이터셋으로 학습 시 모델 성능이 매우 저조합니다. </span>
    - <span style="color:lightgreen"> 하여 ViT는 대용량 데이터셋으로 학습하여 약한 inductive bias를 보완하고 모델 성능을 끌어내었습니다. </span>

> Inductive bias: 학습 시에는 만나보지 않았던 상황에 대하여 정확한 예측을 하기 위해 사용하는 추가적인 가정 (CNN에서는 비슷한 특징을 가진 픽셀들은 주변에 밀집해있다는 가정).

****
# Introduction 🙌
본 논문 이전, 방대한 데이터셋 처리에도 saturating performance 문제에서 자유로운 Transformer는 NLP 영역에서 각광받아 왔습니다.

> **Training**: 방대한 데이터 corpus를 학습하는 과정입니다.
>
> **Fine-tuning**: downstream task에 대한 작은 데이터셋에서 fine-tuning하는 과정입니다.

하나 CV 분야에서 역시 텍스트가 아닌 이미지의 특징들을 self-attention과 융합하고자 한 노력이 있었으나, 하드웨어 가속기에 알맞은 비율로 scaling 되지 않는 등 여러 가지 문제가 발생하곤 했습니다.

본 논문의 **ViT**는 비전 분야에서 Transformer를 결합한 시초로써 큰 의미를 품고 있으며, 이를 기점으로 비전 분야에서 역시 Transformer 기반 비전 모델들이 급증하기 시작했습니다.

****
# Problem Definition 🧿
- **Given** a 2D image dataset $$\mathcal{D}$$.
- **Produce** a pre-trained Transformer model $$\mathcal{T}$$.
- **Such that** $$\mathcal{T}$$ produces comparable performance over existing models on $$\mathcal{D}$$.

****
# Methodology ✨
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/8204eabe-c472-4f9b-b289-f2c22c8f41b3)

## Backbone
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/52f6540c-a22f-4a36-b8ae-ae1fc862f0dc)

- **Patch**: 인풋 이미지($$x;\ H \times W \times C$$)를 하이퍼 파라미터로 지정된 patch 크기$$(N \times(P \times P \times C) \times D)$$의 이미지들로 분할.
    - $$x^i_pE$$: 각 패치 임베딩.
    - $$P$$: patch size.
    - $$C$$: channel size.
    - $$N$$: patch or sequence 개수.
    - $$D$$: embedding dimension.
- **Position Embedding($$E_{pos}$$)**: 각 patch에 대응되는 Position Embedding 추가.
- **Learnable [class] Embedding $$(x_{class})$$.**: 입력 전체 이미지의 class label을 학습하는 embedding. 

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/3e9bec0d-d004-45c8-b1bd-2aef9fd2ca4b)

하이퍼 파라미터로 지정한 크기의 patch만큼 전체 이미지를 flatten하여 분할하고, 이를 Transformer Encoder의 인풋으로 사용하기 위해 linear projection을 통과시켜 각 patch가 모델의 임베딩 차원의 벡터$$(N,P,P,C,D)$$로 표현되도록 하는 모습입니다.

이와 더불어, 각 patch들이 모여 이루는 전체 이미지의 class를 학습하기 위해, 토큰 번호 $$0$$에 extra learnable class embedding을 추가적으로 concat시킨 후, Position Embedding을 더하여 최종적으로 Encoder의 인풋으로 넣어줍니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/22e89139-2b5c-490c-8468-fa72e3a79d9d)

- $$P_h$$: 패치 세로 길이.
- $$P_w$$: 패치 가로 길이.
- $$C$$: embedding dimension.

가령 상기 이미지에서 분할한 패치들을 flatten 시킨 후, Linear Projection에 통과시켜 Encoder의 인풋으로 사용하기 위해 적절한 임베딩 차원$$(C)$$으로 사영시키는 모습입니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/7236c6dd-6a93-417b-be1f-eed8675ea3f9)

상기 이미지에서 flatten된 패치들에 대해 $$0$$ 인덱스에 class 토큰을 추가하고, Position Embedding을 더해주는 모습입니다.

## Transformer Encoder
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/885bd53b-e9e4-49d8-beef-29fe5d502c6a)

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/fb6105a2-3c24-47fe-86db-291256c670e9)

상기 Transformer Encoder 구조와 수식을 비교해보면 직관적으로 이해가 가능합니다.

여기서, Transformer Encoder 인풋으로 들어가는 입력 시퀀스만 앞선 섹션에서 설명한 것처럼 재정의하고,나머지 동작 원리는 기존 Transformer 인코더와 동일합니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/cfe5353c-6f1f-4a37-b8ce-716814ff3053)

상기 이미지에서는 인코더 입력을 Query, Key, Value의 각 가중치와 곱하여 Query, Key, Value를 생성합니다.

이후, 기존 Transformer과 동일하게 Attention을 수행하여 각 head의 출력을 생성합니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c19cbf4a-99b3-4066-b9b1-069231734de3)

모든 head의 출력을 하나의 차원으로 concat한 후, linear projection을 통과시켜 입출력이 동일한 차원을 내뱉도록 만듭니다.

## MLP
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d3a0ed54-f67c-4c2d-86e1-d70b37976807)

- **특성 변환(Feature Transformation)**: MLP 레이어는 임베딩 벡터에 비선형성을 추가하고, 모델이 입력 데이터의 복잡한 특징을 학습할 수 있도록 변환합니다. 이를 통해 모델은 입력 이미지의 다양한 특징을 잘 이해하고 추상화할 수 있습니다.
- **차원 확장(Dimension Expansion)**: ViT는 처음에는 작은 크기의 패치로부터 임베딩 벡터를 얻습니다. 이러한 임베딩 벡터는 일반적으로 작은 차원을 가지고 있습니다. MLP 레이어는 이러한 임베딩 벡터의 차원을 확장시켜 더 큰 차원의 특성 공간으로 매핑합니다. 이로써 모델은 입력 이미지의 복잡한 패턴을 더 잘 표현할 수 있게 됩니다.

## Observations
ViT를 방대한 데이터셋으로 training하고, 이후 작은 downstream task로 fine-tuning합니다.

이 때 사전 학습된 모델은 일반적으로 대규모 데이터셋에 대해 사전 학습되어 있으며, MLP(prediction head)는 해당 데이터셋의 작업에 맞게 조정되어 있습니다.

하나 파인튜닝 작업은 보다 작은 특정 작업에 맞게 모델을 조정하는 과정이기 때문에, 초기 MLP or FFN(prediction head)를 새로운 작업에 더 적합한 형태로 초기화하는 것이 유용할 수 있습니다.

하여 Fine-tuning할 때, 기존 pre-trained prediction head(MLP)를 제거하고, **이를 $$0$$으로 초기화된 새로운 차원의 FFN$$(D \times K)$$으로 대체합니다**.
- $$K$$: downstream task에서의 class 개수입니다.

또한, ViT는 Fine-tuning시 Pre-training에서 사용한 이미지보다 높은 해상도의 입력 이미지를 사용하면 성능이 더 향상됩니다.
    - $$N \times (P \times P \times C)$$에서 $$N$$이 증가하여 공간적인 context를 더 잘 이해한다.

<span style="color:orange"> 하나 $$N$$ 개수가 늘어날수록, Position embedding의 의미가 모호해지기 때문에 </span>, 이 경우에는 <span style="color:lightgreen"> 2차원 position embedding 사용</span>을 고려해볼 수 있습니다.

****
# Experiment 🗂
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/0f7a4101-3ce0-478e-a7f6-9638259a4c6a)

성능은 학습 데이터가 많을 경우 경쟁모델에 비해 압도적으로 좋으나, 적은 데이터셋을 활용하면 성능이 안 좋은 모습입니다.

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/2607118f-5b88-497f-8731-bdb7f47f0e2f)

위처럼 많은 데이터셋을 활용하여 학습하면, ViT의 embedding filter가 기존 방식인 CNN filter와 동일한 모습입니다.

****
# Reference 🧿
[ViT](https://arxiv.org/pdf/2010.11929.pdf)
