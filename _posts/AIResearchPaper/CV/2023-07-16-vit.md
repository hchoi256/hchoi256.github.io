---
layout: single
title: "[논문 분석] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"
categories: AIPaperCV
tag: [Computer Vision, ViT, Vision Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/vit.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/pdf/2010.11929.pdf)

****
# Summary 📌
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/8204eabe-c472-4f9b-b289-f2c22c8f41b3)

- 이미지를 여러 개의 **flattened patches**으로 나누고, 이를 인코더의 인풋으로 활용합니다.
- 기존 CNN 기반 SOTA 모델보다 더 성능이 우수합니다.
- **파라미터 한계 없음**: Transformer의 구조적인 특징으로 더 많은 데이터 및 파라미터를 사용하면 더 좋은 성능을 보여줍니다.
    - <span style="color:orange"> Transformer는 공간 특징을 상대적으로 덜 반영하여 CNN에 비해 inductive biases가 낮으며, 이는 일반화 능력을 저하시켜 적은 데이터셋으로 학습 시 모델 성능이 매우 저조합니다. </span>
    - <span style="color:lightgreen"> 하여 ViT는 대용량 데이터셋으로 학습하여 약한 inductive bias를 보완하고 모델 성능을 끌어내었습니다. </span>

> Inductive bias: 학습 시에는 만나보지 않았던 상황에 대하여 정확한 예측을 하기 위해 사용하는 추가적인 가정 (CNN에서는 비슷한 특징을 가진 픽셀들은 주변에 밀집해있다는 가정).

****
# Introduction 🙌
그간 방대한 데이터셋 처리에도 saturating performance 문제에서 자유로운 Transformer는 NLP 영역에서 각광받아 왔다.
- Training: 방대한 데이터 corpus 학습
- Fine-tuning: downstream task에 대한 작은 데이터셋에서 fine-tuning.

하지만, CV 분야에서는 self-attention과 융합하고자 한 노력이 있었으나, h/w 가속기에 알맞은 비율로 scaling 되지 않는 등 여러 가지 문제가 발생했다.

하여 당시 CNN 네트워크가 여전히 SOTA를 달성하고 있었다.

****
# Problem Definition 🧿
- **Given** a 2D image dataset $$\mathcal{D}$$.
- **Produce** a pre-trained Transformer model $$\mathcal{T}$$.
- **Such that** $$\mathcal{T}$$ produces comparable performance over existing models on $$\mathcal{D}$$.

****
# Methodology ✨
[*ViT 모델 아키텍처*]

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/8204eabe-c472-4f9b-b289-f2c22c8f41b3)

ViT 구조의 각 단계를 하나하나 뜯어보자.

## Backbone
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/3e9bec0d-d004-45c8-b1bd-2aef9fd2ca4b)

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/52f6540c-a22f-4a36-b8ae-ae1fc862f0dc)

- **Patch**: 인풋 이미지($$x;\ H \times W \times C$$)를 하이퍼 파라미터로 지정된 patch 크기($$N \times(P \times P \times C)$$)의 이미지들로 분할.
    - $$x^i_pE$$: 각 패치.
    - $$P$$: patch size.
    - $$C$$: channel.
    - $$N$$: patch or sequence 개수.
    - $$D$$: embedding dimension.
- **Position Embedding($$E_{pos}$$)**: 각 patch에 대응되는 Position Embedding 추가.
    - 전체 이미지에서 각 patch 위치의 center 좌표값.
- **Learnable [class] Embedding $$(x_{class})$$.**: 각 patch의 class 분류가 아닌, 전체 이미지의 class label을 학습하는 embedding 추가. 

하이퍼 파라미터로 지정한 크기의 patch만큼 전체 이미지를 flatten하여 분할하고, 이를 Transformer Encoder의 인풋으로 사용하기 위해 linear projection을 통과시켜 각 patch가 모델의 임베딩 차원의 벡터로 표현되도록 하는 모습이다.

이와 더불어, 각 patch들이 모여 이루는 전체 이미지의 class를 학습하기 위해, 토큰 번호 $$0$$에 extra learnable class embedding을 추가로 Encoder의 인풋으로 넣어준다.

## Transformer Encoder
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/fb6105a2-3c24-47fe-86db-291256c670e9)

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/885bd53b-e9e4-49d8-beef-29fe5d502c6a)

상기 Transformer Encoder 구조와 수식을 비교해보면 직관적으로 이해가 가능하다.

하여 Transformer Encoder 인풋으로 들어가는 sequence만 알맞게 재정의하고, 나머지 Transformer Encoder의 동작 원리는 상기 이미지와 동일하다.

## Observations
- ViT를 방대한 데이터셋으로 training하고, 이후 작은 downstream task로 fine-tuning한다.
    - fine-tuning할 때, pre-trained prediction head(MLP)를 제거하고 0으로 초기화된 새로운 $$d \times K$$ 차원의 FFN으로 대체한다; 
        - $$K$$: downstream task에서 class 개수.

> **사전 학습 v. 파인튜닝**
>
>> 사전 학습된 모델은 일반적으로 대규모 데이터셋에 대해 사전 학습되어 있으며, MLP(prediction head)는 해당 데이터셋의 작업에 맞게 조정되어 있습니다. 하지만 파인튜닝 작업은 보다 작은 특정 작업에 맞게 모델을 조정하는 것입니다. 따라서, 초기 MLP or FFN(prediction head)를 새로운 작업에 더 적합한 형태로 초기화하는 것이 유용할 수 있습니다.

- ViT는 Fine-tuning시 Pre-training에서 사용한 이미지보다 높은 해상도를 사용하면 성능이 오른다.
    - $$N \times (P \times P \times C)$$에서 $$N$$이 증가하여 공간적인 context를 더 잘 이해한다.
        - $$N$$: patch 개수.
    - $$N$$ 개수가 늘어날수록, Position embedding의 의미가 모호해지기 때문에, 2차원 position embedding 사용을 고려해볼 수 있다.

****
# Experiment 🗂
![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/0f7a4101-3ce0-478e-a7f6-9638259a4c6a)

성능은 학습 데이터가 많을 경우 경쟁모델에 비해 압도적으로 좋으나, 적은 데이터셋을 활용하면 성능이 안 좋다.

![image](https://github.com/hchoi256/ai-boot-camp/assets/39285147/2607118f-5b88-497f-8731-bdb7f47f0e2f)

위처럼 많은 데이터셋을 활용하여 학습하면, ViT의 embedding filter가 기존 방식인 CNN filter와 동일한 모습이다.

****
# Reference 🧿
[ViT](https://arxiv.org/pdf/2010.11929.pdf)
