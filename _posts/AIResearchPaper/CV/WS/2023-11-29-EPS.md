---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation (CVPR 2021)"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, CAM, EPS]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2105.08965.pdf)  </span>

****
# í•œì¤„ìš”ì•½ âœ”
- WSSS ì‚¬ìš©í•  ì‹œ Image-level weak supervisionì˜ í•œê³„.
    - sparse object coverage
    - inaccurate object boundary
    - co-occurring pixels from non-target objects
- **Explicit Pseudo-pixel Supervision (EPS)**: two weak supervisionìœ¼ë¡œ pixel-level feedbackì„ ì–»ëŠ”ë‹¤.
    - `localization map` $$\rightarrow$$ distingusih different objects.
        - *CAM(Class Activation Map)*ìœ¼ë¡œ ìƒì„±.
    - `saliency map` $$\rightarrow$$ rich boundary information.
        - *Saliency detection model*ìœ¼ë¡œ ìƒì„±.
        
****
# Preliminaries ğŸ±
- ì¼ë°˜ì ìœ¼ë¡œ WSSSì˜ ì „ì²´ì ì¸ íŒŒì´í”„ë¼ì¸ì€ **two stage**ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.
    - pseudo-mask ìƒì„± (image classifier ì´ìš©).
    - pseudo-maskë¥¼ GTë¡œ ì‚¬ìš©í•˜ì—¬ ê° iterationë§ˆë‹¤ GT ê°±ì‹ í•˜ë©° recursiveí•˜ê²Œ segmentation model í•™ìŠµ.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> sparse object coverage </span>

**C2)** <span style="color:orange"> inaccurate object boundary </span>

**C3)** <span style="color:orange"> co-occurring pixels from non-target objects </span>

**Idea)** <span style="color:lightgreen"> Explicit Pseudo-pixel Supervision (EPS) ì œì•ˆ </span>

****
# Problem Definition â¤ï¸
Given a weak dataset $$\mathcal{D}$$.

Return a model $$\mathcal{S}$$.

Such that $$\mathcal{S}$$ approximates the performance of its fully-supervised model $$\mathcal{T}$$.

****
# Proposed Method ğŸ§¿
## Model Architecture (EPS)
*[Figure 2]*

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d413f1e3-7e55-4d70-8ff0-a8ae92a01ea2)

- Backgroundë¥¼ í¬í•¨í•´ì„œ $$C+1$$ê°œì˜ classë¡œ ë¶„ë¥˜í•˜ëŠ” classifierë¡œ $$C+1$$ê°œì˜ localization mapì„ ìƒì„±í•´ì„œ saliency mapê³¼ ë¹„êµí•œë‹¤.
- Foreground
    - $$C$$ê°œì˜ localization mapì„ í•©ì³ì„œ foreground mapì„ ìƒì„±í•˜ê³ , ì´ë¥¼ saliency mapê³¼ ë§¤ì¹­ì‹œí‚¨ë‹¤. â‡’ **improving boundaries of objects**.
- Background
    - background localization mapê³¼ saliency mapì˜ ë°”ê¹¥ë¶€ë¶„ $$(1âˆ’M_s)$$ ì„ ë§¤ì¹­ì‹œí‚¨ë‹¤. â‡’ **mitigate the co-occuring pixels of non-target objects**.
        - ê³ ì–‘ì´ì™€ ê°œê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ì´ë¯¸ì§€ë¥¼ ê³ ë ¤í•´ ë´…ì‹œë‹¤. ì—¬ê¸°ì„œ ê³ ì–‘ì´ì™€ ê°œê°€ ëŒ€ìƒ(target)ì´ê³ , ë‹¤ë¥¸ ê°ì²´ë“¤ì€ ëŒ€ìƒì´ ì•„ë‹Œ(non-target) ê°ì²´ì…ë‹ˆë‹¤. ì´ ë•Œ, "co-occurring pixels of non-target objects"ëŠ” ì˜ˆë¥¼ ë“¤ì–´ ë°”ë‹¥, ë²½, ë‚˜ë¬´, ê°€êµ¬ ë“±ì˜ í”½ì…€ë“¤ë¡œ êµ¬ì„±ë  ê²ƒì…ë‹ˆë‹¤. ì´ í”½ì…€ë“¤ì€ ì—¬ëŸ¬ ê°ì²´ë“¤ê³¼ í•¨ê»˜ ë“±ì¥í•˜ë©°, ê° ê°ì²´ì˜ ì¼ë¶€ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ê·¸ ìì²´ë¡œëŠ” íŠ¹ì • ê°ì²´ë¥¼ ë‚˜íƒ€ë‚´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.

## Loss Function
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/366f2eca-cda9-48f4-a849-34f537b9c716)

$$\mathcal{L}_{total}=\mathcal{L}_{sal}+\mathcal{L}_{cls}$$

- $$\mathcal{L}_{sal}={1 \over H \cdot W} \Vert M_s - \hat{M}_s \Vert^2$$.
    - $$M_s$$: the off-the-shelf saliency detection modelâ€“ PFAN [51] trained on DUTS dataset
    - $$H \cdot W$$ë¥¼ ë‚˜ëˆ„ëŠ” ì´ìœ ?
        - normalization: ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ loss ê°’ë„ ì»¤ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ loss ê°’ì„ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ normalizeí•©ë‹ˆë‹¤.
    - marked by red box/arrorw in Figure 2.
    - the sum of pixel-wise differences between our estimated saliency map and an actual saliency map.
    - involved in updating the parameters of $$C + 1$$ classes, including target objects and the background.
- $$\mathcal{L}_{cls}={-1 \over C} \Sigma^C_{i=1} y_i log \sigma (\hat{y_i}) + (1-y_i) log (1-\sigma (\hat{y_i}))$$.
    - $$\sigma$$: sigmoid function.
    - *ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(Binary Cross-Entropy Loss)* ê³µì‹.
    - marked by blue box/arrorw in Figure 2.
    - only evaluates the label prediction for $$C$$ classes, excluding the background class.
        - the gradient from $$\mathcal{L}_{cls}$$ does not flow into the background class.
    

## Joint Training
- By jointly training the two objectives, we can synergize the localization map and the saliency map with complementary information
- we observe that noisy and missing information of each other is complemented via our joint training strategy, as illustrated in Figure 3
    - Missing objects: $$(c)$$ì—ì„œ ë†“ì¹œ ì˜ìì™€ ë³´íŠ¸ ê°ì²´ë¥¼ $$(d)$$ì—ì„œëŠ” ì˜ segmentí•˜ëŠ” ëª¨ìŠµ.
    - noise ì œê±°: $$(c)$$ì— ì¡´ì¬í•˜ëŠ” ë¹„í–‰ê¸°ì˜ contrail(ì—°ê¸°) ê°™ì€ ê²ƒë“¤ì´ $$(d)$$ì—ì„œëŠ” ì œê±°ëœ ëª¨ìŠµ.

$$\hat{M}_s=\lambda M_{fg} + (1-\lambda)(1-M_{bg})$$

- $$\hat{M}_s$$: estimated saliency map.
- $$M_s$$: actual saliency map.
- $$M_{fg}$$: foreground saliency map.
    - $$M_{fg}=\Sigma^C_{i=1} y_i \cdot M_i \cdot \mathbb{1} [ \mathcal{O} (M_i,M_s) > \tau]$$.
        - $$\mathcal{O}(M_i,M_s)$$: is the function to compute the overlapping ratio between $$M_i$$ and $$M_s$$.
        - $$M_i$$: $$i$$-th localization map.
            - Assigned to the foreground if $$M_i$$ is overlapped with the saliency map more than $$\tau%$$%, otherwise the background.
        - $$y_i \in \mathbb{R}^C$$: binary image-level label $$[0 or 1]$$.
            - ëª¨ë¸ ì˜ˆì¸¡ì´ ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ê° ê°ì²´ì— ëŒ€í•œ saliency mapì„ í•©í•˜ì—¬ ìµœì¢… foreground saliency mapì„ êµ¬ì„±í•œë‹¤.
- $$M_{bg}$$: background saliency map.
    - $$M_{bg}=\Sigma^C_{i=1} y_i \cdot M_i \cdot \mathbb{1} [ \mathcal{O} (M_i,M_s) <= \tau] + M_{C+1}$$.
- $$\lambda \in [0,1]$$: a hyperparameter to adjust a weighted sum of the foreground map and the inversion of the background map.

****
# Experiment ğŸ‘€
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/eae49c9d-fa7d-4ba4-8c0a-0e890940986c)

## Setup
### Datasets
- PASCAL VOC 2012 and MS COCO 2014
- augmented training set with 10,582 images
    - Augmentation ë¹¼ë©´ ì„±ëŠ¥ ì–´ë–¤ê°€??

### Baseline
- ResNet38 pre-trained on ImageNet

## Boundary Mismatch Problem
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a4d56b33-bac9-4bcd-86f0-fd280c6a2b93)

## Co-occurrence Problem
- What is it?
    - Some background classes frequently appear with target objects in PASCAL VOC 2012
- Dataset: `PASCAL-CONTEXTdataset`.
    - provides pixel-level annotations for a whole scene (e.g., water and railroad).
- Evaluation:

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/96c263c3-9a44-400f-82de-81425918ae54)

- We choose three co-occurring pairs; boat with water, train with railroad, and train with platform. We compare IoU for the target class and the confusion ratio $$m_{k,c}={FP_{k,c} \over TP_c}$$ between a target class and its coincident class.
    - $$FP_{k,c}$$: the number of pixels mis-classified as the target class $$c$$ for the coincident class $$k$$.
    - $$TP_c$$: the number of true-positive pixels for the target class $$c$$.
    - $$k$$: the coincident class.
    - $$c$$: the target class.
- `SEAM` ëŠ” íŠ¹ì´í•˜ê²Œ self-supervised trainingì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ê°ì²´ì— ëŒ€í•´ ê²¹ì¹˜ëŠ” í”½ì…€ë“¤ì— ì˜ëª»ëœ target object ë ˆì´ë¸”ì„ í• ë‹¹í•˜ê³  ì´ê²ƒì„ ì •ë‹µ ë ˆì´ë¸”ë¡œ í•™ìŠµì— í™œìš©í•˜ì—¬ ë” confusion ratioê°€ ë” ë†’ê²Œ ì¸¡ì •ë˜ëŠ” ëª¨ìŠµì´ë‹¤.

## Map Selection Strategies

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/860f1bf7-df3a-41ba-84af-d757a11c34a1)

- **Naive strategy**:
    - The foreground map is the union of all object localization maps; the background map equals the localization map of the background class.
- **Pre-defined class strategy:**
    - We follow the naive strategy with the following exceptions. The localization maps of several pre-determined classes (e.g., sofa, chair, and dining table) are assigned to the background map (i.e., pre-defined class strategy)
        - íŠ¹ì • ê°ì²´ë“¤ì´ target objectsê°€ ì•„ë‹˜ì„ ì•Œê³  ë¯¸ë¦¬ ë°°ê²½ìœ¼ë¡œ ì§€ì •.
- **Our adaptive strategy:**
    - ìƒê¸° EPS ë‚´ìš© ì°¸ì¡°.

**â‡’ `Our adaptive` ê°€ ê°€ì¥ IoU ìˆ˜ì¹˜ê°€ ë†’ê³ , ì´ëŠ” í•´ë‹¹ ì „ëµì´ target objectë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•¨ì„ ì˜ë¯¸í•œë‹¤.**

## Comparison with state-of-the-arts
### Accuracy of pseudo-masks
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/36f66f6d-35a6-4b08-b59a-b88015e42496)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/289c45a1-551d-49c4-b3a2-f12c087f95e0)

### Accuracy of segmentation maps
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/14a18c5d-baf5-4510-bc82-7503493f246d)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c87cd690-d3a2-4ff4-b962-17296e3a1282)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/237e63e7-26da-4c7d-ad0c-5883bf2b3bc1)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/09c17633-b4a8-4396-8081-1dd2c773a89d)

## Effect of saliency detection models
- Notably, our EPS using the unsupervised saliency model outperforms all existing methods using the supervised saliency model

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Conclusion âœ¨
- We propose a novel weakly supervised segmentation framework, namely explicit **pseudo-pixel supervision (EPS)**.

****
# Reference
NA