---
layout: single
title: "[논문분석] Entropy regularization for weakly supervised object localization"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, CAM, Entropy, Object Localization]
toc: true
toc_sticky: true
toc_label: "쭌스log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [논문링크](https://www.sciencedirect.com/science/article/abs/pii/S0167865523000831) </span>.

****
# 한줄요약 ✔
- The objective of classification training is not entirely consistent with that of localization.
    - Very low entropy might be important for classification, but less helpful for localization.
- Sweet spot for localization with respect to entropy.
    - new term to the loss function.
        - predicted class prob. vector가 uniform dist.에 닮게 만드는 정도를 조절 → uniform dist.는 각 클래스 예측 확률이 동일 (class 예측 불확실성 증가) → localization 성능 증가.

****
# Preliminaries 🍱
## Weakly-supervised object localization (WSOL)
- The goal of weakly-supervised object localization (WSOL) is to train a localization model without the location information of the object.
    - Object localization requires expensive pixel-level annotations (e.g., bounding boxes or segmentation masks).
    - A well-known limitation of WSOL methods is that the model only **focuses on the most discriminative part**, which hinders capturing the entire extent of the object accurately.
    - The model captures not only the target object but also frequently **co-occurring background components** (e.g., snowboard-snow, train-railroad).
- The mainstream of WSOL is to train a CNN-based image classification model and then extract an attention map using a visual interpretable method (i.e., CAM, Grad-CAM, etc.).

## Regularization
$$\mathcal{L}(\theta):=(1-\gamma)\bold{H}(y,p_{\theta})-\gamma \bold{KL}(u \vert\vert p_{\theta})$$

### Label smoothing (CVPR'15)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e1eef28d-5b11-4db4-8fd1-21d94c5b9013)

- To avoid overfitting (i.e., Dropout, L2, data aug., etc.).
    - Introduces noise to the ground truth labels.
- 상기 그림에서 target dist.(GT)가 soft labels처럼 변경되어 entropy가 증가한 모습이다. 이는 모델의 예측 분포가 서로 다른 클래스에 미량의 확률을 부여할 수 있기에 GT인 이 soft labels분포와 더 유사해질 수 있어서 모델 예측 결과인 output dist. 의 entropy는 더 낮다.

### Confidence penalty (ICLR'17)
$$\mathcal{L}(\theta)=\bold{H}(y,p_{\theta})-\lambda \bold{H}(p_{\theta})$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{KL}(p_{\theta} \vert \vert u)$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{JSD}(p_{\theta} \vert \vert u)$$

### Jensen Shannon (JS) divergence
$$\bold{JSD}(p_{\theta} \vert \vert u) = \bold{KL}(u \vert \vert p_{\theta}) + \bold{KL}(p_{\theta} \vert \vert u)$$

- 두 확률 분포 사이의 차이를 측정하는 통계적인 방법 중 하나.
- `JSD`는 두 분포 간의 유사성을 나타내는 값으로, 쿨백-라이블러 발산(Kullback-Leibler Divergence)의 제곱근.

****
# Conclusion ✨
NA

****
# Reference
NA