---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Entropy regularization for weakly supervised object localization"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, CAM, Entropy, Object Localization]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [ë…¼ë¬¸ë§í¬](https://www.sciencedirect.com/science/article/abs/pii/S0167865523000831) </span>.

****
# í•œì¤„ìš”ì•½ âœ”
- The objective of classification training is not entirely consistent with that of localization.
    - Very low entropy might be important for classification, but less helpful for localization.
- Sweet spot for localization with respect to entropy.
    - new term to the loss function.
        - predicted class prob. vectorê°€ uniform dist.ì— ë‹®ê²Œ ë§Œë“œëŠ” ì •ë„ë¥¼ ì¡°ì ˆ â†’ uniform dist.ëŠ” ê° í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥ ì´ ë™ì¼ (class ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì¦ê°€) â†’ localization ì„±ëŠ¥ ì¦ê°€.

****
# Preliminaries ğŸ±
## Weakly-supervised object localization (WSOL)
- The goal of weakly-supervised object localization (WSOL) is to train a localization model without the location information of the object.
    - Object localization requires expensive pixel-level annotations (e.g., bounding boxes or segmentation masks).
    - A well-known limitation of WSOL methods is that the model only **focuses on the most discriminative part**, which hinders capturing the entire extent of the object accurately.
    - The model captures not only the target object but also frequently **co-occurring background components** (e.g., snowboard-snow, train-railroad).
- The mainstream of WSOL is to train a CNN-based image classification model and then extract an attention map using a visual interpretable method (i.e., CAM, Grad-CAM, etc.).

## Regularization
$$\mathcal{L}(\theta):=(1-\gamma)\bold{H}(y,p_{\theta})-\gamma \bold{KL}(u \vert\vert p_{\theta})$$

### Label smoothing (CVPR'15)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e1eef28d-5b11-4db4-8fd1-21d94c5b9013)

- To avoid overfitting (i.e., Dropout, L2, data aug., etc.).
    - Introduces noise to the ground truth labels.
- ìƒê¸° ê·¸ë¦¼ì—ì„œ target dist.(GT)ê°€ soft labelsì²˜ëŸ¼ ë³€ê²½ë˜ì–´ entropyê°€ ì¦ê°€í•œ ëª¨ìŠµì´ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¶„í¬ê°€ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ë¯¸ëŸ‰ì˜ í™•ë¥ ì„ ë¶€ì—¬í•  ìˆ˜ ìˆê¸°ì— GTì¸ ì´ soft labelsë¶„í¬ì™€ ë” ìœ ì‚¬í•´ì§ˆ ìˆ˜ ìˆì–´ì„œ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì¸ output dist. ì˜ entropyëŠ” ë” ë‚®ë‹¤.

### Confidence penalty (ICLR'17)
$$\mathcal{L}(\theta)=\bold{H}(y,p_{\theta})-\lambda \bold{H}(p_{\theta})$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{KL}(p_{\theta} \vert \vert u)$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{JSD}(p_{\theta} \vert \vert u)$$

### Jensen Shannon (JS) divergence
$$\bold{JSD}(p_{\theta} \vert \vert u) = \bold{KL}(u \vert \vert p_{\theta}) + \bold{KL}(p_{\theta} \vert \vert u)$$

- ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í†µê³„ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜.
- `JSD`ëŠ” ë‘ ë¶„í¬ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œ, ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler Divergence)ì˜ ì œê³±ê·¼.

## CAM
- a visual interpretable method that extracts an attention map from a classification network.
    - computed as a channel-wise weighted average of the last convolutional feature map and the weights to the predicted class in the last fully connected layer.
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ìˆœí•œ CAMì´ ì•„ë‹ˆë¼, ë§ˆì¹˜ Grad-CAMì²˜ëŸ¼ channel-wiseí•˜ê²Œ CNN ìµœì¢… layerì˜ attn. mapì„ Global Average Pooling (GAP)í•©ë‹ˆë‹¤.

## Others
### Hide-and-Seek (HaS, ICCV'17)
- ì •ì˜
    - a data augmentation that divides the images into rectangular patches and then hides the randomly selected patches during training.
    - ì§ì ‘ ê°€ë¦¼.
- ëª©ì 
    - the classification network might no longer see the most discriminative parts of the object, hence it can learn the less discriminative parts of the object as well.

### CutMix (ICCV'19)
- ì •ì˜
    - a data augmentation technique that cuts and pastes a patch from an image into another image and blends the target labels according to the size of the patch.
    - target objectì— ë‹¤ë¥¸ ì´ë¯¸ì§€ì—ì„œ ê°€ì ¸ì˜¨ random patchë¥¼ ë®ì–´ì”Œì›€.
- ëª©ì 
    - CutMix is known to improve WSOL performance, since the classification network can learn the non-discriminative parts of objects.

### Attention-based Dropout Layer (ADL, PAMI'21)
- ì •ì˜
    - utilizes a self-attention mechanism to find the most discriminative part of the object during training.
        - attached to each layer of the classification network, which hides or boosts the most discriminative part.
- Remark
    - non-discriminative parts ê±¸ëŸ¬ë‚¸ë‹¤ëŠ” ëª…ëª©ì—ì„œ Dropout ë³„ì¹­ ì‚¬ìš©.

### Region-based Dropout Layer with Attention Prior (RDAP, PR'21)
- ì •ì˜
    - utilizes the self-attention mechanism to find the most discriminative part.
    - unlike the ADL, RDAP hides the most discriminative part with a fixed-size square mask, which results in a more effective and robust improvement in WSOL.
        - most discriminative partsë¥¼ ê°€ë¦¬ê³  í•™ìŠµí•˜ë©´ì„œ ì „ì²´ ê°ì²´ì˜ íŠ¹ì„±ì„ í¬í•¨í•˜ëŠ” ìƒëŒ€ì ìœ¼ë¡œ less discriminative ë¶€ë¶„ì— ëŒ€í•œ í•™ìŠµì„ ìœ ë„í•©ë‹ˆë‹¤.
            - ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ì„¤ëª…í•˜ë©´, ë§Œì•½ ê°œë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¶„ë¥˜ê¸°ê°€ ì–¼êµ´ ë¶€ë¶„ì— ì£¼ë¡œ ì£¼ëª©í–ˆë‹¤ë©´, AEë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ë¶€ë¶„ì„ ì§€ìš´ë‹¤ë©´ ë¶„ë¥˜ê¸°ëŠ” ë‹¤ë¥¸ ë¶€ë¶„ì— ì£¼ëª©í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ°ì‹ìœ¼ë¡œ ëª¨ë¸ì€ ë¬¼ì²´ì˜ ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµí•˜ê²Œ ë˜ë©°, ì´ëŠ” WSOLì—ì„œ ì „ì²´ ë¬¼ì²´ë¥¼ ê³ ë ¤í•œ ë” ë‚˜ì€ ì§€ì—­í™” ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Cross Entropy
$$H(p,q)=-\Sigma p(x) log q(x)$$

- In general, classification networks are trained by minimizing cross-entropy $$H(p,q)$$ between ground truth and predicted probability. That is, the entropy decreases as the training progresses, which improves classification accuracy.
    - $$p$$: the target distribution.
    - $$q$$: the approximation of the target distribution; prediction.

**=> Very low entropy might be important for classification, but less helpful for localization**.

**=> The model with very high entropy (e.g., early stage in training) cannot produce informative CAM in terms of object location, which results in poor localization performance**.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> WSOL ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ Classification performanceë¥¼ í¬ìƒí•´ì•¼ í•œë‹¤ </span>.

**Idea)** <span style="color:lightgreen"> Entropyì— ê´€í•´ regularizationì„ ì ìš©í•˜ì—¬ localizationê³¼ classification ì‚¬ì´ì˜ ì ì ˆí•œ sweet pointë¥¼ ì°¾ëŠ”ë‹¤ </span>.

****
# Problem Definition â¤ï¸
**Given** a WSOL model $$\mathcal{T}$$.

**Return** a model $$\mathcal{S}$$.

**Such that** $$\mathcal{S}$$ outperforms $$\mathcal{T}$$ in terms of both localization and classification.

****
# Proposed Method ğŸ§¿
## Loss Function
$$\mathcal{L}(\theta)=\bold{H}(y,p_{\theta})-\lambda \bold{KL}(u \vert \vert p_{\theta})$$
    - $$\bold{KL}(u \vert \vert p_{\theta})=\Sigma u log({u \over p_{\theta}})=\Sigma {1 \over C} log({{1 \over C} \over p_{\theta}})=-{1 \over C} \Sigma log(p_{\theta})$$.
        - ë§ˆì§€ë§‰ $$- {1 \over C}$$ì—ì„œ ìŒìˆ˜ ë¶€í˜¸ë¡œ ì¸í•´ Loss functionì—ì„œ $$+\lambda$$ë¡œ ë¶€í˜¸ê°€ ì¡°ì •ë˜ê³ , ì´ëŠ” $$\bold{KL}$$ê°€ ê°ì†Œí• ìˆ˜ë¡ ëª¨ë¸ ì˜ˆì¸¡ì´ ìµœëŒ€ entropyë¥¼ ê°–ëŠ” uniform dist.ê³¼ ìœ ì‚¬í•´ì§€ë©´ì„œ classification perf.ì„ í¬ìƒí•˜ì—¬ ë”ìš± ë‹¤ì–‘í•œ í´ë˜ìŠ¤ì— ëŒ€í•˜ì—¬ localization ì˜ˆì¸¡ì„ ìœ ë„í•œë‹¤.

****
# Experiment ğŸ‘€
## Formulation
- Baseline: **PreResNet-18 and SE-ResNet-50.**
- Dataset: **CUB** and **OpenImages**.
- Implementation:
    - Last strided convolution to non-strided convolution for doubling the resolution of the last convolutional layer.
    - Inference phase:
        1. attention map ì¶”ì¶œ with CAM.
            1. CUB ì™¸ ë‹¤ë¥¸ datasetì—ì„œëŠ” annotation masks(GT)ê°€ ì œê³µë˜ì–´ ìˆì–´ì„œ ì´ attnetion mapì„ evaluationì— ì§ì ‘ í™œìš©.
        2. ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ (mask annotations ì œê³µí•˜ì§€ ì•ŠëŠ” CUB ê°™ì€ datasetë§Œ í•´ë‹¹):
            1. threshold the attention map with $$\sigma$$ to obtain the object mask.
                1. highlighted ëœ í”½ì…€ê°’ë“¤ë§Œ ì‚´ë¦¬ê³  ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ í•´ë‹¹ ê°ì²´ ë¶€ë¶„ attentionë§Œ ê°•ì¡°í•˜ì—¬ object mask íšë“.
            2. obtain the tightest bounding box containing the largest connected contour in the predicted mask
                1. ê°€ë ¹, CUB datasetì—ì„œëŠ” $$\sigma=0.15$$ë¥¼ ì‚¬ìš©(ì´ì „ ë…¼ë¬¸ ê²°ê³¼ ì¸ìš©).
        3. set $$\lambda=10^{-4}$$ .
- Metrics:
    - Top-1 localization acc. and GT-knownlocalization acc.
    - Datasets:
        - CUB (w/o mask annotations) $$\rightarrow$$ $$IoU > 50%$$.
        - OpenImages, Oxford (w/ mask annotations) $$\rightarrow$$ pixel-wise average precision (`PxAP`).
        - PASCAL VOC 2012 (w/ mask annotations and multiple image classes) $$\rightarrow$$ `mPxAPmetric`.
            - average `PxAP` of all classes.

## Quantitative Eval. Results
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2a0dc154-15d2-4a05-aaaa-7826628a3051)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/00c9d03e-6202-4f94-bea4-4e337f5e5a93)

## Comparison with other entropy regularization methods
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/728c8eb3-88f6-478b-8cce-0dc768fa47e5)

## Ablation
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/1e299682-c88b-4f89-90c8-1ca8d0b17a4d)

- Specifically, the highest performance is achieved with $$\lambda$$ of $$10^{âˆ’4}$$, where the performance gain is significant (86.8% of ours vs. 74.8% of previous SOTA).
- $$\lambda=10^{-3}$$ ì—ì„œ OpenImages ì„±ëŠ¥ ë§¤ìš° ì•ˆ ì¢‹ì€ ì´ìœ ?
    - Training/Validation ë‹¨ê³„ì—ì„œ ì„±ëŠ¥ì´ ì•ˆì¢‹ìŒ(underfitting) â†’ CAM ëª¨ë¸ì´ poor localization ìˆ˜í–‰.
    - ê°„ë‹¨íˆ model selectionìœ¼ë¡œ í•´ê²° ê°€ëŠ¥ ($$\lambda=10^{-4}$$ì—ì„œ ìµœì ì˜ ê²°ê³¼).

**â‡’ it is important to induce the model to have an appropriate entropy for WSOL, and our method is effective in finding the high-performance point.**

### Relationship between performances and entropy
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6fb5a3db-3ee4-4a49-80ac-53f903a88229)

### Comparison with Confidence penalty and label smoothing
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/099666cd-91f5-46eb-a0cc-f3f6a2e5831d)

### Feature map visualization at each convolution layer
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/74cf998b-330c-43f9-8dd3-2eb6b9986689)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/095da2a4-46bd-4872-b8d2-eaa20d0be314)

[**Fig. 5.** Qualitative evaluation results. We concatenate three images to show our results. The left side is the input image with a green prediction box and red ground truth box, the middle is the extracted attention map, and the right side is the overlap of the input image and the attention map.]

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Limitation
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/cfc46e9e-fc56-4adb-9bd2-9dadf3780246)

- ìƒê¸° ì‚¬ì§„ì—ì„œì²˜ëŸ¼ ë³´í†µ í•¸ë“¤ê³¼ ì•ë¬¸ì€ ê°™ì´ ìœ„ì¹˜í•´ìˆëŠ”ë°, ë³¸ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ ì´ ë‘ ê°œë¥¼ êµ¬ë¶„í•˜ì§€ëŠ” ëª»í•œë‹¤.
    - Without pixel-level annotation, it is not easy to obtain such information. Addressing this problem would be an interesting future research direction.

****
# Conclusion âœ¨
NA

****
# Reference
NA