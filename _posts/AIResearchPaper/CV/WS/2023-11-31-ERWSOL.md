---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Entropy regularization for weakly supervised object localization"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, CAM, Entropy, Object Localization]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [ë…¼ë¬¸ë§í¬](https://www.sciencedirect.com/science/article/abs/pii/S0167865523000831) </span>.

****
# í•œì¤„ìš”ì•½ âœ”
- The objective of classification training is not entirely consistent with that of localization.
    - Very low entropy might be important for classification, but less helpful for localization.
- Sweet spot for localization with respect to entropy.
    - new term to the loss function.
        - predicted class prob. vectorê°€ uniform dist.ì— ë‹®ê²Œ ë§Œë“œëŠ” ì •ë„ë¥¼ ì¡°ì ˆ â†’ uniform dist.ëŠ” ê° í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥ ì´ ë™ì¼ (class ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± ì¦ê°€) â†’ localization ì„±ëŠ¥ ì¦ê°€.

****
# Preliminaries ğŸ±
## Weakly-supervised object localization (WSOL)
- The goal of weakly-supervised object localization (WSOL) is to train a localization model without the location information of the object.
    - Object localization requires expensive pixel-level annotations (e.g., bounding boxes or segmentation masks).
    - A well-known limitation of WSOL methods is that the model only **focuses on the most discriminative part**, which hinders capturing the entire extent of the object accurately.
    - The model captures not only the target object but also frequently **co-occurring background components** (e.g., snowboard-snow, train-railroad).
- The mainstream of WSOL is to train a CNN-based image classification model and then extract an attention map using a visual interpretable method (i.e., CAM, Grad-CAM, etc.).

## Regularization
$$\mathcal{L}(\theta):=(1-\gamma)\bold{H}(y,p_{\theta})-\gamma \bold{KL}(u \vert\vert p_{\theta})$$

### Label smoothing (CVPR'15)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e1eef28d-5b11-4db4-8fd1-21d94c5b9013)

- To avoid overfitting (i.e., Dropout, L2, data aug., etc.).
    - Introduces noise to the ground truth labels.
- ìƒê¸° ê·¸ë¦¼ì—ì„œ target dist.(GT)ê°€ soft labelsì²˜ëŸ¼ ë³€ê²½ë˜ì–´ entropyê°€ ì¦ê°€í•œ ëª¨ìŠµì´ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¶„í¬ê°€ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ë¯¸ëŸ‰ì˜ í™•ë¥ ì„ ë¶€ì—¬í•  ìˆ˜ ìˆê¸°ì— GTì¸ ì´ soft labelsë¶„í¬ì™€ ë” ìœ ì‚¬í•´ì§ˆ ìˆ˜ ìˆì–´ì„œ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì¸ output dist. ì˜ entropyëŠ” ë” ë‚®ë‹¤.

### Confidence penalty (ICLR'17)
$$\mathcal{L}(\theta)=\bold{H}(y,p_{\theta})-\lambda \bold{H}(p_{\theta})$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{KL}(p_{\theta} \vert \vert u)$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{JSD}(p_{\theta} \vert \vert u)$$

### Jensen Shannon (JS) divergence
$$\bold{JSD}(p_{\theta} \vert \vert u) = \bold{KL}(u \vert \vert p_{\theta}) + \bold{KL}(p_{\theta} \vert \vert u)$$

- ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í†µê³„ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜.
- `JSD`ëŠ” ë‘ ë¶„í¬ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œ, ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler Divergence)ì˜ ì œê³±ê·¼.

****
# Conclusion âœ¨
NA

****
# Reference
NA