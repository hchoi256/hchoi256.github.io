---
layout: single
title: "[ÎÖºÎ¨∏Î∂ÑÏÑù] Entropy regularization for weakly supervised object localization"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, CAM, Entropy, Object Localization]
toc: true
toc_sticky: true
toc_label: "Ï≠åÏä§log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [ÎÖºÎ¨∏ÎßÅÌÅ¨](https://www.sciencedirect.com/science/article/abs/pii/S0167865523000831) </span>.

****
# ÌïúÏ§ÑÏöîÏïΩ ‚úî
- The objective of classification training is not entirely consistent with that of localization.
    - Very low entropy might be important for classification, but less helpful for localization.
- Sweet spot for localization with respect to entropy.
    - new term to the loss function.
        - predicted class prob. vectorÍ∞Ä uniform dist.Ïóê ÎãÆÍ≤å ÎßåÎìúÎäî Ï†ïÎèÑÎ•º Ï°∞Ï†à ‚Üí uniform dist.Îäî Í∞Å ÌÅ¥ÎûòÏä§ ÏòàÏ∏° ÌôïÎ•†Ïù¥ ÎèôÏùº (class ÏòàÏ∏° Î∂àÌôïÏã§ÏÑ± Ï¶ùÍ∞Ä) ‚Üí localization ÏÑ±Îä• Ï¶ùÍ∞Ä.

****
# Preliminaries üç±
## Weakly-supervised object localization (WSOL)
- The goal of weakly-supervised object localization (WSOL) is to train a localization model without the location information of the object.
    - Object localization requires expensive pixel-level annotations (e.g., bounding boxes or segmentation masks).
    - A well-known limitation of WSOL methods is that the model only **focuses on the most discriminative part**, which hinders capturing the entire extent of the object accurately.
    - The model captures not only the target object but also frequently **co-occurring background components** (e.g., snowboard-snow, train-railroad).
- The mainstream of WSOL is to train a CNN-based image classification model and then extract an attention map using a visual interpretable method (i.e., CAM, Grad-CAM, etc.).

## Regularization
$$\mathcal{L}(\theta):=(1-\gamma)\bold{H}(y,p_{\theta})-\gamma \bold{KL}(u \vert\vert p_{\theta})$$

### Label smoothing (CVPR'15)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e1eef28d-5b11-4db4-8fd1-21d94c5b9013)

- To avoid overfitting (i.e., Dropout, L2, data aug., etc.).
    - Introduces noise to the ground truth labels.
- ÏÉÅÍ∏∞ Í∑∏Î¶ºÏóêÏÑú target dist.(GT)Í∞Ä soft labelsÏ≤òÎüº Î≥ÄÍ≤ΩÎêòÏñ¥ entropyÍ∞Ä Ï¶ùÍ∞ÄÌïú Î™®ÏäµÏù¥Îã§. Ïù¥Îäî Î™®Îç∏Ïùò ÏòàÏ∏° Î∂ÑÌè¨Í∞Ä ÏÑúÎ°ú Îã§Î•∏ ÌÅ¥ÎûòÏä§Ïóê ÎØ∏ÎüâÏùò ÌôïÎ•†ÏùÑ Î∂ÄÏó¨Ìï† Ïàò ÏûàÍ∏∞Ïóê GTÏù∏ Ïù¥ soft labelsÎ∂ÑÌè¨ÏôÄ Îçî Ïú†ÏÇ¨Ìï¥Ïßà Ïàò ÏûàÏñ¥ÏÑú Î™®Îç∏ ÏòàÏ∏° Í≤∞Í≥ºÏù∏ output dist. Ïùò entropyÎäî Îçî ÎÇÆÎã§.

### Confidence penalty (ICLR'17)
$$\mathcal{L}(\theta)=\bold{H}(y,p_{\theta})-\lambda \bold{H}(p_{\theta})$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{KL}(p_{\theta} \vert \vert u)$$

$$=\bold{H}(y,p_{\theta})-\lambda \bold{JSD}(p_{\theta} \vert \vert u)$$

### Jensen Shannon (JS) divergence
$$\bold{JSD}(p_{\theta} \vert \vert u) = \bold{KL}(u \vert \vert p_{\theta}) + \bold{KL}(p_{\theta} \vert \vert u)$$

- Îëê ÌôïÎ•† Î∂ÑÌè¨ ÏÇ¨Ïù¥Ïùò Ï∞®Ïù¥Î•º Ï∏°Ï†ïÌïòÎäî ÌÜµÍ≥ÑÏ†ÅÏù∏ Î∞©Î≤ï Ï§ë ÌïòÎÇò.
- `JSD`Îäî Îëê Î∂ÑÌè¨ Í∞ÑÏùò Ïú†ÏÇ¨ÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Í∞íÏúºÎ°ú, Ïø®Î∞±-ÎùºÏù¥Î∏îÎü¨ Î∞úÏÇ∞(Kullback-Leibler Divergence)Ïùò Ï†úÍ≥±Í∑º.

## CAM
- a visual interpretable method that extracts an attention map from a classification network.
    - computed as a channel-wise weighted average of the last convolutional feature map and the weights to the predicted class in the last fully connected layer.
- Î≥∏ ÎÖºÎ¨∏ÏóêÏÑúÎäî Îã®ÏàúÌïú CAMÏù¥ ÏïÑÎãàÎùº, ÎßàÏπò Grad-CAMÏ≤òÎüº channel-wiseÌïòÍ≤å CNN ÏµúÏ¢Ö layerÏùò attn. mapÏùÑ Global Average Pooling (GAP)Ìï©ÎãàÎã§.

## Others
### Hide-and-Seek (HaS, ICCV'17)
- Ï†ïÏùò
    - a data augmentation that divides the images into rectangular patches and then hides the randomly selected patches during training.
    - ÏßÅÏ†ë Í∞ÄÎ¶º.
- Î™©Ï†Å
    - the classification network might no longer see the most discriminative parts of the object, hence it can learn the less discriminative parts of the object as well.

### CutMix (ICCV'19)
- Ï†ïÏùò
    - a data augmentation technique that cuts and pastes a patch from an image into another image and blends the target labels according to the size of the patch.
    - target objectÏóê Îã§Î•∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑú Í∞ÄÏ†∏Ïò® random patchÎ•º ÎçÆÏñ¥ÏîåÏõÄ.
- Î™©Ï†Å
    - CutMix is known to improve WSOL performance, since the classification network can learn the non-discriminative parts of objects.

### Attention-based Dropout Layer (ADL, PAMI'21)
- Ï†ïÏùò
    - utilizes a self-attention mechanism to find the most discriminative part of the object during training.
        - attached to each layer of the classification network, which hides or boosts the most discriminative part.
- Remark
    - non-discriminative parts Í±∏Îü¨ÎÇ∏Îã§Îäî Î™ÖÎ™©ÏóêÏÑú Dropout Î≥ÑÏπ≠ ÏÇ¨Ïö©.

### Region-based Dropout Layer with Attention Prior (RDAP, PR'21)
- Ï†ïÏùò
    - utilizes the self-attention mechanism to find the most discriminative part.
    - unlike the ADL, RDAP hides the most discriminative part with a fixed-size square mask, which results in a more effective and robust improvement in WSOL.
        - most discriminative partsÎ•º Í∞ÄÎ¶¨Í≥† ÌïôÏäµÌïòÎ©¥ÏÑú Ï†ÑÏ≤¥ Í∞ùÏ≤¥Ïùò ÌäπÏÑ±ÏùÑ Ìè¨Ìï®ÌïòÎäî ÏÉÅÎåÄÏ†ÅÏúºÎ°ú less discriminative Î∂ÄÎ∂ÑÏóê ÎåÄÌïú ÌïôÏäµÏùÑ Ïú†ÎèÑÌï©ÎãàÎã§.
            - Í∞ÑÎã®Ìïú ÏòàÏãúÎ°ú ÏÑ§Î™ÖÌïòÎ©¥, ÎßåÏïΩ Í∞úÎ•º Î∂ÑÎ•òÌïòÎäî Î∂ÑÎ•òÍ∏∞Í∞Ä ÏñºÍµ¥ Î∂ÄÎ∂ÑÏóê Ï£ºÎ°ú Ï£ºÎ™©ÌñàÎã§Î©¥, AEÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏñºÍµ¥ Î∂ÄÎ∂ÑÏùÑ ÏßÄÏö¥Îã§Î©¥ Î∂ÑÎ•òÍ∏∞Îäî Îã§Î•∏ Î∂ÄÎ∂ÑÏóê Ï£ºÎ™©ÌïòÍ≤å Îê©ÎãàÎã§. Ïù¥Îü∞ÏãùÏúºÎ°ú Î™®Îç∏ÏùÄ Î¨ºÏ≤¥Ïùò Îã§ÏñëÌïú ÌäπÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ ÌïôÏäµÌïòÍ≤å ÎêòÎ©∞, Ïù¥Îäî WSOLÏóêÏÑú Ï†ÑÏ≤¥ Î¨ºÏ≤¥Î•º Í≥†Î†§Ìïú Îçî ÎÇòÏùÄ ÏßÄÏó≠Ìôî ÏÑ±Îä•ÏúºÎ°ú Ïù¥Ïñ¥Ïßà Ïàò ÏûàÏäµÎãàÎã§.

## Cross Entropy
$$H(p,q)=-\Sigma p(x) log q(x)$$

- In general, classification networks are trained by minimizing cross-entropy $$H(p,q)$$ between ground truth and predicted probability. That is, the entropy decreases as the training progresses, which improves classification accuracy.
    - $$p$$: the target distribution.
    - $$q$$: the approximation of the target distribution; prediction.

**=> Very low entropy might be important for classification, but less helpful for localization**.

**=> The model with very high entropy (e.g., early stage in training) cannot produce informative CAM in terms of object location, which results in poor localization performance**.

****
# Conclusion ‚ú®
NA

****
# Reference
NA