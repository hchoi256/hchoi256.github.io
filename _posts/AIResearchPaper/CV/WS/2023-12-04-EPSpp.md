---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Saliency as Pseudo-Pixel Supervision for Weakly and Semi-Supervised Semantic Segmentation (PAMI 2023)"
categories: AIPaperCV
tag: [Computer Vision, Weakly-supervised Learning, Semi-supervised Learning CAM, EPS, EPS++]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ws.png
sidebar:
    nav: "docs"
---

<span style="color:sky"> [ë…¼ë¬¸ë§í¬](https://ieeexplore.ieee.org/document/10120949)  </span>

****
# í•œì¤„ìš”ì•½ âœ”
- **Explicit Pseudo-pixel Supervision (EPS++)** learns from pixel-level feedback by combining two types of weak supervision: localization and saliency maps.
    - localization map â†” object identity.
    - saliency map â†” rich object boundaries.
- **Inconsistent Region Drop (IRD) strategy.**
    - Effectively handles errors in saliency maps using fewer hyper-parameters than EPS.
- Extended to solve the **semi-supervised semantic segmentation** problem using image-level weak supervision.
        
****
# Preliminaries ğŸ±
## CAM
### Model Architecture
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/7f1ac706-7a16-42be-b7ec-8b22ff21a26b)

- CNNì˜ ìµœì¢… layer ì¶œë ¥ì¸ feature mapì— channel-wise ê³„ì‚°ë²•ì¸ GAPë¥¼ ì ìš©í•˜ì—¬ feature mapì˜ spatial info.ë¥¼ ìœ ì§€ (ê¸°ì¡´ CAMì—ì„œëŠ” 1ì°¨ì› ë°°ì—´ë¡œ point-wiseí•˜ê²Œ flattenì‹œí‚¨ í›„ fc-layerì˜ ì¸í’‹ìœ¼ë¡œ í™œìš©í•œë‹¤).
    - ê° ì±„ë„ì€ ì´ë¯¸ì§€ì—ì„œ ê° ê°ì²´ì˜ íŠ¹ì§•ì„ í‘œí˜„í•œë‹¤ (ì±„ë„ ê°œìˆ˜=ì»¤ë„ ê°œìˆ˜).
    
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/49132aea-08ab-4392-9494-dab4c2ed9386)

- ì´í›„, FC-layerì—ì„œ ê° íŠ¹ì§•ì„ ë‹´ê³ ìˆëŠ” feature mapì˜ í‰ê· ê°’ì„ ì¸í’‹ìœ¼ë¡œ ë°›ê³ , ê° ì¸í’‹ì˜ ê° classì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ softmaxë¥¼ ê±°ì³ weightsë¡œ í‘œí˜„í•œë‹¤. 

### Equation
$$Y^c=\Sigma_k w^c_k {1 \over Z} \Sigma_{i,j} A^k_{i,j}$$

- $$Y^c$$: class $$c$$ì— ëŒ€í•œ score (ëª¨ë¸ ì˜ˆì¸¡ logit/output).
- $${1 \over Z} \Sigma_{i,j} A^k_{i,j}$$:  feature map $$k$$ì˜ GAP ê°’.
    - $$Z$$: feature map $$k$$ì—ì„œ pixel ê°œìˆ˜.
- $$w^c_k$$: feature map $$k$$ì˜ class $$c$$ì— ëŒ€í•œ ë¯¼ê°ë„.
- $$A^k_{i,j}$$: feature map $$k$$ì˜ $$i,j$$ì— í•´ë‹¹í•˜ëŠ” pixel ê°’.

## Weak Supervision
### 1) Weakly supervised semantic segmentation (WSSS)
- Pseudo-masks are generated for target objects using an image classifier (i.e., CAM).
- Then, the segmentation model is trained using the pseudo-masks as supervision.

### 2) Semi-supervised semantic segmentation (SSSS)
- A segmentation network is trained using a small number of labeled data (e.g., 10% of the original train set) and a large number of $$(1)$$ weakly labeled data from the pipeline of WSSS or $$(2)$$ unlabeled data.
    - $$(1)$$ Full supervision (i.e., pixel-level annotation) is partially available, and the rest is weak supervision.
    - $$(2)$$ Full supervision is partially available, and the rest is unsupervised.
    

â‡’ WSSSë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¼ëŠ” ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $$(1)$$ë²ˆ ê²½ìš°ë¥¼ targetìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.

â‡’ í•˜ì—¬ EPS++ëŠ” ë” ì •í™•í•œ pseudo-masksë¥¼ ìƒì„±í•˜ê³ , ì´ê²ƒì€ SSSSì˜ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§„ë‹¤.

### 3) Saliency-Guided Semantic Segmentation
- Our EPS++ can be categorized as a saliency-guided method.
    - our method utilizes the saliency map as pseudo-pixel feedback for localization maps.

## EPS
[here](https://hchoi256.github.io/aipapercv/EPS/#loss-function)

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> DNN-based semantic segmentation methods require a significant amount of pixel-level annotation, which is extremely expensive and time-consuming to obtain. </span>

**I1)** <span style="color:lightgreen"> Weak supervision. </span>

**C3)** <span style="color:orange"> Existing tentative solutions still have their defects in performing semantic segmentation. </span>

**I2)** <span style="color:lightgreen"> EPS++ </span>

****
# Problem Definition â¤ï¸
Given a weak dataset $$\mathcal{D}$$.

Return a model $$\mathcal{S}$$.

Such that $$\mathcal{S}$$ approximates the performance of its fully-supervised model $$\mathcal{T}$$.

****
# Proposed Method ğŸ§¿
## Erroneous Saliency Maps
### Problems in Saliency Maps
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/156f5732-1779-4fab-a6b2-1b7acb90c526)

- $$(1)$$ **Missing class error:** A saliency map captures the full extent of some target classes, but not all target classes.
- $$(2)$$ **Missing object error:** A saliency map covers only a portion of the target object.
- $$(3)$$ **False object coverage error**: Non-target region is captured as salient region.

**â‡’ This systematic error is inevitable because the saliency model learns the statistics of different datasets.**

### Limitation of EPS
|  | $$(1)$$ | $$(2)$$ | $$(3)$$ |
| --- | --- | --- | --- |
| EPS | O | X | X |

- $$(1)$$: CAM ì‚¬ìš©í•´ì„œ ê° í´ë˜ìŠ¤ ë³„ë¡œ ê°ì²´ ë‚˜ëˆ„ëŠ” localization maps ìƒì„± í›„ ë‹¤ì‹œ ì·¨í•©í•œë‹¤.
- $$(2),(3)$$: class-wise errorsë§Œ í•´ê²°í•˜ê³ , pixel-wise errors í•´ê²° X $$\rightarrow$$ `IRD` ë“±ì¥.

## Inconsistent Region Drop (IRD)
### Background of `IRD`

- ê¸°ì¡´ EPS í•œê³„ë¥¼ ë²—ì–´ë‚˜ pixel level ë‹¨ìœ„ë¡œ handling í•˜ê¸° ìœ„í•´ ë„ì….
- ì •ë‹µ saliency mapê³¼ estimated foreground saliency map $$M_{fg}$$ê°„ì˜ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” `Inconsistent region`ëŠ” error ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ìš”ì¸ì´ë¼ íŒë‹¨.
    - `Inconsistent region`: the region where $$M_{fg}$$ mismatches the saliency map; it could be erroneous.
- ì´ëŸ¬í•œ `Inconsistent region`ì— í•´ë‹¹í•˜ëŠ” pixelë“¤ì€ saliency loss ê³„ì‚° ê³¼ì •ì—ì„œ ì œì™¸.
    - í•˜ì§€ë§Œ, $$M_{fg}$$ì—ëŠ” inaccurate boundariesê°€ ë§ì•„ì„œ ëŒ€ë¶€ë¶„ `inconsistent regions`ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ saliency loss ê°€ ë†’ê²Œ ì¸¡ì • $$\rightarrow$$ `Refinement module` ë“±ì¥.

### `IRD`
- Can preserve boundary information in $$M_{fg}$$ and obtain the refined foreground map $$M_r$$.
- $$M_r$$: refined foreground map obtained by applying `PAMR` to the localization maps $$M$$.
    - $$M$$: ê° í´ë˜ìŠ¤ ê°ì²´ë³„ localization map; CAMìœ¼ë¡œ ë¶€í„° ìƒì„±ë¨.

> **Pixel-adaptive mask refinement (PAMR):**
>
>> Iteratively refine label predictions by utilizing pixel-level affinity (ë³´ë‹¤ ìì„¸í•œ ë‚´ìš©ì€ í•´ë‹¹ ëª¨ë¸ ë…¼ë¬¸ ì°¸ì¡°).

## Architecture
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/3815b72a-3983-4329-9183-439abfa0fc62)

## Loss Function
$$\mathcal{L}_{total}=\mathcal{L}_{cls}+\mathcal{L}_{sal}$$

### Saliency Loss
<span style="color:yellow"> $$\mathcal{L}_{sal} = \frac{1}{\vert 1 - N \vert} \Sigma^{HW}_{p=1} (1-N^p) \cdot (M^p_s - M^p_{fg})^2,$$ </span>

- <span style="color:yellow"> $$N=\mathbb{B}(M_r) \odot \mathbb{B}(M_s),$$ </span>.
    - $$N$$: an `inconsistent region`.
        - $$M_r$$ê³¼ $$M_s$$ ê°„ì˜ `inconsistent region`.
    - $$M_s$$: the saliency map obtained from the off-the-shelf saliency detection model, PFAN trained on the DUTS dataset.
    - $$M^p_{fg}$$: ê¸°ì¡´ EPSìœ¼ë¡œ ìƒì„±ëœ feature map (refined estimated saliency mapê³¼ ë‹¤ë¦„).
    - $$\odot$$: XOR.
    - $$\mathbb{B}$$: the round operation (i.e., $$\mathbb{B}(M^p_k)=1$$ if $$M^p_k >0.5;\mathbb{B}(M^p_k)=0$$).
        - $$p$$: a pixel.
- <span style="color:yellow"> $$M_{fg}=\Sigma^C_{i=1} y_i \cdot M_{i},$$ </span>
    - $$C$$: class.
    - $$y_i \in \mathbb{R}^C$$: the binary image-level label.
    - $$M_i \in \mathbb{R}^{H \times W}$$: the $$i$$th localization map (generated by CAM).
    
### Class Loss
<span style="color:yellow"> $$\mathcal{L}_{cls}=-\frac{1}{C} \Sigma^C_{i=1} y_i log \sigma (\hat{y}_i)+(1-y_i) log (1-\sigma(\hat{y}_i)),$$ </span>

- $$\sigma$$: the sigmoid function.


## WSSS+SSSS
- Employ the idea of EPS++ on both WSSS and SSSS to demonstrate its effectiveness.
- Apply our EPS++ to the semi-supervised semantic segmentation task (i.e., utilizing both full and weak supervision)

â‡’ EPS++ achieves remarkable performances in both weakly and semi-supervised semantic segmentation tasks.

****
# Experiment ğŸ‘€
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b26296ba-a9ab-4b07-8e8c-0123f562878a)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/dbbd7a6a-98e6-4b43-a6c1-fe977806f44b)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e279a473-9bc6-4760-83f2-c28dc8eb8ff6)

- ìš”ì¦˜ì— acceptë˜ëŠ” ë…¼ë¬¸ë“¤ì€ ì‹¤í—˜ ê²°ê³¼ê°€ ê±°ì§„ ì¢‹ì•„ì„œ ì‹œê°„ì´ í—ˆë½ë˜ì§€ ì•Šìœ¼ë©´ êµ³ì´ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ì§„ ì•ŠëŠ”ë‹¤.
- ë‹¤ë§Œ, ê²½ìŸ ëª¨ë¸ë“¤ì´ EPP++ ì´ì „ ëª¨ë¸ì¸ EPS ëª¨ë¸ ë…¼ë¬¸ì˜ íˆ¬ê³  ì´ì „ ì‹œì ì˜ ê²½ìŸ ëª¨ë¸ë“¤ë§Œ í™œìš©í•œ ê²ƒì´ ì˜ë¬¸ì´ë‹¤.
- ë³´ë‹¤ ìì„¸í•œ ì •ë³´ëŠ” í•´ë‹¹ ë…¼ë¬¸ ì°¸ì¡° ìš”ë§.

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Conclusion âœ¨
- We propose a novel weakly supervised and semi-supervised segmentation framework, namely explicit **pseudo-pixel supervision (EPS++)**.

****
# Reference
NA