---
layout: single
title: "[논문분석] PETR: Position Embedding Transformation for Multi-View 3D Object Detection (ECCV, 2022)"
categories: Others
tag: [Computer Vision, 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
author_profile: false
header:
    teaser: /assets/images/posts/petr.png
sidebar:
    nav: "docs"
---

[논문링크](https://arxiv.org/abs/2203.05625)

<!-- <span style="color:lightgreen"> ???? </span> -->

****
# 한줄요약 ✔
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

1. **3D coordinate generator**로 생성된 3D 좌표들과 2D feature를 3D position encoder로 **3D position-aware feature**를 생성합니다.
2. 생성한 3D position-aware feature와 object query가 cross-attention을 통해 객체 Prediction을 수행합니다.

****
# Preliminaries 🍱
## DETR3D
DETR3D 논문 포스트 [여기](https://hchoi256.github.io/aipapercv/detr3d/)

****
# Problem Definition ✏
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main Idea💣
**C1)** <span style="color:orange"> 기존 DETR3D의 reference point 좌표가 부정확 할 수 있으며, projection 시킨 위치의 feature만 sampling 해오기 때문에 global view의 학습에서는 큰 도움이 되지 않습니다. </span>

**Idea 1)** <span style="color:lightgreen"> PETR은 2D feature map에 단순 projection이 아닌 3D coordinate 정보(3D positional embedding) 자체를 encoding 해서 3D 상에서의 feature를 얻습니다.</span>

****
# Proposed Method 🧿
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

## 3D Coordinates Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c9af1a12-255c-4269-9b7c-630a77347ef5)

- **Camera Frustum Space**: $$(W_F,H_F,D)$$.
- **Meshgrid**: $$p^m_j=(u_j \times d_j, v_j \times d_j, d_j, 1)^T$$.
    - $$d_j$$: depth value.
    - $$(u_j,v_j)$$: 픽셀 좌표.

> 맨 마지막 4차원의 $$1$$은 homogeneous counterpart입니다.

3D 좌표를 생성하기 위해서 2D 이미지와 3D Space 사이의 일대일 대응 관계를 먼저 이해해야 합니다.

하여 각 카메라 뷰의 Backbone 출력 2D 이미지 피처맵에 해당하는 <span style="color:red"> 빨간색 Image Plane 영역</span>의 모든 point들을 하이퍼 파라미터로 지정된 $$D$$ 깊이의 3D space으로 확장시킵니다.

이후, 이것을 Meshgrid로 나누어 **Camera Frustum Space**를 생성하고, 각 Point에 대한 3D 좌표를 얻습니다.

<span style="color:yellow"> $$p^{3d}_{i,j}=K^{-1}_i p^m_j$$ </span>

- $$i$$: $$i$$번째 다중 뷰 이미지입니다.

상기 공식을 활용해서 Camera Frustum Space의 3D 좌표를 3D 월드뷰 공간으로 역투영$$(K^{-1} \in \mathbb{R}^{4 \times 4})$$합니다.

이때 생성된 3D Space는 모든 Multi-View를 공유하기 떄문에, 서로 다른 뷰에서 얻은 3D 좌표들을 통합하여 3D 객체 Prediction을 수행합니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b2c2ee98-61b7-4ad4-9773-aa4b4561abd1)

이후, 상기 공식을 활용하여 Normalization을 수행하여 최종적으로 하기 정규화 된 3D 좌표 $$P^{3d}$$를 얻습니다.

<span style="color:yellow"> $$P^{3d}=\{P^{3d}_i \in \mathbb{R}^{(D \times 4) \times H_F \times W_F}, i=1,2,...,N\}$$ </span>

- $$D$$: depth value.
- $$H_F,W_F$$: 백본 출력인 2D 이미지 피처맵의 높이와 너비입니다.

## 3D Position Encoder
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a05e20d4-6572-42f0-84dc-5def8fba4fca)

이 단계에서는 3D Coordinates Generator의 3D 좌표와 Backbone의 2D 이미지 특징을 입력으로 받아 **3D Position-Aware Feature**를 생성합니다.

2D feature은 1DConv를 통과시켜 차원을 맞춰주고, 3D position은 MLP(FC $$\rightarrow$$ ReLU $$\rightarrow$$ FC)를 통과하여 3D position Embedding으로 변환됩니다.

이후, 두 입력을 더하여 3D position-aware feature를 생성합니다.

Decoder의 입력으로 사용하기 위해 Flatten시킵니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/64c87029-c29f-4994-9b6c-b5d4e80be33d)

상기 이미지는 3D Position Embedding을 분석한 표입니다.

첫 번째 행에서 front view의 왼쪽 point를 sampling하는 경우, Front Left view의 오른쪽 부분이 강조되어 유사도가 높은 모습입니다.

하여 통합된 3D space에 대해서 position embedding이 잘 되는 모습입니다.

## Query Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/5f0a28f3-f1c3-41ec-b7cd-842c8823d47f)

- 본 논문에서 anchor point의 개수는 $$1,500$$개를 사용했습니다.

객체 쿼리를 랜덤으로 초기화하는 것이 아닌, 객체가 존재할 것으로 생각되는 지점에 3D anchor point를 생성합니다.

1. Uniform 하게 3D World Space에 learnable anchor point를 initialize합니다.
2. 모든 3D anchor point를 작은 MLP Network에 통과시켜 object query를 생성합니다.

하여 anchor point가 MLP 네트워크 학습을 통해서 object가 있을 법한 곳으로 점차 이동해서 object가 있을 법한 위치에 object query를 생성할 수 있게 됩니다.

이를 통해, 기존 DETR 세팅으로도 충분한 수렴을 달성하였고, detection 성능도 올릴 수 있었다고 합니다.

## Decoder
<span style="color:yellow"> $$Q_l=\Omega_l (F^{3d},Q_{l-1}),l=1,...,L$$ </span>

- $$\Omega_l$$: $$l$$번째 Decoder Layer입니다.
- DETR에서 사용되었던 decoder layer 개수 $$L$$개를 그대로 사용합니다.

3D position-aware feature $$F^{3d}$$와 object query $$Q_{l-1}$$를 cross-attention을 통해 업데이트합니다.

## Loss
<span style="color:yellow"> $$L(y,\hat{y})=\lambda_{cls} * L_{cls}(c,\sigma(\hat{c}))+L_{reg}(b,\sigma(\hat{b}))$$ </span>

Decoder에서 생성된 object query를 사용해 classification을 수햅합니다.

또한, regressor에서 anchor point의 좌표를 예측하고 GT와 비교하여, 이를 역전파 과정에서 업데이트합니다.

****
# Experiment 👀
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/acc0be05-8e7b-4c21-a3da-52b55c60acfc)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/547f9c59-4821-49bf-b98b-5584e7ce147c)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/85b1d29d-11b8-47c8-bee3-a0c6f2c66b92)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c29d74a6-78e6-4f0c-a59f-420d80e264c5)

****
# Open Reivew 💗
NA

****
# Discussion 🍟
- Camera Frustum Space는 각 Depth Level마다 동일한 차원과 픽셀 값들을 가진 Plane으로 나열되었기 때문에, 굳이 3D 공간의 모든 점을 전부 3D 월드뷰 공간으로 일대일 대응시킬 필요가 있을까?
- 

****
# Major Takeaways 😃
- Camera Frustum Space
- 

****
# Conclusion ✨
NA

****
# Reference
NA