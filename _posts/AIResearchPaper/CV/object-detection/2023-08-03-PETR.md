---
layout: single
title: "[논문분석] PETR: Position Embedding Transformation for Multi-View 3D Object Detection (ECCV, 2022)"
categories: AIPaperCV
tag: [Computer Vision, 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
author_profile: false
header:
    teaser: /assets/images/posts/petr.png
sidebar:
    nav: "docs"
---

[논문링크](https://arxiv.org/abs/2203.05625)

<!-- <span style="color:lightgreen"> ???? </span> -->

****
# 한줄요약 ✔
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

1. **3D coordinate generator**로 생성된 3D 좌표들과 2D feature를 3D position encoder로 **3D position-aware feature**를 생성합니다.
2. 생성한 3D position-aware feature와 object query가 cross-attention을 통해 객체 Prediction을 수행합니다.

****
# Preliminaries 🍱
## DETR3D
DETR3D 논문 포스트 [여기](https://hchoi256.github.io/aipapercv/detr3d/)

## Camera Frustum Space
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/159358c4-c80f-4fda-9d5e-5ea2776934f2)

[DSGN: Deep Stereo Geometry Network for 3D Object Detection](https://arxiv.org/pdf/2001.03398.pdf)

카메라 시야에 포함된 공간을 **camera frustum**이라고 합니다.

상기 이미지에서 $$(u,v)$$ 좌표계가 기존 2D 이미지를 표현하고 있고, 여기에 깊이를 나타내는 $$d$$ 차원을 추가해서 해당 객체와 카메라 사이의 거리를 포함합니다.

샹기 논문에서 DSGN은 이 **camera frustum space**를 이용하여 왼쪽과 오른쪽 스테레오 이미지로부터 3D 객체를 탐지하는 방법을 제안하고 있습니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/f809e8f4-5fbc-496d-a9dd-253ed3aaf7cd)

상기 이미지처럼 본래 Stereo 카메라는 두 관점에서 이미지를 생성합니다.

Depth를 추정하기 위해서는 **Disparity**라는 것을 알아야 하고, 이를 위해 Stereo 카메라의 Left Image와 Right Image가 모두 필요합니다.

이때 두 이미지들에서 동일한 한 객체와 카메라 사이 간격의 차이를 **Disparity**라고 부르는데, 이 metric을 통해 Depth 추정이 가능합니다.

가령, 멀리 있는 객체의 경우 Disparity가 작고, 가까이 있는 것은 Disparity가 큽니다.

****
# Problem Definition ✏
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main Idea💣
**C1)** <span style="color:orange"> 기존 DETR3D의 reference point 좌표가 부정확 할 수 있으며, projection 시킨 위치의 feature만 sampling 해오기 때문에 global view의 학습에서는 큰 도움이 되지 않습니다. </span>

**Idea 1)** <span style="color:lightgreen"> PETR은 2D feature map에 단순 projection이 아닌 3D coordinate 정보(3D positional embedding) 자체를 encoding 해서 3D 상에서의 feature를 얻습니다.</span>

****
# Proposed Method 🧿
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

## 3D Coordinates Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c9af1a12-255c-4269-9b7c-630a77347ef5)

- **Camera Frustum Space**: $$(W_F,H_F,D)$$.
- **Meshgrid**: $$p^m_j=(u_j \times d_j, v_j \times d_j, d_j, 1)^T$$.
    - $$d_j$$: depth value.
    - $$(u_j,v_j)$$: 픽셀 좌표.

> 맨 마지막 4차원의 $$1$$은 homogeneous counterpart입니다.

각 카메라 뷰의 Backbone 출력 2D 이미지 피처맵에 해당하는 <span style="color:red"> 빨간색 Image Plane 영역</span>의 모든 point들을 하이퍼 파라미터로 지정된 $$D$$ 깊이의 3D space으로 확장시킵니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/0a3635df-2deb-4098-a0dd-27e7743a926e)

이후, 이것을 Meshgrid로 나누어 **Camera Frustum Space**를 형성하고, 각 Point에 대한 3D 좌표를 얻습니다.

여기서 Camera Frustum Space는 모든 카메라 뷰 이미지에 대해 공유되는 공간입니다.

<span style="color:yellow"> $$p^{3d}_{i,j}=K^{-1}_i p^m_j$$ </span>

- $$i$$: $$i$$번째 다중 뷰 이미지입니다.

상기 공식을 활용해서 Camera Frustum Space의 3D 좌표를 3D 월드뷰 공간으로 역투영$$(K^{-1} \in \mathbb{R}^{4 \times 4})$$합니다.

여기서 역투영 행렬(Camera Calibration Matrix)은 다음과 같으며, 이 또한 camera parameters를 통해 도출합니다:

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/4b67ae34-4e03-46cf-ae46-39f210376127)

이때 생성된 3D Space는 모든 Multi-View를 공유하기 떄문에, 서로 다른 뷰에서 얻은 3D 좌표들을 통합하여 3D 객체 Prediction을 수행합니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b2c2ee98-61b7-4ad4-9773-aa4b4561abd1)

이후, 상기 공식을 활용하여 Normalization을 수행하여 최종적으로 하기 정규화 된 3D 좌표 $$P^{3d}$$를 얻습니다.

<span style="color:yellow"> $$P^{3d}=\{P^{3d}_i \in \mathbb{R}^{(D \times 4) \times H_F \times W_F}, i=1,2,...,N\}$$ </span>

- $$D$$: depth value.
- $$H_F,W_F$$: 백본 출력인 2D 이미지 피처맵의 높이와 너비입니다.

## 3D Position Encoder
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a05e20d4-6572-42f0-84dc-5def8fba4fca)

이 단계에서는 3D Coordinates Generator의 3D 좌표와 Backbone의 2D 이미지 특징을 입력으로 받아 **3D Position-Aware Feature**를 생성합니다.

2D feature은 1DConv를 통과시켜 차원을 맞춰주고, 3D position은 MLP(FC $$\rightarrow$$ ReLU $$\rightarrow$$ FC)를 통과하여 3D position Embedding으로 변환됩니다.

이후, 두 입력을 더하여 3D position-aware feature를 생성합니다.

Decoder의 입력으로 사용하기 위해 Flatten시킵니다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/64c87029-c29f-4994-9b6c-b5d4e80be33d)

상기 이미지는 3D Position Embedding을 분석한 표입니다.

첫 번째 행에서 front view의 왼쪽 point를 sampling하는 경우, Front Left view의 오른쪽 부분이 강조되어 유사도가 높은 모습입니다.

하여 통합된 3D space에 대해서 position embedding이 잘 되는 모습입니다.

## Query Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/5f0a28f3-f1c3-41ec-b7cd-842c8823d47f)

- 본 논문에서 anchor point의 개수는 $$1,500$$개를 사용했습니다.

객체 쿼리를 랜덤으로 초기화하는 것이 아닌, 객체가 존재할 것으로 생각되는 지점에 3D anchor point를 생성합니다.

1. Uniform 하게 3D World Space에 learnable anchor point를 initialize합니다.
2. 모든 3D anchor point를 작은 MLP Network에 통과시켜 object query를 생성합니다.

하여 anchor point가 MLP 네트워크 학습을 통해서 object가 있을 법한 곳으로 점차 이동해서 object가 있을 법한 위치에 object query를 생성할 수 있게 됩니다.

이를 통해, 기존 DETR 세팅으로도 충분한 수렴을 달성하였고, detection 성능도 올릴 수 있었다고 합니다.

## Decoder
<span style="color:yellow"> $$Q_l=\Omega_l (F^{3d},Q_{l-1}),l=1,...,L$$ </span>

- $$\Omega_l$$: $$l$$번째 Decoder Layer입니다.
- DETR에서 사용되었던 decoder layer 개수 $$L$$개를 그대로 사용합니다.

3D position-aware feature $$F^{3d}$$와 object query $$Q_{l-1}$$를 cross-attention을 통해 업데이트합니다.

## Loss
<span style="color:yellow"> $$L(y,\hat{y})=\lambda_{cls} * L_{cls}(c,\sigma(\hat{c}))+L_{reg}(b,\sigma(\hat{b}))$$ </span>

Decoder에서 생성된 object query를 사용해 classification을 수햅합니다.

또한, regressor에서 anchor point의 좌표를 예측하고 GT와 비교하여, 이를 역전파 과정에서 업데이트합니다.

****
# Experiment 👀
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/acc0be05-8e7b-4c21-a3da-52b55c60acfc)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/547f9c59-4821-49bf-b98b-5584e7ce147c)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/85b1d29d-11b8-47c8-bee3-a0c6f2c66b92)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c29d74a6-78e6-4f0c-a59f-420d80e264c5)

****
# Open Reivew 💗
NA

****
# Discussion 🍟
- Camera Frustum Space는 각 Depth Level마다 동일한 차원과 픽셀 값들을 가진 Plane으로 나열되었기 때문에, 굳이 3D 공간의 모든 점을 전부 3D 월드뷰 공간으로 일대일 대응시킬 필요가 있을까?
- Camera Frustum Space 생성 과정을 좀 더 자세히 이해할 필요 있다.

****
# Major Takeaways 😃
- Camera Frustum Space
- Stereo v. Mono Depth Estimation

****
# Conclusion ✨
## Weakness
- <span style="color:lightgreen">기존 DETR 모델들과는 달리 Camera Frustum Space를 생성하기 위한 추가적인 Depth Estimation Network를 필요로 합니다. </span>

****
# Reference
[Stereo Camera](https://skk095.tistory.com/4)