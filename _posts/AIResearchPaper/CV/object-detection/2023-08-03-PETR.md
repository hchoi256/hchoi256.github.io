---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] PETR: Position Embedding Transformation for Multi-View 3D Object Detection (ECCV, 2022)"
categories: Others
tag: [Computer Vision, 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/petr.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/abs/2203.05625)

<!-- <span style="color:lightgreen"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

1. **3D coordinate generator**ë¡œ ìƒì„±ëœ 3D ì¢Œí‘œë“¤ê³¼ 2D featureë¥¼ 3D position encoderë¡œ **3D position-aware feature**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ìƒì„±í•œ 3D position-aware featureì™€ object queryê°€ cross-attentionì„ í†µí•´ ê°ì²´ Predictionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

****
# Preliminaries ğŸ±
## DETR3D
DETR3D ë…¼ë¬¸ í¬ìŠ¤íŠ¸ [ì—¬ê¸°](https://hchoi256.github.io/aipapercv/detr3d/)

****
# Problem Definition âœ
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ DETR3Dì˜ reference point ì¢Œí‘œê°€ ë¶€ì •í™• í•  ìˆ˜ ìˆìœ¼ë©°, projection ì‹œí‚¨ ìœ„ì¹˜ì˜ featureë§Œ sampling í•´ì˜¤ê¸° ë•Œë¬¸ì— global viewì˜ í•™ìŠµì—ì„œëŠ” í° ë„ì›€ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. </span>

**Idea 1)** <span style="color:lightgreen"> PETRì€ 2D feature mapì— ë‹¨ìˆœ projectionì´ ì•„ë‹Œ 3D coordinate ì •ë³´(3D positional embedding) ìì²´ë¥¼ encoding í•´ì„œ 3D ìƒì—ì„œì˜ featureë¥¼ ì–»ìŠµë‹ˆë‹¤.</span>

****
# Proposed Method ğŸ§¿
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/242d5db9-ed73-4b1c-a560-4cac66d757d7)

## 3D Coordinates Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c9af1a12-255c-4269-9b7c-630a77347ef5)

- **Camera Frustum Space**: $$(W_F,H_F,D)$$.
- **Meshgrid**: $$p^m_j=(u_j \times d_j, v_j \times d_j, d_j, 1)^T$$.
    - $$d_j$$: depth value.
    - $$(u_j,v_j)$$: í”½ì…€ ì¢Œí‘œ.

> ë§¨ ë§ˆì§€ë§‰ 4ì°¨ì›ì˜ $$1$$ì€ homogeneous counterpartì…ë‹ˆë‹¤.

3D ì¢Œí‘œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ 2D ì´ë¯¸ì§€ì™€ 3D Space ì‚¬ì´ì˜ ì¼ëŒ€ì¼ ëŒ€ì‘ ê´€ê³„ë¥¼ ë¨¼ì € ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

í•˜ì—¬ ê° ì¹´ë©”ë¼ ë·°ì˜ Backbone ì¶œë ¥ 2D ì´ë¯¸ì§€ í”¼ì²˜ë§µì— í•´ë‹¹í•˜ëŠ” <span style="color:red"> ë¹¨ê°„ìƒ‰ Image Plane ì˜ì—­</span>ì˜ ëª¨ë“  pointë“¤ì„ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •ëœ $$D$$ ê¹Šì´ì˜ 3D spaceìœ¼ë¡œ í™•ì¥ì‹œí‚µë‹ˆë‹¤.

ì´í›„, ì´ê²ƒì„ Meshgridë¡œ ë‚˜ëˆ„ì–´ **Camera Frustum Space**ë¥¼ ìƒì„±í•˜ê³ , ê° Pointì— ëŒ€í•œ 3D ì¢Œí‘œë¥¼ ì–»ìŠµë‹ˆë‹¤.

<span style="color:yellow"> $$p^{3d}_{i,j}=K^{-1}_i p^m_j$$ </span>

- $$i$$: $$i$$ë²ˆì§¸ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ì…ë‹ˆë‹¤.

ìƒê¸° ê³µì‹ì„ í™œìš©í•´ì„œ Camera Frustum Spaceì˜ 3D ì¢Œí‘œë¥¼ 3D ì›”ë“œë·° ê³µê°„ìœ¼ë¡œ ì—­íˆ¬ì˜$$(K^{-1} \in \mathbb{R}^{4 \times 4})$$í•©ë‹ˆë‹¤.

ì´ë•Œ ìƒì„±ëœ 3D SpaceëŠ” ëª¨ë“  Multi-Viewë¥¼ ê³µìœ í•˜ê¸° ë–„ë¬¸ì—, ì„œë¡œ ë‹¤ë¥¸ ë·°ì—ì„œ ì–»ì€ 3D ì¢Œí‘œë“¤ì„ í†µí•©í•˜ì—¬ 3D ê°ì²´ Predictionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b2c2ee98-61b7-4ad4-9773-aa4b4561abd1)

ì´í›„, ìƒê¸° ê³µì‹ì„ í™œìš©í•˜ì—¬ Normalizationì„ ìˆ˜í–‰í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ í•˜ê¸° ì •ê·œí™” ëœ 3D ì¢Œí‘œ $$P^{3d}$$ë¥¼ ì–»ìŠµë‹ˆë‹¤.

<span style="color:yellow"> $$P^{3d}=\{P^{3d}_i \in \mathbb{R}^{(D \times 4) \times H_F \times W_F}, i=1,2,...,N\}$$ </span>

- $$D$$: depth value.
- $$H_F,W_F$$: ë°±ë³¸ ì¶œë ¥ì¸ 2D ì´ë¯¸ì§€ í”¼ì²˜ë§µì˜ ë†’ì´ì™€ ë„ˆë¹„ì…ë‹ˆë‹¤.

## 3D Position Encoder
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a05e20d4-6572-42f0-84dc-5def8fba4fca)

ì´ ë‹¨ê³„ì—ì„œëŠ” 3D Coordinates Generatorì˜ 3D ì¢Œí‘œì™€ Backboneì˜ 2D ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ **3D Position-Aware Feature**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

2D featureì€ 1DConvë¥¼ í†µê³¼ì‹œì¼œ ì°¨ì›ì„ ë§ì¶°ì£¼ê³ , 3D positionì€ MLP(FC $$\rightarrow$$ ReLU $$\rightarrow$$ FC)ë¥¼ í†µê³¼í•˜ì—¬ 3D position Embeddingìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

ì´í›„, ë‘ ì…ë ¥ì„ ë”í•˜ì—¬ 3D position-aware featureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

Decoderì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Flattenì‹œí‚µë‹ˆë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/64c87029-c29f-4994-9b6c-b5d4e80be33d)

ìƒê¸° ì´ë¯¸ì§€ëŠ” 3D Position Embeddingì„ ë¶„ì„í•œ í‘œì…ë‹ˆë‹¤.

ì²« ë²ˆì§¸ í–‰ì—ì„œ front viewì˜ ì™¼ìª½ pointë¥¼ samplingí•˜ëŠ” ê²½ìš°, Front Left viewì˜ ì˜¤ë¥¸ìª½ ë¶€ë¶„ì´ ê°•ì¡°ë˜ì–´ ìœ ì‚¬ë„ê°€ ë†’ì€ ëª¨ìŠµì…ë‹ˆë‹¤.

í•˜ì—¬ í†µí•©ëœ 3D spaceì— ëŒ€í•´ì„œ position embeddingì´ ì˜ ë˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

## Query Generator
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/5f0a28f3-f1c3-41ec-b7cd-842c8823d47f)

- ë³¸ ë…¼ë¬¸ì—ì„œ anchor pointì˜ ê°œìˆ˜ëŠ” $$1,500$$ê°œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ê°ì²´ ì¿¼ë¦¬ë¥¼ ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ê°ì²´ê°€ ì¡´ì¬í•  ê²ƒìœ¼ë¡œ ìƒê°ë˜ëŠ” ì§€ì ì— 3D anchor pointë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

1. Uniform í•˜ê²Œ 3D World Spaceì— learnable anchor pointë¥¼ initializeí•©ë‹ˆë‹¤.
2. ëª¨ë“  3D anchor pointë¥¼ ì‘ì€ MLP Networkì— í†µê³¼ì‹œì¼œ object queryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

í•˜ì—¬ anchor pointê°€ MLP ë„¤íŠ¸ì›Œí¬ í•™ìŠµì„ í†µí•´ì„œ objectê°€ ìˆì„ ë²•í•œ ê³³ìœ¼ë¡œ ì ì°¨ ì´ë™í•´ì„œ objectê°€ ìˆì„ ë²•í•œ ìœ„ì¹˜ì— object queryë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

ì´ë¥¼ í†µí•´, ê¸°ì¡´ DETR ì„¸íŒ…ìœ¼ë¡œë„ ì¶©ë¶„í•œ ìˆ˜ë ´ì„ ë‹¬ì„±í•˜ì˜€ê³ , detection ì„±ëŠ¥ë„ ì˜¬ë¦´ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.

## Decoder
<span style="color:yellow"> $$Q_l=\Omega_l (F^{3d},Q_{l-1}),l=1,...,L$$ </span>

- $$\Omega_l$$: $$l$$ë²ˆì§¸ Decoder Layerì…ë‹ˆë‹¤.
- DETRì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ decoder layer ê°œìˆ˜ $$L$$ê°œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

3D position-aware feature $$F^{3d}$$ì™€ object query $$Q_{l-1}$$ë¥¼ cross-attentionì„ í†µí•´ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

## Loss
<span style="color:yellow"> $$L(y,\hat{y})=\lambda_{cls} * L_{cls}(c,\sigma(\hat{c}))+L_{reg}(b,\sigma(\hat{b}))$$ </span>

Decoderì—ì„œ ìƒì„±ëœ object queryë¥¼ ì‚¬ìš©í•´ classificationì„ ìˆ˜í–…í•©ë‹ˆë‹¤.

ë˜í•œ, regressorì—ì„œ anchor pointì˜ ì¢Œí‘œë¥¼ ì˜ˆì¸¡í•˜ê³  GTì™€ ë¹„êµí•˜ì—¬, ì´ë¥¼ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

****
# Experiment ğŸ‘€
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/acc0be05-8e7b-4c21-a3da-52b55c60acfc)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/547f9c59-4821-49bf-b98b-5584e7ce147c)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/85b1d29d-11b8-47c8-bee3-a0c6f2c66b92)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/c29d74a6-78e6-4f0c-a59f-420d80e264c5)

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
- Camera Frustum SpaceëŠ” ê° Depth Levelë§ˆë‹¤ ë™ì¼í•œ ì°¨ì›ê³¼ í”½ì…€ ê°’ë“¤ì„ ê°€ì§„ Planeìœ¼ë¡œ ë‚˜ì—´ë˜ì—ˆê¸° ë•Œë¬¸ì—, êµ³ì´ 3D ê³µê°„ì˜ ëª¨ë“  ì ì„ ì „ë¶€ 3D ì›”ë“œë·° ê³µê°„ìœ¼ë¡œ ì¼ëŒ€ì¼ ëŒ€ì‘ì‹œí‚¬ í•„ìš”ê°€ ìˆì„ê¹Œ?
- 

****
# Major Takeaways ğŸ˜ƒ
- Camera Frustum Space
- 

****
# Conclusion âœ¨
NA

****
# Reference
NA