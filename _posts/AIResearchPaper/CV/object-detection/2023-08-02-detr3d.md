---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] DETR3D (CoRL 2021)"
categories: AIPaperCV
tag: [Computer Vision, Camera-based 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/3d-od-cam.png
sidebar:
    nav: "docs"
---

[**ë…¼ë¬¸**](https://arxiv.org/pdf/2110.06922.pdf)

<!-- <span style="color:blue"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d0a480f2-ee7a-477a-9740-781f7e5a70f8)

- **Camera-based** Object Detectionì…ë‹ˆë‹¤.
    - ë‹¤ì¤‘ ë·°(multi-view) 2D ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ 3D ê³µê°„ì—ì„œ ì •ì˜ëœ ê°ì²´ ì¿¼ë¦¬ë“¤ì˜ ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
- **ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ**: ResNetê³¼ (multi-scale)FPNì„ ì‚¬ìš©í•´ì„œ ë‹¤ì¤‘ ë·° ì…ë ¥ ì´ë¯¸ì§€ë“¤ë¡œ ë¶€í„° multi-scale 2D íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
- **2D-to-3D Feature Transformation**: 3D ê³µê°„ì˜ reference pointì— ëŒ€í•œ ë‹¤ì¤‘ ë·° í”¼ì²˜ë§µë“¤ì—ì„œì˜ íŠ¹ì§•ì„ bilinear interpolationì„ í†µí•´ ë³´ê°„í•˜ê³ , ê·¸ë“¤ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
- **Loss**: ê¸°ì¡´ DETRê³¼ ë™ì¼í•œ Hungarian Lossë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- nuScenes datasetì„ ì´ìš©í•´ì„œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ê³  **SOTA ì„±ëŠ¥**ì„ ëƒˆìŠµë‹ˆë‹¤.

****
# Preliminaries ğŸ±
## Feature Pyramid Network (FPN)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b9d2f6e9-7bec-4d3f-adeb-c4c54d74baa0)

Feature Pyramid Network (FPN)ì€ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ê°ì²´ë¥¼ íƒì§€í•˜ê³  ë¶„í• í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì…ë‹ˆë‹¤.

FPNì€ **í•˜ìœ„ ë ˆì´ì–´**ì˜ ë†’ì€ í•´ìƒë„ì˜ ì •ë³´ì™€ **ë†’ì€ ë ˆì´ì–´**ì˜ ì¶”ìƒì  ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ ê°ì²´ íƒì§€ ë° ë¶„í•  ì‘ì—…ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

FPNì€ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œì¸ **Bottom-up**ê³¼ **Top-down**ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì•„ë˜ëŠ” ê° êµ¬ì„± ìš”ì†Œì˜ ì—­í• ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.

### Bottom-up
Bottom-upì€ ì¼ë°˜ì ì¸ CNN(Convolutional Neural Network) ê¸°ë°˜ì˜ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.

ë†’ì€ í•´ìƒë„ì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µ(Feature Map)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ResNetê³¼ ê°™ì€ ê¸°ì¡´ì˜ CNN ì•„í‚¤í…ì²˜ë¥¼ Bottom-upìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Top-down
Top-downì€ Bottom-upìœ¼ë¡œë¶€í„° ìƒì„±ëœ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ì´ìš©í•˜ì—¬ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤

ì¦‰, Bottom-upìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ë‚®ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ì„ Upsamplingí•œ í›„, ë‹¤ë¥¸ ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ê³¼ ê²°í•©í•˜ì—¬ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§•ê³¼ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µì„ ì–»ì–´ëƒ…ë‹ˆë‹¤.

## Bilinear Interpolation
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/7a9480e3-3fa2-45b5-bf7c-ae716aaa24d9)

Bilinear interpolationì€ ì£¼ì–´ì§„ ë‘ ì ì˜ ì‚¬ì´ì— ìœ„ì¹˜í•˜ëŠ” ê°’ë“¤ì„ ì¶”ì •í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë³´ê°„ ê¸°ë²•ìœ¼ë¡œ, ë‘ ì  ì‚¬ì´ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê°’ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## Camera Parameters
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/51be0e1a-1959-4119-8a6a-f04a76462bbc)

Camera ParameterëŠ” World Coordinateì— ìˆëŠ” 3D Voxel $$(U,V,W)$$ë¥¼ Image Coordinateì˜ 2D Pixel $$(x,y)$$ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì´ë•Œ 3ì°¨ì›ì˜ ì  $$(U,V,W)$$ëŠ” **homogeneous counterpart**ë¡œì¨ ìƒˆë¡œìš´ ì°¨ì›ì— $$1$$ ê°’ì„ ìƒˆë¡œ ì¶”ê°€í•˜ì—¬ $$3D \rightarrow 2D$$ ì°¨ì› ë³€í™˜ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•œë‹¤.
- 2D ì°¨ì›ìœ¼ë¡œì˜ íˆ¬ì˜ í–‰ë ¬ì˜ shapeì´ $$(3,4)$$ì´ê¸° ë•Œë¬¸ì— $$3$$ì°¨ì›ì˜ ì ì€ $$4$$ì°¨ì›ìœ¼ë¡œ ë§ì¶°ì¤„ í•„ìš”ê°€ ìˆë‹¤.
- ìƒˆë¡œìš´ ì°¨ì›ì˜ ê°’ì„ $$1$$ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì¨ í•´ë‹¹ ì°¨ì› ê°’ì€ íˆ¬ì˜ í–‰ë ¬ì— ì˜í•´ **depth(ê¹Šì´)**ê°€ ëœë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/393f3c48-615f-47d8-a6d8-3caf3a19e933)

- $$K$$: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(intrinsic parameter).
- $$[R \vert t]$$: ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„°(extrinsic parameter).
    - $$R$$: Rotation Matrix.
    - $$t$$: 3ì°¨ì› ê°’ì— ëŒ€í•œ scaling factor.
- $$K[R \vert t]$$: camera matrix ë˜ëŠ” projection matrix (íˆ¬ì˜ í–‰ë ¬).

### Extrinsic Parameter
ì¹´ë©”ë¼ ì¢Œí‘œê³„(2D)ì™€ ì›”ë“œ ì¢Œí‘œê³„(3D) ì‚¬ì´ì˜ ë³€í™˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œì„œ ë‘ ì¢Œí‘œê³„ ì‚¬ì´ì˜ **íšŒì „(rotation)** ë° **í‰í–‰ì´ë™(translation) ë³€í™˜**ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

### Intrinsic Parameter
#### 1) Focal Length(ì´ˆì ê±°ë¦¬): $$fx, fy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2d9556ec-561e-47ce-97dd-7a18ad22181c)

$$fx$$ì™€ $$fy$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ í‰ë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¡œ, ì´ˆì  ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

$$fx$$ëŠ” $$x$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼, $$fy$$ëŠ” $$y$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì´ˆì  ê±°ë¦¬ëŠ” ë Œì¦ˆì˜ êµ´ì ˆë ¥ê³¼ ê´€ë ¨ë˜ì–´ ê°ì²´ì˜ í¬ê¸°ì™€ ì¹´ë©”ë¼ ì‹œì ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.

#### 2) principal point(ì£¼ì ): $$cx, cy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/44fce7f7-2136-4a45-b73e-4910986bf8b0)

$$cx$$ì™€ $$cy$$ëŠ” ì´ë¯¸ì§€ í‰ë©´ ìƒì—ì„œ ì£¼ì ì˜ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì´ë¯¸ì§€ì˜ **ì¤‘ì‹¬ì **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì£¼ì ì€ ì¹´ë©”ë¼ì˜ ê´‘í•™ ì¶•ê³¼ ì´ë¯¸ì§€ í‰ë©´ì˜ êµì°¨ì ìœ¼ë¡œ, ì¹´ë©”ë¼ ì‹œì ê³¼ ê°ì²´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ ì •ë³´ì…ë‹ˆë‹¤.â€‹

#### 3) skew coefficient(ë¹„ëŒ€ì¹­ê³„ìˆ˜): $$skew_c = tan \alpha$$
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6b9aa1cf-7f0d-4ed7-bee4-2f909977a53e)

$$skew_c$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆê°€ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ë¹„ëŒ€ì¹­ì¸ ê²½ìš° $$y$$ì¶•ì´ ê¸°ìš¸ì–´ì§„ ì •ë„(ë¹„ëŒ€ì¹­ ì •ë„)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ $$tan(\alpha)$$ì˜ í˜•íƒœë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

ë¹„ëŒ€ì¹­ ê³„ìˆ˜ëŠ” ì£¼ë¡œ ì¹´ë©”ë¼ì˜ ë Œì¦ˆ ë˜ëŠ” ì´ë¯¸ì§€ ì„¼ì„œì˜ íŠ¹ì„±ê³¼ ê´€ë ¨ë˜ë©°, ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ ë³€í™˜ ë° ë³´ì • ì‘ì—…ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## DETR ëª¨ë¸
![image](https://user-images.githubusercontent.com/39285147/197422990-0d50e9ab-0866-40d2-9940-ff3ffb91fdde.png)

ë…¼ë¬¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ [ë§í¬](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).

****
# Problem Definition âœ
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ depth estimation networkì— ì˜ì¡´í•˜ì—¬ ë‚®ì€ qualityì˜ estimated depthì— ëŒ€í•´ 3D detection ì„±ëŠ¥ì— í•´ë¡œìš´ **compounding error**ë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. </span>


**Idea 1)** <span style="color:lightgreen"> DETR3DëŠ” depth estimation networkë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  **Camera Transformation Matrix**ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë·°ë¡œ ë¶€í„°ì˜ 2D Feature Extractionê³¼ 3D Object Predictionì„ ì—°ê²°í•˜ì—¬ ì´í›„ depth ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤. </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ í›„ì²˜ë¦¬ ê³¼ì •ì¸ **NMS**ì— ì˜ì¡´í•˜ê²Œ ë©ë‹ˆë‹¤.</span>

**Idea 2)** <span style="color:lightgreen"> DETR3DëŠ” End-to-End í•™ìŠµì„ í•˜ëŠ” DETR êµ¬ì¡°ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì— í›„ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. </span>

****
# Proposed Method ğŸ§¿
## Data Format
- $$\mathcal{L}=\{im_1,...,im_L\} \subset \mathbb{R}^{H_{im} \times W_{im} \times 3}$$: Multi-view Camera Image.
- $$\mathcal{T}=\{T_1,...,T_K\} \subset \mathbb{R}^{3 \times 4}$$: Camera Transformation Matrix.
    - intrinsic & extrinsic parameterë¥¼ í†µí•´ ë§Œë“œëŠ” í–‰ë ¬ì…ë‹ˆë‹¤ [ì—¬ê¸°](#camera-parameters).
- $$\mathcal{B}=\{b_1,...,b_j,...,b_M\} \subset \mathbb{R}^9$$: GT Bbox; Each $$b_j$$ contains (position, size, heading angle, velocity) in the birds-eye view (BEV).
    - Position: $$(x,y,z)$$.
    - Size: $$(w,h,d)$$.
    - Heading Angle: $$\theta$$.
    - Velocity: $$\mathcal{v}$$.
- $$\mathcal{C}=\{c_1,...,c_j,...,c_M\} \subset \mathbb{Z}$$: categorical labels.
- $$\mathcal{F}_k=\{f_{k1},...,f_{k6}\} \subset \mathbb{R}^{H \times W \times C}$$: a level of features of the 6 images.
    - $$f_{ki}$$: $$k$$ë²ˆì§¸ í¬ê¸° levelì˜ $$i$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·° í”¼ì²˜ë§µì…ë‹ˆë‹¤.

> NuScenes ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì¤‘ ë·° ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ ê°ê° ì´ 6ê°œì˜ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## Feature Learning
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/086244df-4a27-48d8-8f1b-6956ccaf8a60)

ResNetê³¼ [FPN](#feature-pyramid-network-fpn)ì„ ì‚¬ìš©í•˜ì—¬ì„œ Multi-Scaleì—ì„œì˜ Featureë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
- Multi-Scale FPN: ì„œë¡œ ë‹¤ë¥¸ sizeì˜ objectë¥¼ detection í•˜ëŠ”ë° í•„ìš”í•œ í’ë¶€í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ $$6$$ê°œì˜ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ë§ˆë‹¤ ì´ $$K$$ê°œ ë§Œí¼ì˜ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í”¼ì²˜ë§µ levelì„ ì–»ê³ , ì´ $$6 \times K$$ê°œì˜ í”¼ì²˜ë§µì„ íšë“í•©ë‹ˆë‹¤.

## Detection Head (Main Contribution)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a164ac5a-c859-4ec6-94e7-1163df4b05e3)

ê¸°ì¡´ Bottom-Up ë°©ì‹ë“¤ê³¼ëŠ” ë‹¬ë¦¬, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **Top-down ë°©ì‹**ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ NMSì™€ depth estimationìœ¼ë¡œ ë¶€í„° í•´ë°©ë©ë‹ˆë‹¤.

> Bottom-Up ë°©ì‹
>
>> Imageë‹¹ ë§ì€ bboxë¥¼ ì˜ˆì¸¡í•˜ê³  ë¶ˆí•„ìš”í•œ bboxë¥¼ NMSì™€ ê°™ì€ post-processingì„ í†µí•˜ì—¬ ê±¸ëŸ¬ì¤€ í›„, ê°ê°ì˜ viewì—ì„œ ì–»ì€ ê²°ê³¼ë¥¼ í•©ì³ì£¼ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

### í•™ìŠµê³¼ì •
1) Object Queryë¥¼ Neural Network$$(\Phi^{ref})$$ì— í†µê³¼ì‹œì¼œ Bbox Centerì˜ ì§‘í•©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

<span style="color:yellow"> $$c_{li}=\Phi^{ref}(q_{li})$$ </span>

- $$q_{li}$$: $$l$$ë²ˆì¨° ë ˆì´ì–´ì˜ $$i$$ë²ˆì§¸ ê°ì²´ ì¿¼ë¦¬ì—ì„œì˜ reference pointì…ë‹ˆë‹¤.
- $$c_{li}$$: $$l$$ë²ˆì¨° ë ˆì´ì–´ì˜ $$i$$ë²ˆì§¸ ê°ì²´ ì¿¼ë¦¬ì—ì„œì˜ $$3$$ ì°¨ì› Bbox Centerì…ë‹ˆë‹¤.

> Reference Point: í˜„ì¬ Targetìœ¼ë¡œ Focusí•˜ê³  ìˆëŠ” ì˜ì—­ í˜¹ì€ í”½ì…€ì…ë‹ˆë‹¤.

    reference_points (Tensor):  The normalized reference
                    points with shape (bs, num_query, 4),
                    all elements is range in [0, 1], top-left (0,0),
                    bottom-right (1, 1), including padding area.
                    or (N, Length_{query}, num_levels, 4), add
                    additional two dimensions is (w, h) to
                    form reference boxes.

ìƒê¸° ì½”ë“œì—ì„œ ì •ì˜ëœ `reference_points`ê°€ ë°”ë¡œ $$c_{li}$$ì´ë©°, ì´ê²ƒì˜ ì°¨ì›ì—ì„œ $$4$$ëŠ” $$(x,y,z,1)$$ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ($$1$$ì€ homegeneous counterpartì…ë‹ˆë‹¤).

    def forward(self,
                    query,
                    key,
                    value,
                    residual=None,
                    query_pos=None,
                    key_padding_mask=None,
                    reference_points=None,
                    spatial_shapes=None,
                    level_start_index=None,
                    **kwargs):

    reference_points_3d, output, mask = feature_sampling(
                value, reference_points, self.pc_range, kwargs['img_metas'])

forward í•¨ìˆ˜ë‚´ì—ì„œ `feature_sampling` í•¨ìˆ˜ë¥¼ í†µí•´ 3D ê°ì²´ ì¿¼ë¦¬ì˜ reference pointsë¥¼ Multi-Scale ë‹¤ì¤‘ ë·° ì¹´ë©”ë¼ ì´ë¯¸ì§€ í”¼ì²˜ë§µì´ë¼ëŠ” 2D ê³µê°„ìœ¼ë¡œ ì‚¬ì˜ì‹œí‚¨ ì§€ì ì˜ 2D ì´ë¯¸ì§€ íŠ¹ì§•ë“¤ì„ outputì— ë‹´ìŠµë‹ˆë‹¤ (`mask`ëŠ” ì‚¬ì˜ì‹œì¼°ì„ ë•Œ 2D ê³µê°„ ë°–ìœ¼ë¡œ ì‚¬ì˜ëœ ê²ƒë“¤ì˜ ìœ„ì¹˜ì— maskingì„ ì ìš©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤).

****

2) ì•ì„œ êµ¬í•œ 3D Bbox Centerë¥¼ Camera Tranformation Matrixë¥¼ í†µí•´ 2D Feature Mapìœ¼ë¡œ Projection(ì‚¬ì˜)ì‹œí‚µë‹ˆë‹¤.

<span style="color:yellow"> $$c^*_{li}=c_{li} \oplus 1$$ </span>

Camera Tranformation Matrixì˜ shapeì´ $$(3,4)$$ì´ê¸° ë•Œë¬¸ì— $$3$$ì°¨ì›ì—ì„œ $$2$$ì°¨ì›ìœ¼ë¡œì˜ ë³€í™˜ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•˜ê³ ì $$1$$ì„ Concatí•˜ì—¬ Homogeneous Counterpartë¡œ í‘œí˜„í•´ì¤ë‹ˆë‹¤.

    def feature_sampling(mlvl_feats, reference_points, pc_range, img_metas):
        reference_points = torch.cat((reference_points, torch.ones_like(reference_points[..., :1])), -1)

feature_sampling í•¨ìˆ˜ì…ë‹ˆë‹¤.

ìƒê¸° ì½”ë“œì—ì„œ concatì„ í†µí•´ $$c_{li}$$ì— ì°¨ì›ì„ í•˜ë‚˜ ë”í•´ì£¼ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

<span style="color:yellow"> $$c_{lmi}=T_mc^*_{li}$$ </span>

- $$T_m$$: $$m$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·°ì˜ íˆ¬ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ $$3$$ì°¨ì›ì˜ Centerë¥¼ $$4$$ì°¨ì›ìœ¼ë¡œ í™•ì¥í•œ í›„, íˆ¬ì˜ í–‰ë ¬ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ìµœì¢…ì ìœ¼ë¡œ 2D ì°¨ì› $$(x^{\prime},y^{\prime},1)$$ìœ¼ë¡œ ì‚¬ì˜ì‹œí‚µë‹ˆë‹¤.

> $$(x^{\prime},y^{\prime},1) = (x^{\prime},y^{\prime})$$.

    lidar2img = []
    for img_meta in img_metas:
        lidar2img.append(img_meta['lidar2img'])
    lidar2img = np.asarray(lidar2img)
    lidar2img = reference_points.new_tensor(lidar2img) # (B, N, 4, 4)

ì—¬ê¸°ì„œ `lidar2img`ëŠ” Camera Transformation Matrix $$T_m$$ì…ë‹ˆë‹¤.

    reference_points_cam = torch.matmul(lidar2img, reference_points).squeeze(-1)

ìƒê¸° ì½”ë“œì¤„ì€ íˆ¬ì˜ í–‰ë ¬ê³¼ reference pointsë¥¼ ê³±í•˜ì—¬ 2D ê³µê°„ìœ¼ë¡œ ì‚¬ì˜ì‹œí‚¤ëŠ” $$T_mc^*_{li}$$ ì—°ì‚°ì„ ì˜ë¯¸í•˜ë©°, ê²°ê³¼ì ìœ¼ë¡œ `reference_points_cam`ì— 2D ê³µê°„ìœ¼ë¡œ ì‚¬ì˜ì‹œí‚¨ reference pointsê°€ ë‹´ê¸°ê²Œ ë©ë‹ˆë‹¤.

****

3) ê° ì¹´ë©”ë¼ ë·° ì´ë¯¸ì§€ìœ¼ë¡œ ì‚¬ì˜ëœ ìœ„ì¹˜ì—ì„œ [Bilinear Interpolation](#bilinear-interpolation)ì„ í†µí•´ Featureë¥¼ Sample í•˜ê³  Object Queryì™€ ë”í•´ì¤ë‹ˆë‹¤.

<span style="color:yellow"> $$f_{lkmi}=f^{bilinear}(\mathcal{F}_{km},c_{lmi})$$ </span>

- $$f^{bilinear}$$: $$\mathcal{F}_{km}$$ í”¼ì²˜ë§µì—ì„œ $$c_{lmi} \in \mathbb{R}$$ ì¤‘ì‹¬ì ì„ ì ì ˆí•œ Indexë¡œ ë³´ê°„í•´ì£¼ê¸° ìœ„í•œ Bilinear Interpolation í•¨ìˆ˜ì…ë‹ˆë‹¤.

Bilinear Interpolationì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” $$3$$ì°¨ì›ì—ì„œ $$2$$ì°¨ì›ìœ¼ë¡œ ì‚¬ì˜ëœ CenterëŠ” ì‹¤ìˆ˜ê°’ ì¼ìˆ˜ë„ ìˆëŠ”ë°, $$2$$ì°¨ì›ì—ì„œ ì•Œë§ì€ Center Indexë¡œ ë³´ê°„í•´ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

    reference_points_cam_lvl = reference_points_cam.view(B*N, num_query, 1, 2)
    sampled_feat = F.grid_sample(feat, reference_points_cam_lvl)

ìƒê¸° ì½”ë“œì—ì„œ `feat`ëŠ” $$F_{km}$$ì´ê³ , `F.grid_sample` í•¨ìˆ˜ëŠ” $$f^{bilinear}$$ ë¶€ë¶„ìœ¼ë¡œ pytorch ë¬¸ì„œì— ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

torch.nn.functional.grid_sample(input, grid, mode='<span style="color:red">bilinear</span>', padding_mode='zeros', align_corners=None)

í•˜ì—¬ í•´ë‹¹ í•¨ìˆ˜ëŠ” bilinear interpolationì„ defaultë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

<span style="color:yellow"> $$ \textbf{f}_{li}={1 \over \Sigma_k \Sigma_m \sigma_{lkmi} +\epsilon} \Sigma_k \Sigma_m  \textbf{f}_{lkmi} \sigma_{lkmi}$$ </span>

- $$\sigma_{lkmi}$$: $$l$$ë²ˆì§¸ ë ˆì´ì–´ì˜ $$k$$ë²ˆì§¸ levelì˜ $$m$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·°ì˜ $$i$$ë²ˆì§¸ Pointê°€ Image Plane ë°–ìœ¼ë¡œ ì‚¬ì˜ëì„ ê²½ìš° í•„í„°ë§ í•˜ê¸°ìœ„í•œ binary ê°’ì…ë‹ˆë‹¤.
- $$\epsilon$$: ë¶„ëª¨ê°€ 0ìœ¼ë¡œ ë‚˜ëˆ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ ì ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•´ì¤ë‹ˆë‹¤.

ìš°ì„  `reference_points_cam`ì„ ì •ê·œí™”ë¥¼ í•˜ì—¬ Multi-Scale í”¼ì²˜ë§µë“¤ì˜ ë²”ìœ„ë¥¼ í†µì¼í™” ì‹œì¼œì¤ë‹ˆë‹¤.

    eps = 1e-5
    mask = (reference_points_cam[..., 2:3] > eps)
    reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(reference_points_cam[..., 2:3], torch.ones_like(reference_points_cam[..., 2:3])*eps)
    reference_points_cam[..., 0] /= img_metas[0]['img_shape'][0][1]
    reference_points_cam[..., 1] /= img_metas[0]['img_shape'][0][0]
    reference_points_cam = (reference_points_cam - 0.5) * 2

ì—¬ê¸°ì„œ `eps`ëŠ” ìˆ˜ì‹ì—ì„œ $$\epsilon$$ì„ ì˜ë¯¸í•˜ê³ , ìƒê¸° ì½”ë“œëŠ” normalizationì„ ìˆ˜í–‰í•˜ì—¬ 2D í”¼ì²˜ë§µì„ $$[-1,1]$$ ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.

    mask = (mask & (reference_points_cam[..., 0:1] > -1.0) 
                 & (reference_points_cam[..., 0:1] < 1.0) 
                 & (reference_points_cam[..., 1:2] > -1.0) 
                 & (reference_points_cam[..., 1:2] < 1.0))
    mask = mask.view(B, num_cam, 1, num_query, 1, 1).permute(0, 2, 3, 1, 4, 5)
    mask = torch.nan_to_num(mask)

ìƒê¸° ì½”ë“œì—ì„œ ì •ê·œí™” ëœ 2D í”¼ì²˜ë§µì˜ ë²”ìœ„ ë°–ìœ¼ë¡œ ì‚¬ì˜ëœ ê²ƒë“¤ì— ëŒ€í•´ maskingì„ ë¶€ì—¬í•©ë‹ˆë‹¤.

    sampled_feats = []
    for lvl, feat in enumerate(mlvl_feats):
        B, N, C, H, W = feat.size()
        feat = feat.view(B*N, C, H, W)
        reference_points_cam_lvl = reference_points_cam.view(B*N, num_query, 1, 2)
        sampled_feat = F.grid_sample(feat, reference_points_cam_lvl)
        sampled_feat = sampled_feat.view(B, N, C, num_query, 1).permute(0, 2, 3, 1, 4)
        sampled_feats.append(sampled_feat)
    sampled_feats = torch.stack(sampled_feats, -1)
    sampled_feats = sampled_feats.view(B, C, num_query, num_cam,  1, len(mlvl_feats))
    return reference_points_3d, sampled_feats, mask

ìœ„ ì½”ë“œë¥¼ í†µí•´ `sampled_feats` ë³€ìˆ˜ì— ìˆ˜ì‹ì—ì„œ $$f_{li}$$ ê°’ì¸ 2D ì´ë¯¸ì§€ íŠ¹ì§•ë§µë“¤ì„ ë‹´ì•„ ë¦¬í„´í•˜ê²Œ ë©ë‹ˆë‹¤.

    reference_points_3d, output, mask = feature_sampling(value, reference_points, self.pc_range, kwargs['img_metas'])

ì´ì „ feature_sampling í•¨ìˆ˜ë¥¼ invokeí–ˆë˜ ë¶€ë¶„ìœ¼ë¡œ ë‹¤ì‹œ ëŒì•„ì™€ì„œ, `output` ë³€ìˆ˜ì— `sampled_feats` ê°’ì´ ë‹´ê¸°ê²Œ ë©ë‹ˆë‹¤.

ì´ ë•Œ outputì˜ shapeëŠ” $$(B, C, num_query, num_cam, 1, len(mlvl_feats))$$ê°€ ë©ë‹ˆë‹¤.
- $$B$$: ë°°ì¹˜ í¬ê¸°.
- $$C$$: ì±„ë„ ìˆ˜ (íŠ¹ì§• ë§µì˜ ì±„ë„ ìˆ˜).
- `num_query`: ê°ì²´ ì¿¼ë¦¬ì˜ ê°œìˆ˜.
- `num_cam`: ë‹¤ì¤‘ ë·° ì¹´ë©”ë¼ì˜ ê°œìˆ˜.
- $$1$$: í•˜ë‚˜ì˜ í”¼ì²˜ë§µ ìƒ˜í”Œì„ ë‚˜íƒ€ë‚´ëŠ” ì°¨ì›.
- len(`mlvl_feats`): ë‹¤ì¤‘ ë ˆë²¨ì˜ í”¼ì²˜ë§µë“¤ì˜ ê°œìˆ˜.

ì´í›„ ë‹¤ìŒ ì½”ë“œ ì „ê°œë¥¼ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤:

    output = torch.nan_to_num(output) // outputì— ìˆëŠ” NaN ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜
    // ì°¨ì› ë³€í™” ì—†ìŒ

    mask = torch.nan_to_num(mask) // maskì— ìˆëŠ” NaN ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜
    // ì°¨ì› ë³€í™” ì—†ìŒ

    attention_weights = attention_weights.sigmoid() * mask // attention_weightsì— ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê³  maskì™€ ê³±í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
    // ì°¨ì› ë³€í™” ì—†ìŒ

    output = output * attention_weights // outputì— attention_weightsë¥¼ ê³±í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©
    // ì°¨ì› ë³€í™” ì—†ìŒ

    output = output.sum(-1).sum(-1).sum(-1) // outputì˜ ë§ˆì§€ë§‰ 3ê°œ ì¶•ì„ í•©í•˜ì—¬ ìƒˆë¡œìš´ í˜•íƒœë¡œ ë³€í™˜
    // ìƒˆë¡œìš´ ì°¨ì›: (B, C, num_query)

ì—¬ê¸°ì„œ `mask`ëŠ” `attention_weights`ì— ê³±í•´ì§€ê³ , ì´í›„ `attention_weights`ê°€ `output`ê°€ ê³±í•´ì ¸ì„œ maskingì´ ì ìš©ë˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ `output.sum(-1).sum(-1).sum(-1)` ë¶€ë¶„ì—ì„œ 6ê°œì˜ ë‹¤ì¤‘ ì¹´ë©”ë¼ ë·° ì´ë¯¸ì§€ë“¤ì— ëŒ€í•œ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ 2D í”¼ì²˜ë§µì˜ íŠ¹ì§•ë“¤ì„ ëª¨ë‘ ë”í•˜ê²Œ ë˜ê³ , ì´ ê³¼ì •ì€ ìƒê¸° ìˆ˜ì‹ê³¼ ë™ì¼í•©ë‹ˆë‹¤.

<span style="color:yellow"> $$ \textbf{q}_{l+1}= \textbf{f}_{li}+ \textbf{q}_{li}$$ </span>

ì´ë ‡ê²Œ $$2$$ì°¨ì›ìœ¼ë¡œ ë¶€í„° ê° ê°ì²´ ì¿¼ë¦¬ë§ˆë‹¤ ë”í•´ì§„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ íŠ¹ì§•ì„ í•´ë‹¹ ê°ì²´ ì¿¼ë¦¬ì— ë”í•˜ì—¬ ë‹¤ìŒ ë ˆì´ì–´ë¥¼ êµ¬ì„±í•˜ê²Œ ë©ë‹ˆë‹¤.

    output = output.permute(2, 0, 1) // outputì˜ ì°¨ì› ìˆœì„œë¥¼ ë³€ê²½í•˜ì—¬ ìƒˆë¡œìš´ í˜•íƒœë¡œ ë³€í™˜
    // ìƒˆë¡œìš´ ì°¨ì›: (num_query, B, C)

    output = self.output_proj(output) // outputì— self.output_projë¥¼ ì ìš©í•˜ì—¬ ì°¨ì› ë³€í™˜
    // ìƒˆë¡œìš´ ì°¨ì›: (num_query, B, embed_dims)

    // (num_query, bs, embed_dims)
    pos_feat = self.position_encoder(inverse_sigmoid(reference_points_3d)).permute(1, 0, 2) // reference_points_3dë¥¼ ì—­ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìœ„ì¹˜ ì¸ì½”ë”© ê³„ì‚°í•˜ê³  ì°¨ì› ë³€í™˜
    // ìƒˆë¡œìš´ ì°¨ì›: (num_query, bs, embed_dims)

    // ìµœì¢… output ì°¨ì›: (num_query, B, embed_dims)
    return self.dropout(output) + inp_residual + pos_feat // self.dropoutì„ ì ìš©í•œ outputê³¼ inp_residual, pos_featë¥¼ ë”í•˜ì—¬ ìµœì¢… ê²°ê³¼ ë°˜í™˜
    // ìµœì¢… ì°¨ì›: (num_query, B, embed_dims)

ì´í›„ return ë¶€ë¶„ì—ì„œ 3D ê°ì²´ ì¿¼ë¦¬ì˜ ìœ„ì¹˜ ì¸ì½”ë”© `pos_feat`ê³¼ 2D ì´ë¯¸ì§€ íŠ¹ì§• `output`, ê·¸ë¦¬ê³  ì”ì°¨ `inp_residual`ì„ ë”í•´ì„œ CrossAttentionì˜ ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.

****

4) DETRì²˜ëŸ¼ Multi-Head Attentionì„ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ Object ê°„ì˜ Interactionì„ í•™ìŠµí•©ë‹ˆë‹¤.

****

5) ê° Object Queryë¥¼ Bbox Regressionê³¼ Classificationì„ ìœ„í•œ 2ê°œì˜ MLPì˜ ì…ë ¥ìœ¼ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.

****

6) ëª¨ë¸ ì˜ˆì¸¡ìœ¼ë¡œ íšë“í•œ Class ì˜ˆì¸¡ê³¼ Bbox ì˜ˆì¸¡ì„ ì •ë‹µ ë ˆì´ë¸”(GT)ì™€ ë¹„êµí•˜ì—¬ Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´í›„, ì´ Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ Bboxì˜ Center$$(x,y,z)$$ê³¼ í¬ê¸°$$(w,h,d)$$ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

## Loss
ì•ì„œ ì„¤ëª…í–ˆë“¯ set-to-set lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ëŠ” class labelì„ ìœ„í•œ focal lossì™€ bbox parameterë¥¼ ìœ„í•œ L1 lossë¡œ êµ¬ì„±ì´ ë©ë‹ˆë‹¤.

DETRì˜ Hungarian Lossì™€ ë™ì¼í•©ë‹ˆë‹¤ (ì•„ë˜ í¬ìŠ¤íŠ¸ ì°¸ì¡°í•´ì£¼ì„¸ìš”).

[DETR](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).


****
# Experiment ğŸ‘€
## ê¸°ì¡´ ë°©ì‹ë“¤ê³¼ì˜ ë¹„êµ Table
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/16b84f27-a3ef-42e3-887e-b900dc68af8d)

- ê¸°ì¡´ì˜ monocular 3D object detectionì€ multi-viewì— ì ìš©í•˜ë ¤ë©´ ê°ê°ì˜ viewì— ë”°ë¡œ ì§„í–‰í•´ì•¼ í•˜ëŠ” ë‹¨ì ì´ ì¡´ì¬í•˜ì§€ë§Œ, DETR3Dì€ multi-viewë¥¼ í•œ ë²ˆì— ì‚¬ìš©í•˜ì—¬ ë§¤ìš° íš¨ìœ¨ì ì…ë‹ˆë‹¤.

## Multi-View Cameraì—ì„œ ê²¹ì¹˜ëŠ” ì˜ì—­ì— ëŒ€í•œ ë¹„êµ Table

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/bab80cee-ce15-435d-bb7c-8a6145ee6e44)

- DETR3DëŠ” Multi-Viewë¥¼ í•œë²ˆì— ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ëŸ¬ Viewì— ëŒ€í•´ ì¤‘ë³µë˜ì–´ Objectê°€ ì˜ë¦¬ëŠ” í˜„ìƒì— ëŒ€í•´ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## Pseudo-LiDAR ë¹„êµ Table
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/9106e323-d870-427f-a9c6-522ae6bb6315)

ë§¤ìš° í° ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ëŠ”ë°, Pseudo-LiDARê°€ ê²ªëŠ” compounding errorë¥¼ DETR3DëŠ” ê²ªì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

## Detection Head Layer Refinement ì‹¤í—˜
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/36aa8a1f-49e6-4026-a12a-8dcd6c9b92e4)

- Layerê°€ ê¹Šì–´ì§ì— ë”°ë¼ ì˜ˆì¸¡ë˜ëŠ” bboxê°€ Ground Truthì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆê³  *Table 5*ë¥¼ ë´ë„ NDSê°€ layerê°€ ê¹Šì–´ì§ì— ë”°ë¼ ì ì  ì˜¤ë¥´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Object Query ê°œìˆ˜ ì‹¤í—˜
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/59061a06-419e-45a6-93b1-91d2022a1391)

ê²°ê³¼ì ìœ¼ë¡œ $$900$$ê°œì—ì„œ saturate(í¬í™”) ë˜ëŠ” ëª¨ìŠµì´ë©°, ì´ë¥¼ í†µí•´ ì ë‹¹í•œ ì–‘ì˜ ê°ì²´ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

****
# Open Reivew ğŸ’—
- Limited ablations.
- However, I do not understand why they compared against FCOS3D without fine-tuning.
- DETR ëª¨ë¸ ì„¤ëª… ë¬´ì¡±.

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
- Camera Transformation Matrix.

****
# Conclusion âœ¨
## Strength
- <span style="color:lightgreen">**End-to-End í•™ìŠµ**</span>: DETR-3DëŠ” ì´ë¯¸ì§€ì™€ 3D ê°ì²´ ì •ë³´ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ End-to-Endë¡œ í•™ìŠµë˜ëŠ” ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ë³„ë„ì˜ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ ì—†ì´ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ ê°ì²´ íƒì§€ì™€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- <span style="color:lightgreen"> **3D ê°ì²´ ì¿¼ë¦¬ ì‚¬ìš©**</span>: 3D ê°ì²´ ì¿¼ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê°ì²´ì˜ ìœ„ì¹˜, í¬ê¸° ë° ë°©í–¥ ì •ë³´ë¥¼ í‘œí˜„í•¨ìœ¼ë¡œì¨, 3D ê³µê°„ì—ì„œì˜ ê°ì²´ íƒì§€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ 2D íƒì§€ ëª¨ë¸ë³´ë‹¤ ë” ì •í™•í•œ 3D ê°ì²´ íƒì§€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- <span style="color:lightgreen">**NMS ì œì•½ ì—†ìŒ**</span>.
- <span style="color:lightgreen">**ë³„ë„ì˜ Depth Estimatino Network í•„ìš” ì—†ìŒ**</span>.

## Weakness
- <span style="color:orange"> **ì„±ëŠ¥ ëŒ€ë¹„ ë†’ì€ ê³„ì‚° ë¹„ìš©** </span>: DETR-3DëŠ” ê°ì²´ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë³µì¡í•œ êµ¬ì¡°ë¡œ ì¸í•´ ë‹¤ë¥¸ 2D ê°ì²´ íƒì§€ ëª¨ë¸ë³´ë‹¤ ë†’ì€ ê³„ì‚° ë¹„ìš©ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- <span style="color:orange"> **ì¼ë¶€ ì¸¡ë©´ì—ì„œ ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµì  ì„±ëŠ¥ ì°¨ì´** </span>: ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´ ì¼ë¶€ ê²½ìš°ì— FCOS3Dë‚˜ CenterNetê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì´ ë” ë‚®ê²Œ ë‚˜ì˜¨ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶€ë¶„ì— ëŒ€í•œ ì„±ëŠ¥ í–¥ìƒì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

****
# Reference
NA