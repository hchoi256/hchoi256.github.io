---
layout: single
title: "[논문 분석] DETR3D (CoRL 2021)"
categories: AIPaperCV
tag: [Computer Vision, Camera-based 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/3d-od-cam.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/pdf/2110.06922.pdf)

<!-- <span style="color:blue"> ???? </span> -->

****
# 한줄요약 ✔
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d0a480f2-ee7a-477a-9740-781f7e5a70f8)

- **Camera-based** Object Detection입니다.
    - 다중 뷰(multi-view) 2D 이미지를 활용하여 3D 공간에서 정의된 객체 쿼리들의 정보를 추론합니다.
- **이미지 특징 추출**: ResNet과 (multi-scale)FPN을 사용해서 다중 뷰 입력 이미지들로 부터 multi-scale 2D 특징들을 추출합니다.
- **2D-to-3D Feature Transformation**: 3D 공간의 reference point에 대한 다중 뷰 피처맵들에서의 특징을 bilinear interpolation을 통해 보간하고, 그들의 평균값을 계산합니다.
- **Loss**: 기존 DETR과 동일한 Hungarian Loss를 따릅니다.
- nuScenes dataset을 이용해서 성능을 평가하였고 **SOTA 성능**을 냈습니다.

****
# Preliminaries 🍱
## Feature Pyramid Network (FPN)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b9d2f6e9-7bec-4d3f-adeb-c4c54d74baa0)

Feature Pyramid Network (FPN)은 이미지에서 다양한 스케일의 객체를 탐지하고 분할하는데 사용되는 딥러닝 네트워크 구조입니다.

FPN은 **하위 레이어**의 높은 해상도의 정보와 **높은 레이어**의 추상적 정보를 동시에 활용하여 객체 탐지 및 분할 작업의 정확도를 향상시키는데 주로 사용됩니다.

FPN은 두 가지 주요 구성 요소인 **Bottom-up**과 **Top-down**의 조합으로 이루어집니다. 아래는 각 구성 요소의 역할에 대한 설명입니다.

### Bottom-up
Bottom-up은 일반적인 CNN(Convolutional Neural Network) 기반의 네트워크입니다.

높은 해상도의 입력 이미지를 저수준의 특징 맵(Feature Map)으로 변환하는 역할을 합니다.

예를 들어, ResNet과 같은 기존의 CNN 아키텍처를 Bottom-up으로 사용할 수 있습니다.

### Top-down
Top-down은 Bottom-up으로부터 생성된 저수준의 특징 맵을 이용하여 고수준의 특징 맵을 생성하는 역할을 합니다

즉, Bottom-up으로부터 나온 낮은 해상도의 특징 맵들을 Upsampling한 후, 다른 높은 해상도의 특징 맵들과 결합하여 저수준의 특징과 고수준의 특징을 모두 포함하는 높은 해상도의 특징 맵을 얻어냅니다.

## Camera Parameters
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/51be0e1a-1959-4119-8a6a-f04a76462bbc)

Camera Parameter는 World Coordinate에 있는 3D Voxel $$(U,V,W)$$를 Image Coordinate의 2D Pixel $$(x,y)$$로 변환하기 위해 사용됩니다.

이때 3차원의 점 $$(U,V,W)$$는 **homogeneous counterpart**로써 새로운 차원에 $$1$$ 값을 새로 추가하여 $$3D \rightarrow 2D$$ 차원 변환 과정을 용이하게 한다.
- 2D 차원으로의 투영 행렬의 shape이 $$(3,4)$$이기 때문에 $$3$$차원의 점은 $$4$$차원으로 맞춰줄 필요가 있다.
- 새로운 차원의 값을 $$1$$로 설정함으로써 해당 차원 값은 투영 행렬에 의해 **depth(깊이)**가 된다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/393f3c48-615f-47d8-a6d8-3caf3a19e933)

- $$K$$: 카메라 내부 파라미터(intrinsic parameter).
- $$[R | t]$$: 카메라 외부 파라미터(extrinsic parameter).
- $$A[R|t]$$: camera matrix 또는 projection matrix (투영 행렬).

### Extrinsic Parameter
카메라 좌표계(2D)와 월드 좌표계(3D) 사이의 변환 관계를 설명하는 파라미터로서 두 좌표계 사이의 **회전(rotation)** 및 **평행이동(translation) 변환**으로 표현합니다.

### Intrinsic Parameter
#### 1) Focal Length(초점거리): $$fx, fy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2d9556ec-561e-47ce-97dd-7a18ad22181c)

$$fx$$와 $$fy$$는 카메라의 렌즈로부터 이미지 평면까지의 거리로, 초점 거리를 나타내는 파라미터입니다.

$$fx$$는 $$x$$축 방향의 초점 거리를, $$fy$$는 $$y$$축 방향의 초점 거리를 의미합니다.

초점 거리는 렌즈의 굴절력과 관련되어 객체의 크기와 카메라 시점에 영향을 줍니다.

#### 2) principal point(주점): $$cx, cy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/44fce7f7-2136-4a45-b73e-4910986bf8b0)

$$cx$$와 $$cy$$는 이미지 평면 상에서 주점의 좌표를 나타내는 파라미터로, 이미지의 **중심점**을 의미합니다.

주점은 카메라의 광학 축과 이미지 평면의 교차점으로, 카메라 시점과 객체의 상대적인 위치를 나타내는 중요한 정보입니다.​

#### 3) skew coefficient(비대칭계수): $$skew_c = tan \alpha$$
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6b9aa1cf-7f0d-4ed7-bee4-2f909977a53e)

$$skew_c$$는 카메라의 렌즈가 정사각형이 아닌 비대칭인 경우 $$y$$축이 기울어진 정도(비대칭 정도)를 나타내는 파라미터로, 일반적으로 $$tan(\alpha)$$의 형태로 표현됩니다.

비대칭 계수는 주로 카메라의 렌즈 또는 이미지 센서의 특성과 관련되며, 이를 고려하여 이미지 변환 및 보정 작업에 사용될 수 있습니다.

## DETR 모델
![image](https://user-images.githubusercontent.com/39285147/197422990-0d50e9ab-0866-40d2-9940-ff3ffb91fdde.png)

논문 블로그 포스트 [링크](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).

****
# Problem Definition ✏
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main Idea💣
**C1)** <span style="color:orange"> 기존 모델들은 depth estimation network에 의존하여 낮은 quality의 estimated depth에 대해 3D detection 성능에 해로운 **compounding error**를 겪을 수 있습니다. </span>


**Idea 1)** <span style="color:lightgreen"> DETR3D는 depth estimation network를 사용하지 않고 **Camera Transformation Matrix**을 사용하여 다중 뷰로 부터의 2D Feature Extraction과 3D Object Prediction을 연결하여 이후 depth. </span>

**C2)** <span style="color:orange"> 기존 모델들은 후처리 과정인 **NMS**에 의존하게 됩니다.</span>

**Idea 2)** <span style="color:lightgreen"> DETR3D는 End-to-End 학습을 하는 DETR 구조를 따르기 때문에 후처리 과정이 필요가 없습니다. </span>

****
# Proposed Method 🧿
## Feature Learning
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/086244df-4a27-48d8-8f1b-6956ccaf8a60)


## Detection Head (Main Contribution)

## Loss
DETR의 Hungarian Loss와 동일합니다 (아래 포스트 참조해주세요).

[DETR](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).


****
# Experiment 👀

****
# Open Reivew 💗

****
# Discussion 🍟

****
# Major Takeaways 😃

****
# Conclusion ✨
## Strength
## Weakness

****
# Reference