---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] DETR3D (CoRL 2021)"
categories: AIPaperCV
tag: [Computer Vision, Camera-based 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/3d-od-cam.png
sidebar:
    nav: "docs"
---

[**ë…¼ë¬¸**](https://arxiv.org/pdf/2110.06922.pdf)

<!-- <span style="color:blue"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d0a480f2-ee7a-477a-9740-781f7e5a70f8)

- **Camera-based** Object Detectionì…ë‹ˆë‹¤.
    - ë‹¤ì¤‘ ë·°(multi-view) 2D ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ 3D ê³µê°„ì—ì„œ ì •ì˜ëœ ê°ì²´ ì¿¼ë¦¬ë“¤ì˜ ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
- **ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ**: ResNetê³¼ (multi-scale)FPNì„ ì‚¬ìš©í•´ì„œ ë‹¤ì¤‘ ë·° ì…ë ¥ ì´ë¯¸ì§€ë“¤ë¡œ ë¶€í„° multi-scale 2D íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
- **2D-to-3D Feature Transformation**: 3D ê³µê°„ì˜ reference pointì— ëŒ€í•œ ë‹¤ì¤‘ ë·° í”¼ì²˜ë§µë“¤ì—ì„œì˜ íŠ¹ì§•ì„ bilinear interpolationì„ í†µí•´ ë³´ê°„í•˜ê³ , ê·¸ë“¤ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
- **Loss**: ê¸°ì¡´ DETRê³¼ ë™ì¼í•œ Hungarian Lossë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- nuScenes datasetì„ ì´ìš©í•´ì„œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ê³  **SOTA ì„±ëŠ¥**ì„ ëƒˆìŠµë‹ˆë‹¤.

****
# Preliminaries ğŸ±
## Feature Pyramid Network (FPN)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b9d2f6e9-7bec-4d3f-adeb-c4c54d74baa0)

Feature Pyramid Network (FPN)ì€ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ê°ì²´ë¥¼ íƒì§€í•˜ê³  ë¶„í• í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì…ë‹ˆë‹¤.

FPNì€ **í•˜ìœ„ ë ˆì´ì–´**ì˜ ë†’ì€ í•´ìƒë„ì˜ ì •ë³´ì™€ **ë†’ì€ ë ˆì´ì–´**ì˜ ì¶”ìƒì  ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ ê°ì²´ íƒì§€ ë° ë¶„í•  ì‘ì—…ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

FPNì€ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œì¸ **Bottom-up**ê³¼ **Top-down**ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì•„ë˜ëŠ” ê° êµ¬ì„± ìš”ì†Œì˜ ì—­í• ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.

### Bottom-up
Bottom-upì€ ì¼ë°˜ì ì¸ CNN(Convolutional Neural Network) ê¸°ë°˜ì˜ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.

ë†’ì€ í•´ìƒë„ì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µ(Feature Map)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ResNetê³¼ ê°™ì€ ê¸°ì¡´ì˜ CNN ì•„í‚¤í…ì²˜ë¥¼ Bottom-upìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Top-down
Top-downì€ Bottom-upìœ¼ë¡œë¶€í„° ìƒì„±ëœ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ì´ìš©í•˜ì—¬ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤

ì¦‰, Bottom-upìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ë‚®ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ì„ Upsamplingí•œ í›„, ë‹¤ë¥¸ ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ê³¼ ê²°í•©í•˜ì—¬ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§•ê³¼ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µì„ ì–»ì–´ëƒ…ë‹ˆë‹¤.

## Camera Parameters
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/51be0e1a-1959-4119-8a6a-f04a76462bbc)

Camera ParameterëŠ” World Coordinateì— ìˆëŠ” 3D Voxel $$(U,V,W)$$ë¥¼ Image Coordinateì˜ 2D Pixel $$(x,y)$$ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì´ë•Œ 3ì°¨ì›ì˜ ì  $$(U,V,W)$$ëŠ” **homogeneous counterpart**ë¡œì¨ ìƒˆë¡œìš´ ì°¨ì›ì— $$1$$ ê°’ì„ ìƒˆë¡œ ì¶”ê°€í•˜ì—¬ $$3D \rightarrow 2D$$ ì°¨ì› ë³€í™˜ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•œë‹¤.
- 2D ì°¨ì›ìœ¼ë¡œì˜ íˆ¬ì˜ í–‰ë ¬ì˜ shapeì´ $$(3,4)$$ì´ê¸° ë•Œë¬¸ì— $$3$$ì°¨ì›ì˜ ì ì€ $$4$$ì°¨ì›ìœ¼ë¡œ ë§ì¶°ì¤„ í•„ìš”ê°€ ìˆë‹¤.
- ìƒˆë¡œìš´ ì°¨ì›ì˜ ê°’ì„ $$1$$ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì¨ í•´ë‹¹ ì°¨ì› ê°’ì€ íˆ¬ì˜ í–‰ë ¬ì— ì˜í•´ **depth(ê¹Šì´)**ê°€ ëœë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/393f3c48-615f-47d8-a6d8-3caf3a19e933)

- $$K$$: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(intrinsic parameter).
- $$[R | t]$$: ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„°(extrinsic parameter).
- $$A[R|t]$$: camera matrix ë˜ëŠ” projection matrix (íˆ¬ì˜ í–‰ë ¬).

### Extrinsic Parameter
ì¹´ë©”ë¼ ì¢Œí‘œê³„(2D)ì™€ ì›”ë“œ ì¢Œí‘œê³„(3D) ì‚¬ì´ì˜ ë³€í™˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œì„œ ë‘ ì¢Œí‘œê³„ ì‚¬ì´ì˜ **íšŒì „(rotation)** ë° **í‰í–‰ì´ë™(translation) ë³€í™˜**ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

### Intrinsic Parameter
#### 1) Focal Length(ì´ˆì ê±°ë¦¬): $$fx, fy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2d9556ec-561e-47ce-97dd-7a18ad22181c)

$$fx$$ì™€ $$fy$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ í‰ë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¡œ, ì´ˆì  ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

$$fx$$ëŠ” $$x$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼, $$fy$$ëŠ” $$y$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì´ˆì  ê±°ë¦¬ëŠ” ë Œì¦ˆì˜ êµ´ì ˆë ¥ê³¼ ê´€ë ¨ë˜ì–´ ê°ì²´ì˜ í¬ê¸°ì™€ ì¹´ë©”ë¼ ì‹œì ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.

#### 2) principal point(ì£¼ì ): $$cx, cy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/44fce7f7-2136-4a45-b73e-4910986bf8b0)

$$cx$$ì™€ $$cy$$ëŠ” ì´ë¯¸ì§€ í‰ë©´ ìƒì—ì„œ ì£¼ì ì˜ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì´ë¯¸ì§€ì˜ **ì¤‘ì‹¬ì **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì£¼ì ì€ ì¹´ë©”ë¼ì˜ ê´‘í•™ ì¶•ê³¼ ì´ë¯¸ì§€ í‰ë©´ì˜ êµì°¨ì ìœ¼ë¡œ, ì¹´ë©”ë¼ ì‹œì ê³¼ ê°ì²´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ ì •ë³´ì…ë‹ˆë‹¤.â€‹

#### 3) skew coefficient(ë¹„ëŒ€ì¹­ê³„ìˆ˜): $$skew_c = tan \alpha$$
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6b9aa1cf-7f0d-4ed7-bee4-2f909977a53e)

$$skew_c$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆê°€ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ë¹„ëŒ€ì¹­ì¸ ê²½ìš° $$y$$ì¶•ì´ ê¸°ìš¸ì–´ì§„ ì •ë„(ë¹„ëŒ€ì¹­ ì •ë„)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ $$tan(\alpha)$$ì˜ í˜•íƒœë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

ë¹„ëŒ€ì¹­ ê³„ìˆ˜ëŠ” ì£¼ë¡œ ì¹´ë©”ë¼ì˜ ë Œì¦ˆ ë˜ëŠ” ì´ë¯¸ì§€ ì„¼ì„œì˜ íŠ¹ì„±ê³¼ ê´€ë ¨ë˜ë©°, ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ ë³€í™˜ ë° ë³´ì • ì‘ì—…ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## DETR ëª¨ë¸
![image](https://user-images.githubusercontent.com/39285147/197422990-0d50e9ab-0866-40d2-9940-ff3ffb91fdde.png)

ë…¼ë¬¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ [ë§í¬](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).

****
# Problem Definition âœ
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ depth estimation networkì— ì˜ì¡´í•˜ì—¬ ë‚®ì€ qualityì˜ estimated depthì— ëŒ€í•´ 3D detection ì„±ëŠ¥ì— í•´ë¡œìš´ **compounding error**ë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. </span>


**Idea 1)** <span style="color:lightgreen"> DETR3DëŠ” depth estimation networkë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  **Camera Transformation Matrix**ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë·°ë¡œ ë¶€í„°ì˜ 2D Feature Extractionê³¼ 3D Object Predictionì„ ì—°ê²°í•˜ì—¬ ì´í›„ depth. </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ í›„ì²˜ë¦¬ ê³¼ì •ì¸ **NMS**ì— ì˜ì¡´í•˜ê²Œ ë©ë‹ˆë‹¤.</span>

**Idea 2)** <span style="color:lightgreen"> DETR3DëŠ” End-to-End í•™ìŠµì„ í•˜ëŠ” DETR êµ¬ì¡°ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì— í›„ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. </span>

****
# Proposed Method ğŸ§¿
## Feature Learning
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/086244df-4a27-48d8-8f1b-6956ccaf8a60)


## Detection Head (Main Contribution)

## Loss
DETRì˜ Hungarian Lossì™€ ë™ì¼í•©ë‹ˆë‹¤ (ì•„ë˜ í¬ìŠ¤íŠ¸ ì°¸ì¡°í•´ì£¼ì„¸ìš”).

[DETR](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).


****
# Experiment ğŸ‘€

****
# Open Reivew ğŸ’—

****
# Discussion ğŸŸ

****
# Major Takeaways ğŸ˜ƒ

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference