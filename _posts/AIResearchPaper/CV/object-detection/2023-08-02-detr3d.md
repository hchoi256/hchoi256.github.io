---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] DETR3D (CoRL 2021)"
categories: AIPaperCV
tag: [Computer Vision, Camera-based 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/3d-od-cam.png
sidebar:
    nav: "docs"
---

[**ë…¼ë¬¸**](https://arxiv.org/pdf/2110.06922.pdf)

<!-- <span style="color:blue"> ???? </span> -->

****
# í•œì¤„ìš”ì•½ âœ”
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d0a480f2-ee7a-477a-9740-781f7e5a70f8)

- **Camera-based** Object Detectionì…ë‹ˆë‹¤.
    - ë‹¤ì¤‘ ë·°(multi-view) 2D ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ 3D ê³µê°„ì—ì„œ ì •ì˜ëœ ê°ì²´ ì¿¼ë¦¬ë“¤ì˜ ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
- **ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ**: ResNetê³¼ (multi-scale)FPNì„ ì‚¬ìš©í•´ì„œ ë‹¤ì¤‘ ë·° ì…ë ¥ ì´ë¯¸ì§€ë“¤ë¡œ ë¶€í„° multi-scale 2D íŠ¹ì§•ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
- **2D-to-3D Feature Transformation**: 3D ê³µê°„ì˜ reference pointì— ëŒ€í•œ ë‹¤ì¤‘ ë·° í”¼ì²˜ë§µë“¤ì—ì„œì˜ íŠ¹ì§•ì„ bilinear interpolationì„ í†µí•´ ë³´ê°„í•˜ê³ , ê·¸ë“¤ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
- **Loss**: ê¸°ì¡´ DETRê³¼ ë™ì¼í•œ Hungarian Lossë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- nuScenes datasetì„ ì´ìš©í•´ì„œ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ê³  **SOTA ì„±ëŠ¥**ì„ ëƒˆìŠµë‹ˆë‹¤.

****
# Preliminaries ğŸ±
## Feature Pyramid Network (FPN)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b9d2f6e9-7bec-4d3f-adeb-c4c54d74baa0)

Feature Pyramid Network (FPN)ì€ ì´ë¯¸ì§€ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ê°ì²´ë¥¼ íƒì§€í•˜ê³  ë¶„í• í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì…ë‹ˆë‹¤.

FPNì€ **í•˜ìœ„ ë ˆì´ì–´**ì˜ ë†’ì€ í•´ìƒë„ì˜ ì •ë³´ì™€ **ë†’ì€ ë ˆì´ì–´**ì˜ ì¶”ìƒì  ì •ë³´ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ ê°ì²´ íƒì§€ ë° ë¶„í•  ì‘ì—…ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ë° ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

FPNì€ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œì¸ **Bottom-up**ê³¼ **Top-down**ì˜ ì¡°í•©ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì•„ë˜ëŠ” ê° êµ¬ì„± ìš”ì†Œì˜ ì—­í• ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.

### Bottom-up
Bottom-upì€ ì¼ë°˜ì ì¸ CNN(Convolutional Neural Network) ê¸°ë°˜ì˜ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.

ë†’ì€ í•´ìƒë„ì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µ(Feature Map)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ResNetê³¼ ê°™ì€ ê¸°ì¡´ì˜ CNN ì•„í‚¤í…ì²˜ë¥¼ Bottom-upìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Top-down
Top-downì€ Bottom-upìœ¼ë¡œë¶€í„° ìƒì„±ëœ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ì´ìš©í•˜ì—¬ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§• ë§µì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤

ì¦‰, Bottom-upìœ¼ë¡œë¶€í„° ë‚˜ì˜¨ ë‚®ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ì„ Upsamplingí•œ í›„, ë‹¤ë¥¸ ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µë“¤ê³¼ ê²°í•©í•˜ì—¬ ì €ìˆ˜ì¤€ì˜ íŠ¹ì§•ê³¼ ê³ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ë†’ì€ í•´ìƒë„ì˜ íŠ¹ì§• ë§µì„ ì–»ì–´ëƒ…ë‹ˆë‹¤.

## Bilinear Interpolation
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/7a9480e3-3fa2-45b5-bf7c-ae716aaa24d9)

Bilinear interpolationì€ ì£¼ì–´ì§„ ë‘ ì ì˜ ì‚¬ì´ì— ìœ„ì¹˜í•˜ëŠ” ê°’ë“¤ì„ ì¶”ì •í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë³´ê°„ ê¸°ë²•ìœ¼ë¡œ, ë‘ ì  ì‚¬ì´ì˜ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê°’ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

## Camera Parameters
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/51be0e1a-1959-4119-8a6a-f04a76462bbc)

Camera ParameterëŠ” World Coordinateì— ìˆëŠ” 3D Voxel $$(U,V,W)$$ë¥¼ Image Coordinateì˜ 2D Pixel $$(x,y)$$ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì´ë•Œ 3ì°¨ì›ì˜ ì  $$(U,V,W)$$ëŠ” **homogeneous counterpart**ë¡œì¨ ìƒˆë¡œìš´ ì°¨ì›ì— $$1$$ ê°’ì„ ìƒˆë¡œ ì¶”ê°€í•˜ì—¬ $$3D \rightarrow 2D$$ ì°¨ì› ë³€í™˜ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•œë‹¤.
- 2D ì°¨ì›ìœ¼ë¡œì˜ íˆ¬ì˜ í–‰ë ¬ì˜ shapeì´ $$(3,4)$$ì´ê¸° ë•Œë¬¸ì— $$3$$ì°¨ì›ì˜ ì ì€ $$4$$ì°¨ì›ìœ¼ë¡œ ë§ì¶°ì¤„ í•„ìš”ê°€ ìˆë‹¤.
- ìƒˆë¡œìš´ ì°¨ì›ì˜ ê°’ì„ $$1$$ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì¨ í•´ë‹¹ ì°¨ì› ê°’ì€ íˆ¬ì˜ í–‰ë ¬ì— ì˜í•´ **depth(ê¹Šì´)**ê°€ ëœë‹¤.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/393f3c48-615f-47d8-a6d8-3caf3a19e933)

- $$K$$: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(intrinsic parameter).
- $$[R | t]$$: ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„°(extrinsic parameter).
- $$A[R|t]$$: camera matrix ë˜ëŠ” projection matrix (íˆ¬ì˜ í–‰ë ¬).

### Extrinsic Parameter
ì¹´ë©”ë¼ ì¢Œí‘œê³„(2D)ì™€ ì›”ë“œ ì¢Œí‘œê³„(3D) ì‚¬ì´ì˜ ë³€í™˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œì„œ ë‘ ì¢Œí‘œê³„ ì‚¬ì´ì˜ **íšŒì „(rotation)** ë° **í‰í–‰ì´ë™(translation) ë³€í™˜**ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

### Intrinsic Parameter
#### 1) Focal Length(ì´ˆì ê±°ë¦¬): $$fx, fy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2d9556ec-561e-47ce-97dd-7a18ad22181c)

$$fx$$ì™€ $$fy$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ í‰ë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¡œ, ì´ˆì  ê±°ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

$$fx$$ëŠ” $$x$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼, $$fy$$ëŠ” $$y$$ì¶• ë°©í–¥ì˜ ì´ˆì  ê±°ë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì´ˆì  ê±°ë¦¬ëŠ” ë Œì¦ˆì˜ êµ´ì ˆë ¥ê³¼ ê´€ë ¨ë˜ì–´ ê°ì²´ì˜ í¬ê¸°ì™€ ì¹´ë©”ë¼ ì‹œì ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.

#### 2) principal point(ì£¼ì ): $$cx, cy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/44fce7f7-2136-4a45-b73e-4910986bf8b0)

$$cx$$ì™€ $$cy$$ëŠ” ì´ë¯¸ì§€ í‰ë©´ ìƒì—ì„œ ì£¼ì ì˜ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì´ë¯¸ì§€ì˜ **ì¤‘ì‹¬ì **ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì£¼ì ì€ ì¹´ë©”ë¼ì˜ ê´‘í•™ ì¶•ê³¼ ì´ë¯¸ì§€ í‰ë©´ì˜ êµì°¨ì ìœ¼ë¡œ, ì¹´ë©”ë¼ ì‹œì ê³¼ ê°ì²´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ ì •ë³´ì…ë‹ˆë‹¤.â€‹

#### 3) skew coefficient(ë¹„ëŒ€ì¹­ê³„ìˆ˜): $$skew_c = tan \alpha$$
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6b9aa1cf-7f0d-4ed7-bee4-2f909977a53e)

$$skew_c$$ëŠ” ì¹´ë©”ë¼ì˜ ë Œì¦ˆê°€ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ë¹„ëŒ€ì¹­ì¸ ê²½ìš° $$y$$ì¶•ì´ ê¸°ìš¸ì–´ì§„ ì •ë„(ë¹„ëŒ€ì¹­ ì •ë„)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ $$tan(\alpha)$$ì˜ í˜•íƒœë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

ë¹„ëŒ€ì¹­ ê³„ìˆ˜ëŠ” ì£¼ë¡œ ì¹´ë©”ë¼ì˜ ë Œì¦ˆ ë˜ëŠ” ì´ë¯¸ì§€ ì„¼ì„œì˜ íŠ¹ì„±ê³¼ ê´€ë ¨ë˜ë©°, ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ ë³€í™˜ ë° ë³´ì • ì‘ì—…ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## DETR ëª¨ë¸
![image](https://user-images.githubusercontent.com/39285147/197422990-0d50e9ab-0866-40d2-9940-ff3ffb91fdde.png)

ë…¼ë¬¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ [ë§í¬](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).

****
# Problem Definition âœ
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ depth estimation networkì— ì˜ì¡´í•˜ì—¬ ë‚®ì€ qualityì˜ estimated depthì— ëŒ€í•´ 3D detection ì„±ëŠ¥ì— í•´ë¡œìš´ **compounding error**ë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. </span>


**Idea 1)** <span style="color:lightgreen"> DETR3DëŠ” depth estimation networkë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  **Camera Transformation Matrix**ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì¤‘ ë·°ë¡œ ë¶€í„°ì˜ 2D Feature Extractionê³¼ 3D Object Predictionì„ ì—°ê²°í•˜ì—¬ ì´í›„ depth. </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ë“¤ì€ í›„ì²˜ë¦¬ ê³¼ì •ì¸ **NMS**ì— ì˜ì¡´í•˜ê²Œ ë©ë‹ˆë‹¤.</span>

**Idea 2)** <span style="color:lightgreen"> DETR3DëŠ” End-to-End í•™ìŠµì„ í•˜ëŠ” DETR êµ¬ì¡°ë¥¼ ë”°ë¥´ê¸° ë•Œë¬¸ì— í›„ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. </span>

****
# Proposed Method ğŸ§¿
## Data Format
- $$\mathcal{L}=\{im_1,...,im_L\} \subset \mathbb{R}^{H_{im} \times W_{im} \times 3}$$: Multi-view Camera Image.
- $$\mathcal{T}=\{T_1,...,T_K\} \subset \mathbb{R}^{3 \times 4}$$: Camera Transformation Matrix.
    - intrinsic & extrinsic parameterë¥¼ í†µí•´ ë§Œë“œëŠ” í–‰ë ¬ì…ë‹ˆë‹¤ [ì—¬ê¸°](#camera-parameters).
- $$\mathcal{B}=\{b_1,...,b_j,...,b_M\} \subset \mathbb{R}^9$$: GT Bbox; Each $$b_j$$ contains (position, size, heading angle, velocity) in the birds-eye view (BEV).
    - Position: $$(x,y,z)$$.
    - Size: $$(w,h,d)$$.
    - Heading Angle: $$\theta$$.
    - Velocity: $$\mathcal{v}$$.
- $$\mathcal{C}=\{c_1,...,c_j,...,c_M\} \subset \mathbb{Z}$$: categorical labels.
- $$\mathcal{F}_k=\{f_{k1},...,f_{k6}\} \subset \mathbb{R}^{H \times W \times C}$$: a level of features of the 6 images.
    - $$f_{ki}$$: $$k$$ë²ˆì§¸ í¬ê¸° levelì˜ $$i$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·° í”¼ì²˜ë§µì…ë‹ˆë‹¤.

> NuScenes ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë‹¤ì¤‘ ë·° ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ ê°ê° ì´ 6ê°œì˜ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## Feature Learning
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/086244df-4a27-48d8-8f1b-6956ccaf8a60)

ResNetê³¼ [FPN](#feature-pyramid-network-fpn)ì„ ì‚¬ìš©í•˜ì—¬ì„œ Multi-Scaleì—ì„œì˜ Featureë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
- Multi-Scale FPN: ì„œë¡œ ë‹¤ë¥¸ sizeì˜ objectë¥¼ detection í•˜ëŠ”ë° í•„ìš”í•œ í’ë¶€í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ $$6$$ê°œì˜ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ë§ˆë‹¤ ì´ $$K$$ê°œ ë§Œí¼ì˜ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ í”¼ì²˜ë§µ levelì„ ì–»ê³ , ì´ $$6 \times K$$ê°œì˜ í”¼ì²˜ë§µì„ íšë“í•©ë‹ˆë‹¤.

## Detection Head (Main Contribution)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a164ac5a-c859-4ec6-94e7-1163df4b05e3)

ê¸°ì¡´ Bottom-Up ë°©ì‹ë“¤ê³¼ëŠ” ë‹¬ë¦¬, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **Top-down ë°©ì‹**ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ NMSì™€ depth estimationìœ¼ë¡œ ë¶€í„° í•´ë°©ë©ë‹ˆë‹¤.

> Bottom-Up ë°©ì‹
>
>> Imageë‹¹ ë§ì€ bboxë¥¼ ì˜ˆì¸¡í•˜ê³  ë¶ˆí•„ìš”í•œ bboxë¥¼ NMSì™€ ê°™ì€ post-processingì„ í†µí•˜ì—¬ ê±¸ëŸ¬ì¤€ í›„, ê°ê°ì˜ viewì—ì„œ ì–»ì€ ê²°ê³¼ë¥¼ í•©ì³ì£¼ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

1. Object Queryë¥¼ Neural Network$$(\Phi^{ref})$$ì— í†µê³¼ì‹œì¼œ Bbox Centerì˜ ì§‘í•©ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

$$C_{li}=\Phi^{ref}(q_{li})$$

- $$q_{li}$$: $$l$$ë²ˆì¨° ë ˆì´ì–´ì˜ ê°ì²´ ì¿¼ë¦¬ì—ì„œì˜ $$i$$ë²ˆì§¸ reference pointì…ë‹ˆë‹¤.
- $$c_{li}$$: $$l$$ë²ˆì¨° ë ˆì´ì–´ì˜ ê°ì²´ ì¿¼ë¦¬ì—ì„œì˜ $$i$$ë²ˆì§¸ $$3$$ ì°¨ì› Bbox Centerì…ë‹ˆë‹¤.

> Reference Point: í˜„ì¬ Targetìœ¼ë¡œ Focusí•˜ê³  ìˆëŠ” ì˜ì—­ í˜¹ì€ í”½ì…€ì…ë‹ˆë‹¤.

2. ì•ì„œ êµ¬í•œ 3D Bbox Centerë¥¼ Camera Tranformation Matrixë¥¼ í†µí•´ 2D Feature Mapìœ¼ë¡œ Projection(ì‚¬ì˜)ì‹œí‚µë‹ˆë‹¤.

$$c^*_{li}=c_{li} \oplus 1$$

Camera Tranformation Matrixì˜ shapeì´ $$(3,4)$$ì´ê¸° ë•Œë¬¸ì— $$3$$ì°¨ì›ì—ì„œ $$2$$ì°¨ì›ìœ¼ë¡œì˜ ë³€í™˜ ê³¼ì •ì„ ìš©ì´í•˜ê²Œ í•˜ê³ ì $$1$$ì„ Concatí•˜ì—¬ Homogeneous Counterpartë¡œ í‘œí˜„í•´ì¤ë‹ˆë‹¤.

$$c_{lmi}=T_mc^*_{li}$$

- $$T_m$$: $$m$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·°ì˜ íˆ¬ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.

ê²°ê³¼ì ìœ¼ë¡œ $$3$$ì°¨ì›ì˜ Centerë¥¼ $$4$$ì°¨ì›ìœ¼ë¡œ í™•ì¥í•œ í›„, íˆ¬ì˜ í–‰ë ¬ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ ìµœì¢…ì ìœ¼ë¡œ 2D ì°¨ì› $$(x^{\prime},y^{\prime},1)$$ìœ¼ë¡œ ì‚¬ì˜ì‹œí‚µë‹ˆë‹¤.

> $$(x^{\prime},y^{\prime},1) = (x^{\prime},y^{\prime})$$.

> ìƒê¸° ê³¼ì •ë“¤ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [ì—¬ê¸°](#camera-parameters)ë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”.

3. ê° ì¹´ë©”ë¼ ë·° ì´ë¯¸ì§€ìœ¼ë¡œ ì‚¬ì˜ëœ ìœ„ì¹˜ì—ì„œ [Bilinear Interpolation](#bilinear-interpolation)ì„ í†µí•´ Featureë¥¼ Sample í•˜ê³  Object Queryì™€ ë”í•´ì¤ë‹ˆë‹¤.

$$f_{lkmi}=f^{bilinear}(\mathcal{F}_{km},c_{lmi})$$

- $$f^{bilinear}$$: $$\mathcal{F}_{km}$$ í”¼ì²˜ë§µì—ì„œ $$c_{lmi} \in \mathbb{R}$$ ì¤‘ì‹¬ì ì„ ì ì ˆí•œ Indexë¡œ ë³´ê°„í•´ì£¼ê¸° ìœ„í•œ Bilinear Interpolation í•¨ìˆ˜ì…ë‹ˆë‹¤.

Bilinear Interpolationì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” $$3$$ì°¨ì›ì—ì„œ $$2$$ì°¨ì›ìœ¼ë¡œ ì‚¬ì˜ëœ CenterëŠ” ì‹¤ìˆ˜ê°’ ì¼ìˆ˜ë„ ìˆëŠ”ë°, $$2$$ì°¨ì›ì—ì„œ ì•Œë§ì€ Center Indexë¡œ ë³´ê°„í•´ì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

$$ \textbf{f}_{li}={1 \over \Sigma_k \Sigma_m \sigma_{lkmi} +\epsilon} \Sigma_k \Sigma_m  \textbf{f}_{lkmi} \sigma_{lkmi}$$

- $$\sigma_{lkmi}$$: $$l$$ë²ˆì§¸ ë ˆì´ì–´ì˜ $$k$$ë²ˆì§¸ levelì˜ $$m$$ë²ˆì§¸ ì¹´ë©”ë¼ ë·°ì˜ $$i$$ë²ˆì§¸ Pointê°€ Image Plane ë°–ìœ¼ë¡œ ì‚¬ì˜ëì„ ê²½ìš° í•„í„°ë§ í•˜ê¸°ìœ„í•œ binary ê°’ì…ë‹ˆë‹¤.
- $$\epsilon$$: ë¶„ëª¨ê°€ 0ìœ¼ë¡œ ë‚˜ëˆ ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ ì ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•´ì¤ë‹ˆë‹¤.

$$ \textbf{q}_{l+1}= \textbf{f}_{li}+ \textbf{q}_{li}$$

ì´ë ‡ê²Œ $$2$$ì°¨ì›ìœ¼ë¡œ ë¶€í„° ê° ê°ì²´ ì¿¼ë¦¬ë§ˆë‹¤ ë”í•´ì§„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ íŠ¹ì§•ì„ í•´ë‹¹ ê°ì²´ ì¿¼ë¦¬ì— ë”í•˜ì—¬ ë‹¤ìŒ ë ˆì´ì–´ë¥¼ êµ¬ì„±í•˜ê²Œ ë©ë‹ˆë‹¤.

4. DETEì²˜ëŸ¼ Multi-Head Attentionì„ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ Object ê°„ì˜ Interactionì„ í•™ìŠµí•©ë‹ˆë‹¤.

DETRê³¼ ë™ì¼í•œ ë™ì‘ ì›ë¦¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.

5. ê° Object Queryë¥¼ Bbox Regressionê³¼ Classificationì„ ìœ„í•œ 2ê°œì˜ MLPì˜ ì…ë ¥ìœ¼ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.

6. ëª¨ë¸ ì˜ˆì¸¡ìœ¼ë¡œ íšë“í•œ Class ì˜ˆì¸¡ê³¼ Bbox ì˜ˆì¸¡ì„ ì •ë‹µ ë ˆì´ë¸”(GT)ì™€ ë¹„êµí•˜ì—¬ Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´í›„, ì´ Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ Bboxì˜ Center$$(x,y,z)$$ê³¼ í¬ê¸°$$(w,h,d)$$ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

## Loss
ì•ì„œ ì„¤ëª…í–ˆë“¯ set-to-set lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ëŠ” class labelì„ ìœ„í•œ focal lossì™€ bbox parameterë¥¼ ìœ„í•œ L1 lossë¡œ êµ¬ì„±ì´ ë©ë‹ˆë‹¤.

DETRì˜ Hungarian Lossì™€ ë™ì¼í•©ë‹ˆë‹¤ (ì•„ë˜ í¬ìŠ¤íŠ¸ ì°¸ì¡°í•´ì£¼ì„¸ìš”).

[DETR](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).


****
# Experiment ğŸ‘€

****
# Open Reivew ğŸ’—

****
# Discussion ğŸŸ

****
# Major Takeaways ğŸ˜ƒ

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference