---
layout: single
title: "[논문 분석] DETR3D (CoRL 2021)"
categories: AIPaperCV
tag: [Computer Vision, Camera-based 3D Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/3d-od-cam.png
sidebar:
    nav: "docs"
---

[**논문**](https://arxiv.org/pdf/2110.06922.pdf)

<!-- <span style="color:blue"> ???? </span> -->

****
# 한줄요약 ✔
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/d0a480f2-ee7a-477a-9740-781f7e5a70f8)

- **Camera-based** Object Detection입니다.
    - 다중 뷰(multi-view) 2D 이미지를 활용하여 3D 공간에서 정의된 객체 쿼리들의 정보를 추론합니다.
- **이미지 특징 추출**: ResNet과 (multi-scale)FPN을 사용해서 다중 뷰 입력 이미지들로 부터 multi-scale 2D 특징들을 추출합니다.
- **2D-to-3D Feature Transformation**: 3D 공간의 reference point에 대한 다중 뷰 피처맵들에서의 특징을 bilinear interpolation을 통해 보간하고, 그들의 평균값을 계산합니다.
- **Loss**: 기존 DETR과 동일한 Hungarian Loss를 따릅니다.
- nuScenes dataset을 이용해서 성능을 평가하였고 **SOTA 성능**을 냈습니다.

****
# Preliminaries 🍱
## Feature Pyramid Network (FPN)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/b9d2f6e9-7bec-4d3f-adeb-c4c54d74baa0)

Feature Pyramid Network (FPN)은 이미지에서 다양한 스케일의 객체를 탐지하고 분할하는데 사용되는 딥러닝 네트워크 구조입니다.

FPN은 **하위 레이어**의 높은 해상도의 정보와 **높은 레이어**의 추상적 정보를 동시에 활용하여 객체 탐지 및 분할 작업의 정확도를 향상시키는데 주로 사용됩니다.

FPN은 두 가지 주요 구성 요소인 **Bottom-up**과 **Top-down**의 조합으로 이루어집니다. 아래는 각 구성 요소의 역할에 대한 설명입니다.

### Bottom-up
Bottom-up은 일반적인 CNN(Convolutional Neural Network) 기반의 네트워크입니다.

높은 해상도의 입력 이미지를 저수준의 특징 맵(Feature Map)으로 변환하는 역할을 합니다.

예를 들어, ResNet과 같은 기존의 CNN 아키텍처를 Bottom-up으로 사용할 수 있습니다.

### Top-down
Top-down은 Bottom-up으로부터 생성된 저수준의 특징 맵을 이용하여 고수준의 특징 맵을 생성하는 역할을 합니다

즉, Bottom-up으로부터 나온 낮은 해상도의 특징 맵들을 Upsampling한 후, 다른 높은 해상도의 특징 맵들과 결합하여 저수준의 특징과 고수준의 특징을 모두 포함하는 높은 해상도의 특징 맵을 얻어냅니다.

## Bilinear Interpolation
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/7a9480e3-3fa2-45b5-bf7c-ae716aaa24d9)

Bilinear interpolation은 주어진 두 점의 사이에 위치하는 값들을 추정하는데 사용되는 보간 기법으로, 두 점 사이의 가중 평균으로 값을 추정하는 방법입니다.

## Camera Parameters
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/51be0e1a-1959-4119-8a6a-f04a76462bbc)

Camera Parameter는 World Coordinate에 있는 3D Voxel $$(U,V,W)$$를 Image Coordinate의 2D Pixel $$(x,y)$$로 변환하기 위해 사용됩니다.

이때 3차원의 점 $$(U,V,W)$$는 **homogeneous counterpart**로써 새로운 차원에 $$1$$ 값을 새로 추가하여 $$3D \rightarrow 2D$$ 차원 변환 과정을 용이하게 한다.
- 2D 차원으로의 투영 행렬의 shape이 $$(3,4)$$이기 때문에 $$3$$차원의 점은 $$4$$차원으로 맞춰줄 필요가 있다.
- 새로운 차원의 값을 $$1$$로 설정함으로써 해당 차원 값은 투영 행렬에 의해 **depth(깊이)**가 된다.

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/393f3c48-615f-47d8-a6d8-3caf3a19e933)

- $$K$$: 카메라 내부 파라미터(intrinsic parameter).
- $$[R$$ | $$t]$$: 카메라 외부 파라미터(extrinsic parameter).
- $$K[R $$|$$ t]$$: camera matrix 또는 projection matrix (투영 행렬).

### Extrinsic Parameter
카메라 좌표계(2D)와 월드 좌표계(3D) 사이의 변환 관계를 설명하는 파라미터로서 두 좌표계 사이의 **회전(rotation)** 및 **평행이동(translation) 변환**으로 표현합니다.

### Intrinsic Parameter
#### 1) Focal Length(초점거리): $$fx, fy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/2d9556ec-561e-47ce-97dd-7a18ad22181c)

$$fx$$와 $$fy$$는 카메라의 렌즈로부터 이미지 평면까지의 거리로, 초점 거리를 나타내는 파라미터입니다.

$$fx$$는 $$x$$축 방향의 초점 거리를, $$fy$$는 $$y$$축 방향의 초점 거리를 의미합니다.

초점 거리는 렌즈의 굴절력과 관련되어 객체의 크기와 카메라 시점에 영향을 줍니다.

#### 2) principal point(주점): $$cx, cy$$.
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/44fce7f7-2136-4a45-b73e-4910986bf8b0)

$$cx$$와 $$cy$$는 이미지 평면 상에서 주점의 좌표를 나타내는 파라미터로, 이미지의 **중심점**을 의미합니다.

주점은 카메라의 광학 축과 이미지 평면의 교차점으로, 카메라 시점과 객체의 상대적인 위치를 나타내는 중요한 정보입니다.​

#### 3) skew coefficient(비대칭계수): $$skew_c = tan \alpha$$
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/6b9aa1cf-7f0d-4ed7-bee4-2f909977a53e)

$$skew_c$$는 카메라의 렌즈가 정사각형이 아닌 비대칭인 경우 $$y$$축이 기울어진 정도(비대칭 정도)를 나타내는 파라미터로, 일반적으로 $$tan(\alpha)$$의 형태로 표현됩니다.

비대칭 계수는 주로 카메라의 렌즈 또는 이미지 센서의 특성과 관련되며, 이를 고려하여 이미지 변환 및 보정 작업에 사용될 수 있습니다.

## DETR 모델
![image](https://user-images.githubusercontent.com/39285147/197422990-0d50e9ab-0866-40d2-9940-ff3ffb91fdde.png)

논문 블로그 포스트 [링크](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).

****
# Problem Definition ✏
                Given an Transformer-based model for object detection 

                Return a more efficient model

                Such that it outperforms the original model in terms of detecting small objects and inference time while maintaining accuracy.

****
# Challenges and Main Idea💣
**C1)** <span style="color:orange"> 기존 모델들은 depth estimation network에 의존하여 낮은 quality의 estimated depth에 대해 3D detection 성능에 해로운 **compounding error**를 겪을 수 있습니다. </span>


**Idea 1)** <span style="color:lightgreen"> DETR3D는 depth estimation network를 사용하지 않고 **Camera Transformation Matrix**을 사용하여 다중 뷰로 부터의 2D Feature Extraction과 3D Object Prediction을 연결하여 이후 depth 정보를 추론합니다. </span>

**C2)** <span style="color:orange"> 기존 모델들은 후처리 과정인 **NMS**에 의존하게 됩니다.</span>

**Idea 2)** <span style="color:lightgreen"> DETR3D는 End-to-End 학습을 하는 DETR 구조를 따르기 때문에 후처리 과정이 필요가 없습니다. </span>

****
# Proposed Method 🧿
## Data Format
- $$\mathcal{L}=\{im_1,...,im_L\} \subset \mathbb{R}^{H_{im} \times W_{im} \times 3}$$: Multi-view Camera Image.
- $$\mathcal{T}=\{T_1,...,T_K\} \subset \mathbb{R}^{3 \times 4}$$: Camera Transformation Matrix.
    - intrinsic & extrinsic parameter를 통해 만드는 행렬입니다 [여기](#camera-parameters).
- $$\mathcal{B}=\{b_1,...,b_j,...,b_M\} \subset \mathbb{R}^9$$: GT Bbox; Each $$b_j$$ contains (position, size, heading angle, velocity) in the birds-eye view (BEV).
    - Position: $$(x,y,z)$$.
    - Size: $$(w,h,d)$$.
    - Heading Angle: $$\theta$$.
    - Velocity: $$\mathcal{v}$$.
- $$\mathcal{C}=\{c_1,...,c_j,...,c_M\} \subset \mathbb{Z}$$: categorical labels.
- $$\mathcal{F}_k=\{f_{k1},...,f_{k6}\} \subset \mathbb{R}^{H \times W \times C}$$: a level of features of the 6 images.
    - $$f_{ki}$$: $$k$$번째 크기 level의 $$i$$번째 카메라 뷰 피처맵입니다.

> NuScenes 벤치마크에서 다중 뷰 데이터셋은 이미지 각각 총 6개의 다중 뷰 이미지를 포함하고 있습니다.

## Feature Learning
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/086244df-4a27-48d8-8f1b-6956ccaf8a60)

ResNet과 [FPN](#feature-pyramid-network-fpn)을 사용하여서 Multi-Scale에서의 Feature를 추출합니다.
- Multi-Scale FPN: 서로 다른 size의 object를 detection 하는데 필요한 풍부한 정보를 얻을 수 있습니다.

결과적으로 $$6$$개의 다중 뷰 이미지마다 총 $$K$$개 만큼의 서로 다른 크기의 피처맵 level을 얻고, 총 $$6 \times K$$개의 피처맵을 획득합니다.

## Detection Head (Main Contribution)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a164ac5a-c859-4ec6-94e7-1163df4b05e3)

기존 Bottom-Up 방식들과는 달리, 본 논문에서는 **Top-down 방식**으로 진행하여 NMS와 depth estimation으로 부터 해방됩니다.

> Bottom-Up 방식
>
>> Image당 많은 bbox를 예측하고 불필요한 bbox를 NMS와 같은 post-processing을 통하여 걸러준 후, 각각의 view에서 얻은 결과를 합쳐주는 과정입니다.

1) Object Query를 Neural Network$$(\Phi^{ref})$$에 통과시켜 Bbox Center의 집합을 예측합니다.

<span style="color:yellow"> $$c_{li}=\Phi^{ref}(q_{li})$$ </span>

- $$q_{li}$$: $$l$$번쨰 레이어의 객체 쿼리에서의 $$i$$번째 reference point입니다.
- $$c_{li}$$: $$l$$번쨰 레이어의 객체 쿼리에서의 $$i$$번째 $$3$$ 차원 Bbox Center입니다.

> Reference Point: 현재 Target으로 Focus하고 있는 영역 혹은 픽셀입니다.

2) 앞서 구한 3D Bbox Center를 Camera Tranformation Matrix를 통해 2D Feature Map으로 Projection(사영)시킵니다.

<span style="color:yellow"> $$c^*_{li}=c_{li} \oplus 1$$ </span>

Camera Tranformation Matrix의 shape이 $$(3,4)$$이기 때문에 $$3$$차원에서 $$2$$차원으로의 변환 과정을 용이하게 하고자 $$1$$을 Concat하여 Homogeneous Counterpart로 표현해줍니다.

<span style="color:yellow"> $$c_{lmi}=T_mc^*_{li}$$ </span>

- $$T_m$$: $$m$$번째 카메라 뷰의 투영 행렬입니다.

결과적으로 $$3$$차원의 Center를 $$4$$차원으로 확장한 후, 투영 행렬의 입력으로 넣어 최종적으로 2D 차원 $$(x^{\prime},y^{\prime},1)$$으로 사영시킵니다.

> $$(x^{\prime},y^{\prime},1) = (x^{\prime},y^{\prime})$$.

> 상기 과정들에 대한 자세한 설명은 [여기](#camera-parameters)를 참조해주세요.

3) 각 카메라 뷰 이미지으로 사영된 위치에서 [Bilinear Interpolation](#bilinear-interpolation)을 통해 Feature를 Sample 하고 Object Query와 더해줍니다.

<span style="color:yellow"> $$f_{lkmi}=f^{bilinear}(\mathcal{F}_{km},c_{lmi})$$ </span>

- $$f^{bilinear}$$: $$\mathcal{F}_{km}$$ 피처맵에서 $$c_{lmi} \in \mathbb{R}$$ 중심점을 적절한 Index로 보간해주기 위한 Bilinear Interpolation 함수입니다.

Bilinear Interpolation을 사용하는 이유는 $$3$$차원에서 $$2$$차원으로 사영된 Center는 실수값 일수도 있는데, $$2$$차원에서 알맞은 Center Index로 보간해주기 위함입니다.

<span style="color:yellow"> $$ \textbf{f}_{li}={1 \over \Sigma_k \Sigma_m \sigma_{lkmi} +\epsilon} \Sigma_k \Sigma_m  \textbf{f}_{lkmi} \sigma_{lkmi}$$ </span>

- $$\sigma_{lkmi}$$: $$l$$번째 레이어의 $$k$$번째 level의 $$m$$번째 카메라 뷰의 $$i$$번째 Point가 Image Plane 밖으로 사영됐을 경우 필터링 하기위한 binary 값입니다.
- $$\epsilon$$: 분모가 0으로 나눠지는 것을 방지하고자 아주 작은 값을 더해줍니다.

<span style="color:yellow"> $$ \textbf{q}_{l+1}= \textbf{f}_{li}+ \textbf{q}_{li}$$ </span>

이렇게 $$2$$차원으로 부터 각 객체 쿼리마다 더해진 하나의 이미지 특징을 해당 객체 쿼리에 더하여 다음 레이어를 구성하게 됩니다.

4) DETE처럼 Multi-Head Attention을 사용하여 서로 다른 Object 간의 Interaction을 학습합니다.

DETR과 동일한 동작 원리를 공유합니다.

5) 각 Object Query를 Bbox Regression과 Classification을 위한 2개의 MLP의 입력으로 통과시킵니다.

6) 모델 예측으로 획득한 Class 예측과 Bbox 예측을 정답 레이블(GT)와 비교하여 Loss를 계산합니다.

이후, 이 Loss를 기반으로 역전파 과정에서 Bbox의 Center$$(x,y,z)$$과 크기$$(w,h,d)$$가 업데이트됩니다.

## Loss
앞서 설명했듯 set-to-set loss를 사용합니다.

이는 class label을 위한 focal loss와 bbox parameter를 위한 L1 loss로 구성이 됩니다.

DETR의 Hungarian Loss와 동일합니다 (아래 포스트 참조해주세요).

[DETR](https://hchoi256.github.io/aipapercv/end-to-end-od-transformer/).


****
# Experiment 👀
## 기존 방식들과의 비교 Table
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/16b84f27-a3ef-42e3-887e-b900dc68af8d)

- 기존의 monocular 3D object detection은 multi-view에 적용하려면 각각의 view에 따로 진행해야 하는 단점이 존재하지만, DETR3D은 multi-view를 한 번에 사용하여 매우 효율적입니다.

## Multi-View Camera에서 겹치는 영역에 대한 비교 Table

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/bab80cee-ce15-435d-bb7c-8a6145ee6e44)

- DETR3D는 Multi-View를 한번에 사용하므로 여러 View에 대해 중복되어 Object가 잘리는 현상에 대해서 효율적으로 처리가 가능합니다.

## Pseudo-LiDAR 비교 Table
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/9106e323-d870-427f-a9c6-522ae6bb6315)

매우 큰 성능 차이를 보이는데, Pseudo-LiDAR가 겪는 compounding error를 DETR3D는 겪지 않기 때문입니다.

## Detection Head Layer Refinement 실험
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/36aa8a1f-49e6-4026-a12a-8dcd6c9b92e4)

- Layer가 깊어짐에 따라 예측되는 bbox가 Ground Truth에 가까워지는 것을 볼 수 있고 *Table 5*를 봐도 NDS가 layer가 깊어짐에 따라 점점 오르는 것을 볼 수 있습니다.

## Object Query 개수 실험
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/59061a06-419e-45a6-93b1-91d2022a1391)

결과적으로 $$900$$개에서 saturate(포화) 되는 모습이며, 이를 통해 적당한 양의 객체 쿼리를 사용하는 것이 좋다는 것을 알게 되었습니다.

****
# Open Reivew 💗
- Limited ablations.
- However, I do not understand why they compared against FCOS3D without fine-tuning.
- DETR 모델 설명 무족.

****
# Discussion 🍟
NA

****
# Major Takeaways 😃
- Camera Transformation Matrix.

****
# Conclusion ✨
## Strength
- <span style="color:lightgreen">**End-to-End 학습**</span>: DETR-3D는 이미지와 3D 객체 정보를 함께 고려하여 End-to-End로 학습되는 모델이기 때문에 별도의 복잡한 파이프라인 없이 하나의 네트워크로 객체 탐지와 분류를 수행할 수 있습니다.
- <span style="color:lightgreen"> **3D 객체 쿼리 사용**</span>: 3D 객체 쿼리를 활용하여 객체의 위치, 크기 및 방향 정보를 표현함으로써, 3D 공간에서의 객체 탐지를 가능하게 합니다. 이를 통해 2D 탐지 모델보다 더 정확한 3D 객체 탐지가 가능합니다.
- <span style="color:lightgreen">**NMS 제약 없음**</span>.
- <span style="color:lightgreen">**별도의 Depth Estimatino Network 필요 없음**</span>.

## Weakness
- <span style="color:orange"> **성능 대비 높은 계산 비용** </span>: DETR-3D는 객체 쿼리를 사용하는 등의 복잡한 구조로 인해 다른 2D 객체 탐지 모델보다 높은 계산 비용을 가질 수 있습니다.
- <span style="color:orange"> **일부 측면에서 기존 방법과 비교적 성능 차이** </span>: 논문에서 언급한 바와 같이 일부 경우에 FCOS3D나 CenterNet과 비교하여 성능이 더 낮게 나온 경우가 있습니다. 이러한 부분에 대한 성능 향상이 필요할 수 있습니다.

****
# Reference
NA