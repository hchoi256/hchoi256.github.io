---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Weaker Than You Think: A Critical Look at Weakly Supervised Learning"
categories: AIPaperNLP
tag: [NLP, Weakly-supervised Learning, Semi-supervised Learning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ai-thumbnail.jpg
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2305.17442.pdf)

<span style="color:lightgreen"> ???? </span>

****
# í•œì¤„ìš”ì•½ âœ”
- Weakly-supervised Learningì€ ì£¼ë¡œ noisy labelsê°€ ë§ë‹¤.
  - ì—¬ì „íˆ fully-supervised model ë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ ì´ìœ .
- <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ê³¼ì¥ë˜ì—ˆë‹¤ </span>.
  - **Clean validation samples**:
    - Correct labelsë¥¼ ê°€ì§„ ë°ì´í„°ë¡œ ê²€ì¦í•˜ëŠ” ê²ƒ; Early stopping, meta learning ë“±ì˜ ì´ìœ ë¡œ ì‚¬ìš©.
    - <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>.
      - ì´ê²ƒì„ training dataë¡œ ì‚¬ìš©í•˜ì—¬ fine-tuningí•œ ëª¨ë¸ì´ ì´ë¥¼ validation setìœ¼ë¡œ í™œìš©í•œ ìµœì‹  WSL ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë”ë¼.. (*Figure 1*)
        - WSL ëª¨ë¸ë“¤ì€ ë‹¨ìˆœ weak labels ë³´ë‹¤ëŠ” ì„±ëŠ¥ ì¢‹ê¸´í•¨..
- <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ì€ ë‹¨ìˆœíˆ linguistic correlationì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ì—®ì´ë„ë¡ í•™ìŠµí•˜ì˜€ëŠ”ë° ì´ê²ƒì€ **í¸í–¥**ë•Œë¬¸ì— ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ë  ìˆ˜ ìˆë‹¤ </span>.
  - <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>.
- Contributions:
  - Validation samplesë¥¼ training dataë¡œì¨ í™œìš©í•˜ì—¬ fine-tuning.
    - Revisiting the true benefits of WSL.

****
# Preliminaries ğŸ±
## Weak Supervision
- definition
  - proposed to ease the annotation bottleneck in training machine learning models.
  - uses weak sources to automatically annotate the data
- drawback
  - its annotations could be noisy (some annotations are incorrect); causing poor generalization
  - solutions
    - to re-weight the impact of examples in loss computation
    - to train noise-robust models using KD
      - equipped with meta-learning in case of being fragile
    - to leverage the knowledge from pre-trained LLM
- datasets
  - WRENCH
  - WALNUT

## Realistic Evaluation
### Semi-supervised Learning
- often trains with a few hundred training examples while retaining thousands of validation samples for model selection
  - ë ˆì´ë¸” ì‚¬ëŒì´ ìˆ˜ë™ìœ¼ë¡œ ë‹¬ì•„ì£¼ëŠ” ê²ƒ í•œê³„
- discard the validation set and use a fixed set of hyperparameters across datasets
  - ì¼ì¼íˆ ê° ë°ì´í„°ì…‹ì— ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‚¬ëŒì´ ì°¾ìŒ;
- prompt-based few-shot learning
  - sensitive to prompt selection and requires additional data for prompt evaluation
    - few-shot learningì— ëª¨ìˆœ!
  - prompt ì˜ˆì‹œ: ë‹¤ìŒ ë¦¬ë·°ê°€ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ íŒë‹¨í•˜ì‹­ì‹œì˜¤:â€
- recent work
  - fine-grained model selection ìƒëµ
  - Number of validation samples strictly controlled

### ì˜ì˜
- To our knowledge, no similar work exists exploring the aforementioned problems in the context of weak supervision.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ë°ì´í„° í™œìš©ì´ ê³¼ì¥ë˜ì—ˆë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ í•™ìŠµ ë°©ë²•ì— í¸í–¥ì´ ì¡´ì¬í•œë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>

****
# Proposed Method ğŸ§¿
**Given** a pre-trained model on $$D_w$$ ~ $$\mathcal{D}_n$$.

**Return** a model.

**Such that** it generalizes well on $$D_{test}$$ ~ $$\mathcal{D}_c$$.

****
# Setup ğŸ‘€
## Formulation
- $$\mathcal{X}$$: feature.
- $$\mathcal{Y}$$: label space.
  - $$\hat{y}_i$$: labels obtained from weak labeling sources; could be different from the GT label $$y_i$$.
- $$D={(x_i,y_i)}^N_{i=1}$$.
  - $$D_c$$: clean data distribution.
  - $$D_w$$: weakly labeled dataset.
  - $$\mathcal{D}_n$$: noisy distribution.
- The goal of WSL algorithms is to obtain a model that generalizes well on $$D_{test} âˆ¼ D_c$$ despite being trained on $$D_w âˆ¼ D_n$$.
- baseline: $$RoBERTa-base$$.

## Datasets
<img width="398" alt="image" src="https://github.com/hchoi256/FluidGPT4/assets/39285147/350b6671-c444-4632-9924-661c7169b57d">

- eight datasets covering different NLP tasks in English

## WSL baselines
<img width="328" alt="image" src="https://github.com/hchoi256/FluidGPT4/assets/39285147/5d3e95e8-03cd-42a2-ada6-1f8169e304f6">

- $$FT_W$$: standard fine-tuning approach for WSL.
- $$L2R$$: meta-learning to determine the optimal weights for each noisy training sample.
- $$MLC$$: meta-learning for the meta-model to correct the noisy labels.
- $$BOND$$: noise-aware self-training framework designed for learning with weak annotations.
- $$COSINE$$: self-training with contrastive regularization to improve noise robustness further.

yì¶•(*Relative performance improvement over weak labels*):

$$G_{\alpha}={(P_{\alpha}-P_{WL}) \over P_{WL}}$$

- $$P_{\alpha}$$: the performance achieved by weak labels.
- $$P_{WL}$$: a certain WSL method.

**â‡’ Without clean validation samples, existing WSL approaches do not work.**

<span style="color:yellow"> ê³¼ì—° ì •ë§ clean dataê°€ ì—†ìœ¼ë©´ WSL ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì„ ìˆ˜ë°–ì— ì—†ë‚˜? </span>.

# Clean Data
<img width="550" alt="image" src="https://github.com/hchoi256/FluidGPT4/assets/39285147/c67433e5-5337-4e9f-b603-47b9b668c619">

**â‡’ a small amount of clean validation samples may be sufficient for current WSL methods to achieve good performance**

<img width="538" alt="image" src="https://github.com/hchoi256/FluidGPT4/assets/39285147/8495bb2a-4b8a-4e83-91e4-9e295c3f6e09">

**â‡’ the advantage of using WSL approaches vanishes when we have as few as 10 clean samples per class**

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference
NA