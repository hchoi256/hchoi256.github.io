---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Weaker Than You Think: A Critical Look at Weakly Supervised Learning"
categories: AIPaperNLP
tag: [NLP, Weakly-supervised Learning, Semi-supervised Learning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ai-thumbnail.jpg
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2305.17442.pdf)

<span style="color:lightgreen"> ???? </span>

****
# í•œì¤„ìš”ì•½ âœ”
- Weakly-supervised Learningì€ ì£¼ë¡œ noisy labelsê°€ ë§ë‹¤.
  - ì—¬ì „íˆ fully-supervised model ë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ ì´ìœ .
- <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ê³¼ì¥ë˜ì—ˆë‹¤ </span>.
  - **Clean validation samples**:
    - Correct labelsë¥¼ ê°€ì§„ ë°ì´í„°ë¡œ ê²€ì¦í•˜ëŠ” ê²ƒ; Early stopping, meta learning ë“±ì˜ ì´ìœ ë¡œ ì‚¬ìš©.
    - <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>.
      - ì´ê²ƒì„ training dataë¡œ ì‚¬ìš©í•˜ì—¬ fine-tuningí•œ ëª¨ë¸ì´ ì´ë¥¼ validation setìœ¼ë¡œ í™œìš©í•œ ìµœì‹  WSL ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë”ë¼.. (*Figure 1*)
        - WSL ëª¨ë¸ë“¤ì€ ë‹¨ìˆœ weak labels ë³´ë‹¤ëŠ” ì„±ëŠ¥ ì¢‹ê¸´í•¨..
- <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ì€ ë‹¨ìˆœíˆ linguistic correlationì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ì—®ì´ë„ë¡ í•™ìŠµí•˜ì˜€ëŠ”ë° ì´ê²ƒì€ **í¸í–¥**ë•Œë¬¸ì— ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ë  ìˆ˜ ìˆë‹¤ </span>.
  - <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>.
- Contributions:
  - Validation samplesë¥¼ training dataë¡œì¨ í™œìš©í•˜ì—¬ fine-tuning.
    - Revisiting the true benefits of WSL.

****
# Preliminaries ğŸ±
## Weak Supervision
- definition
  - proposed to ease the annotation bottleneck in training machine learning models.
  - uses weak sources to automatically annotate the data
- drawback
  - its annotations could be noisy (some annotations are incorrect); causing poor generalization
  - solutions
    - to re-weight the impact of examples in loss computation
    - to train noise-robust models using KD
      - equipped with meta-learning in case of being fragile
    - to leverage the knowledge from pre-trained LLM
- datasets
  - WRENCH
  - WALNUT

## Realistic Evaluation
### Semi-supervised Learning
- often trains with a few hundred training examples while retaining thousands of validation samples for model selection
  - ë ˆì´ë¸” ì‚¬ëŒì´ ìˆ˜ë™ìœ¼ë¡œ ë‹¬ì•„ì£¼ëŠ” ê²ƒ í•œê³„
- discard the validation set and use a fixed set of hyperparameters across datasets
  - ì¼ì¼íˆ ê° ë°ì´í„°ì…‹ì— ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‚¬ëŒì´ ì°¾ìŒ;
- prompt-based few-shot learning
  - sensitive to prompt selection and requires additional data for prompt evaluation
    - few-shot learningì— ëª¨ìˆœ!
  - prompt ì˜ˆì‹œ: ë‹¤ìŒ ë¦¬ë·°ê°€ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ íŒë‹¨í•˜ì‹­ì‹œì˜¤:â€
- recent work
  - fine-grained model selection ìƒëµ
  - Number of validation samples strictly controlled

### ì˜ì˜
- To our knowledge, no similar work exists exploring the aforementioned problems in the context of weak supervision.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ë°ì´í„° í™œìš©ì´ ê³¼ì¥ë˜ì—ˆë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ í•™ìŠµ ë°©ë²•ì— í¸í–¥ì´ ì¡´ì¬í•œë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>

****
# Proposed Method ğŸ§¿
**Given** a pre-trained model on $D_w$ ~ $\mathcal{D}_n$.

**Return** a model.

**Such that** it generalizes well on $D_{test}$ ~ $\mathcal{D}_c$.

****
# Setup ğŸ‘€
## Formulation
- $\mathcal{X}$: feature.
- $\mathcal{Y}$: label space.
  - $\hat{y}_i$: labels obtained from weak labeling sources; could be different from the GT label $y_i$.
- $D={(x_i,y_i)}^N_{i=1}$.
  - $D_c$: clean data distribution.
  - $D_w$: weakly labeled dataset.
  - $\mathcal{D}_n$: noisy distribution.
- The goal of WSL algorithms is to obtain a model that generalizes well on $D_{test} âˆ¼ D_c$ despite being trained on $D_w âˆ¼ D_n$.
- baseline: $RoBERTa-base$.

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Conclusion âœ¨
## Strength
## Weakness

****
# Reference
NA