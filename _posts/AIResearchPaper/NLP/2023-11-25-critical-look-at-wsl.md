---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Weaker Than You Think: A Critical Look at Weakly Supervised Learning"
categories: AIPaperNLP
tag: [NLP, Weakly-supervised Learning, Semi-supervised Learning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/ai-thumbnail.jpg
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2305.17442.pdf)

****
# í•œì¤„ìš”ì•½ âœ”
- Weakly-supervised Learningì€ ì£¼ë¡œ noisy labelsê°€ ë§ë‹¤.
  - ì—¬ì „íˆ fully-supervised model ë³´ë‹¤ ì„±ëŠ¥ì´ ì•ˆì¢‹ì€ ì´ìœ .
- <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ê³¼ì¥ë˜ì—ˆë‹¤ </span>.
  - **Clean validation samples**:
    - Correct labelsë¥¼ ê°€ì§„ ë°ì´í„°ë¡œ ê²€ì¦í•˜ëŠ” ê²ƒ; Early stopping, meta learning ë“±ì˜ ì´ìœ ë¡œ ì‚¬ìš©.
    - <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>.
      - ì´ê²ƒì„ training dataë¡œ ì‚¬ìš©í•˜ì—¬ fine-tuningí•œ ëª¨ë¸ì´ ì´ë¥¼ validation setìœ¼ë¡œ í™œìš©í•œ ìµœì‹  WSL ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë”ë¼.. (*Figure 1*)
        - WSL ëª¨ë¸ë“¤ì€ ë‹¨ìˆœ weak labels ë³´ë‹¤ëŠ” ì„±ëŠ¥ ì¢‹ê¸´í•¨..
- <span style="color:orange"> ê¸°ì¡´ ëª¨ë¸ì€ ë‹¨ìˆœíˆ linguistic correlationì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ì—®ì´ë„ë¡ í•™ìŠµí•˜ì˜€ëŠ”ë° ì´ê²ƒì€ **í¸í–¥**ë•Œë¬¸ì— ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜ë  ìˆ˜ ìˆë‹¤ </span>.
  - <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>.
- Contributions:
  - Validation samplesë¥¼ training dataë¡œì¨ í™œìš©í•˜ì—¬ fine-tuning.
    - Revisiting the true benefits of WSL.

****
# Preliminaries ğŸ±
## Weak Supervision
- definition
  - proposed to ease the annotation bottleneck in training machine learning models.
  - uses weak sources to automatically annotate the data
- drawback
  - its annotations could be noisy (some annotations are incorrect); causing poor generalization
  - solutions
    - to re-weight the impact of examples in loss computation
    - to train noise-robust models using KD
      - equipped with meta-learning in case of being fragile
    - to leverage the knowledge from pre-trained LLM
- datasets
  - WRENCH
  - WALNUT

## Realistic Evaluation
### Semi-supervised Learning
- often trains with a few hundred training examples while retaining thousands of validation samples for model selection
  - ë ˆì´ë¸” ì‚¬ëŒì´ ìˆ˜ë™ìœ¼ë¡œ ë‹¬ì•„ì£¼ëŠ” ê²ƒ í•œê³„
- discard the validation set and use a fixed set of hyperparameters across datasets
  - ì¼ì¼íˆ ê° ë°ì´í„°ì…‹ì— ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‚¬ëŒì´ ì°¾ìŒ;
- prompt-based few-shot learning
  - sensitive to prompt selection and requires additional data for prompt evaluation
    - few-shot learningì— ëª¨ìˆœ!
  - prompt ì˜ˆì‹œ: ë‹¤ìŒ ë¦¬ë·°ê°€ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ íŒë‹¨í•˜ì‹­ì‹œì˜¤:â€
- recent work
  - fine-grained model selection ìƒëµ
  - Number of validation samples strictly controlled

### ì˜ì˜
- To our knowledge, no similar work exists exploring the aforementioned problems in the context of weak supervision.

****
# Challenges and Main IdeağŸ’£
**C1)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ ë°ì´í„° í™œìš©ì´ ê³¼ì¥ë˜ì—ˆë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ì°¨ë¼ë¦¬ ì´ê²ƒì„ training datasetì— í¬í•¨ì‹œì¼°ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ë”ë¼ </span>

**C2)** <span style="color:orange"> ê¸°ì¡´ WS ë°©ë²•ë“¤ì€ í•™ìŠµ ë°©ë²•ì— í¸í–¥ì´ ì¡´ì¬í•œë‹¤. </span>

**Idea)** <span style="color:lightgreen"> ê¸ì •ì ì¸ ê²ƒì— ë¶€ì •ì ì¸ ë ˆì´ë¸”ì„ ê°•ì œí•˜ì—¬ further tuning â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ </span>

****
# Problem Definition â¤ï¸
**Given** a pre-trained model on $$D_w$$ ~ $$\mathcal{D}_n$$.

**Return** a model.

**Such that** it generalizes well on $$D_{test}$$ ~ $$\mathcal{D}_c$$.

****
# Methodology ğŸ‘€
## Setup
### Formulation
- $$\mathcal{X}$$: feature.
- $$\mathcal{Y}$$: label space.
  - $$\hat{y}_i$$: labels obtained from weak labeling sources; could be different from the GT label $$y_i$$.
- $$D={(x_i,y_i)}^N_{i=1}$$.
  - $$D_c$$: clean data distribution.
  - $$D_w$$: weakly labeled dataset.
  - $$\mathcal{D}_n$$: noisy distribution.
- The goal of WSL algorithms is to obtain a model that generalizes well on $$D_{test} âˆ¼ D_c$$ despite being trained on $$D_w âˆ¼ D_n$$.
- baseline: $$RoBERTa-base$$.

### Datasets
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/49678179-8970-4b82-b37f-4512e2011e37)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/62fdabd9-7077-4f7c-833e-3685d0f3229d)

- eight datasets covering different NLP tasks in English

### WSL baselines
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/554c8620-e38b-495e-a36a-b28b60a2571a)

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/74975c44-6e64-46ac-9c81-fe3abe4f428b)

- $$FT_W$$: standard fine-tuning approach for WSL.
- $$L2R$$: meta-learning to determine the optimal weights for each noisy training sample.
- $$MLC$$: meta-learning for the meta-model to correct the noisy labels.
- $$BOND$$: noise-aware self-training framework designed for learning with weak annotations.
- $$COSINE$$: self-training with contrastive regularization to improve noise robustness further.

yì¶•(*Relative performance improvement over weak labels*):

$$G_{\alpha}={(P_{\alpha}-P_{WL}) \over P_{WL}}$$

- $$P_{\alpha}$$: the performance achieved by weak labels.
- $$P_{WL}$$: a certain WSL method.

**â‡’ Without clean validation samples, existing WSL approaches do not work.**

<span style="color:yellow"> ê³¼ì—° ì •ë§ clean dataê°€ ì—†ìœ¼ë©´ WSL ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì„ ìˆ˜ë°–ì— ì—†ë‚˜? </span>.

## Clean Data
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/e141050b-eae2-40a9-a6cc-49ab6580f9d2)

**â‡’ a small amount of clean validation samples may be sufficient for current WSL methods to achieve good performance**

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/675f3182-cc92-49b8-894f-20254ce3c29d)

**â‡’ the advantage of using WSL approaches vanishes when we have as few as 10 clean samples per class**

## Continuous Fine-tuning (CFT)
![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/a6034902-43dc-4028-8ee0-c83a72599219)

- **CFT**
  - In the first phase, we apply WSL approaches on the weakly labeled training set, using the clean data for validation.
  - In the second phase, we take the model trained on the weakly labeled data as a starting point and continue to train it on the clean data.

**â‡’ the net benefit of using sophisticated WSL approaches may be significantly overestimated and impractical for real-world use cases.**

- ê·¸ëƒ¥ FT í•œ ê²ƒë§Œìœ¼ë¡œë„ ê¸°ì¡´ WSL ë°©ë²•ë“¤ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ (even when # clean data = low)
- L2Rì˜ Yelp ë°ì´í„°ì…‹ ê²°ê³¼ì˜ ê²½ìš° CFT ì´í›„ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ ëª¨ìŠµì¸ë°, ì´ê²ƒì€ L2Rê°€ validation lossë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸ì— ê²€ì¦ ìƒ˜í”Œì˜ ê°€ì¹˜ê°€ í° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•˜ì„ì§€ë„..

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/fb8c45c6-5c48-4887-9914-2959edf88873)

**â‡’ Pre-training on more data clearly helps to overcome biases from weak labels.**
- pre-training provides the model with an inductive bias to seek more general linguistic correlations instead of superficial correlations from the weak labels

![image](https://github.com/hchoi256/hchoi256.github.io/assets/39285147/ef30d31e-ab38-4528-a989-31c7d9249168)

**â‡’ contradictory samples play a more important role here and at least a minimum set of contradictory samples are required for CFT to be beneficial**

****
# Open Reivew ğŸ’—
NA

****
# Discussion ğŸŸ
NA

****
# Major Takeaways ğŸ˜ƒ
NA

****
# Conclusion âœ¨
## Strength
- If a proposed WSL method requires extra clean data, such as for validation, then the simple FTW+CFT baseline should be included in evaluation to claim the real benefits gained by applying the method.

## Weakness
- it may be possible to perform model selection by utilizing prior knowledge about the dataset
- For low-resource languages where no PLMs are available, training may not be that effective
- We have not extended our research to more diverse types of weak labels

****
# Reference
NA