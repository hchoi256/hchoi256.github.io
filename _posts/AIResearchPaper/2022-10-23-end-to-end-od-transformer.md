---
layout: single
title: "[ë…¼ë¬¸ ë¶„ì„] End-to-End Object Detection with Transformers (ECCV 2020)"
categories: AIPaperCV
tag: [Object Detection, Transformer]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/od.png
sidebar:
    nav: "docs"
---

[**ë…¼ë¬¸**](https://link.springer.com/content/pdf/10.1007/978-3-030-58452-8_13.pdf)

What paper you have read?
- End-to-End Object Detection with Transformers (ECCV 2020)

What area or technology addressed in the paper that interested you the most? And can they be applied to Carla?
- DETR Accuracy > SOTA R-CNN
- panoptic segmentation
- DETR is straightforward to implement
- high performance on large dataset


****
# ë°°ê²½ ğŸ™Œ
**Object detection**ì€ ë¶„ë¥˜ë¥¼ ìœ„í•œ cateogry boxë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì´ë‹¤.

[*non-maximal suppression*]

![image](https://user-images.githubusercontent.com/39285147/197414699-970639a6-076d-4b2b-b1de-763931c9082e.png)

í˜„ëŒ€ detectorsë“¤ì€ *ê°„ì ‘ì ì¸ ë°©ë²•*(hand-designed = ì‚¬ì „ì‘ì—…)ìœ¼ë¡œ ê°ì²´ íƒì§€ë¥¼ êµ¬í˜„í–ˆë‹¤: anchors, non-maximal suppression, window centers, ëŒ€ìš©ëŸ‰ proposals, etc.

> **Non-Maximum Suppression**
>> object detectorê°€ ì˜ˆì¸¡í•œ bounding box ì¤‘ì—ì„œ ì •í™•í•œ bounding boxë¥¼ ì„ íƒí•˜ë„ë¡ í•˜ëŠ” ê¸°ë²•

ì´ëŸ¬í•œ ê°„ì ‘ì ì¸ ë°©ë²•ë“¤ì€ *í›„ì²˜ë¦¬*(ì˜¤ì°¨ ì˜ˆì¸¡ ë° ì œê±°)ì— ë§‰ëŒ€í•˜ê²Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.

> ì˜¤ì°¨: ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’

ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ì—¬ëŸ¬ ê°€ì§€ ë³µì¡í•œ ì˜ˆì¸¡ ë¬¸ì œì— ëŒ€í•œ ê°ì²´ íƒì§€ ë¬¸ì œì—ì„œ í•œê³„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

ì´ëŸ¬í•œ pipeline(ê³¼ì •)ì—ì„œì˜ surrogate tasksë¥¼ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ê²ƒì´ **Direct set prediction ë°©ë²•**ì´ë‹¤.

> Surrogate ëª¨ë¸: outputì´ ì •í™•íˆ ì¸¡ì •ë  ìˆ˜ ì—†ì„ ë•Œ, ëŒ€ì‹  output ê¸°ì¤€ì„ ì œê³µí•˜ëŠ” ëŒ€ë¦¬ ëª¨ë¸

ì´ëŸ¬í•œ ìƒˆë¡œìš´ ëª¨ë¸ë“¤ì€ ì—¬ëŸ¬ ê°€ì§€ ë³µì¡í•œ ì˜ˆì¸¡ ë¬¸ì œ í•´ê²°ì— íš¨ê³¼ì ì´ë‹¤.

ê·¸ëŸ¬í•œ ë°©ë²•ì˜ ëŒ€í‘œì ì¸ ì˜ˆì‹œì¸ **DETR**ì— ëŒ€í•´ ì´ë²ˆ ë…¼ë¬¸ì—ì„œ ì•Œì•„ë³´ì.

****
# INTRODUCTION ğŸ‘€
 ![image](https://user-images.githubusercontent.com/39285147/197411640-e6c3de0f-b4f3-4665-ae05-6a0b45c90bf3.png)

DETR
- **í•˜ë‚˜ì˜ CNNë¥¼ Transformer ì•„í‚¤í…ì³ì™€ ë³‘í•©**í•˜ì—¬ ë³‘ë ¬ì²˜ë¦¬ë¥¼ í†µí•´ **ì§ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡** ìˆ˜í–‰
- extra-long training schedule
- auxiliary decoding losses in the transformer
- the conjunction of the bipartite matching loss and transformers with (non-autoregressive) parallel decoding

> **ìê¸°íšŒê·€(AR; Autoregressive)**
>> ê³¼ê±°ì˜ ì›€ì§ì„ì— ê¸°ë°˜í•´ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ 

## ê¸°ìˆ  ìš©ì–´
### Bipartite matching
- ground truth boxesë¥¼ ì‚¬ìš©í•´ ë…ë¦½ì  ì˜ˆì¸¡ì„ í•œë‹¤.
    - no match: "no object"
- uniquely assigns a prediction to a ground truth object
    - ì˜ˆì¸¡ê°’ ìˆœì—´ = invariant = ì˜ˆì¸¡í•  ê°ì²´ì— ëŒ€í•´ ê³ ìœ í•¨ --> parallellism ë³´ì¥
    - ë°˜ëŒ€ë¡œ ê¸°ì¡´ RNN ëª¨ë¸ = autoregressive decoding
- Hungarian algorithm

### Self-attention
- pairwise interactions between elements in a sequence,
- duplicate predictions ì œê±°

### Set loss function
- performs bipartite matching between predicted and ground-truth objects.

### COCO
- one of the most popular object detection datasets

### Transformer Decoder
**ê¸°ì¡´ Transformer** = autoregressive (output sequenceë¥¼ í•˜ë‚˜í•˜ë‚˜ ë„£ì–´ì£¼ëŠ” ë°©ì‹) 

**DETR ëª¨ë¸** = í•œë²ˆì— Nê°œì˜ obejctë¥¼ ë³‘ë ¬ ì˜ˆì¸¡
- Nê°œì˜ ë‹¤ë¥¸ ê²°ê³¼ --> Nê°œì˜ ì„œë¡œ ë‹¤ë¥¸ input embedding
    - Input embedding: "*object query(positional encoding)*"ë¥¼ í†µí•´ í‘œí˜„
        - 1) Nê°œì˜ object queryëŠ” ë””ì½”ë”ì— ì˜í•´ output embeddingìœ¼ë¡œ ë³€í™˜
        - 2) Nê°œì˜ ë§ˆì§€ë§‰ ì˜ˆì¸¡ ì‚°ì¶œ
        - 3) self/encoder-decoderê°„ ì–´í…ì…˜ì„ í†µí•´ ê° object ê°„ì˜ global ê´€ê³„ í•™ìŠµ

### Auxiliary decoding losses
ëª¨ë¸ì´ ë§ëŠ” ê°œìˆ˜ì˜ objectë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ì„ ì£¼ëŠ” auxiliary lossë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ìš°ë¦¬ëŠ” ê° decoder layerì´í›„ì— prediction FFNê³¼ Hungarian lossë¥¼ ì¶”ê°€í•˜ì˜€ë‹¤. ëª¨ë“  prediction FFNì€ ê°™ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ìš°ë¦¬ëŠ” ë‹¤ë¥¸ ë””ì½”ë”ë ˆì´ì–´ì—ì„œì˜ inputì„ ì •ê·œí™”í•˜ê¸° ìœ„í•´ layer normì„ ì¶”ê°€í•˜ì˜€ë‹¤.

****
# Related Work ğŸ—‚
## Set Prediction
**Indirect Set Prediction**
- multilabel classification (postprocessing)
    - near-identical boxes í•´ê²° ë¶ˆê°€ëŠ¥

**Direct Set Prediction**
- postprocessing-free
    - *global inference schemes*
        - model interactions between all predicted elements to avoid redundancy.
    - Auto-regressive sequence models (i.e., recurrent neural networks)
        - bipartite matching loss            
        - permutation-invariance

**DETR**
- bipartite matching loss + transformers (non-autoregressive) parallel decoding

## Transformers and Parallel Decoding

## Object Detection


****
# DETR Model âœ’
## Object Detection Set Prediction Loss

## DETR Architecture

****
# Experiments âœ

## ì„±ëŠ¥ ë¹„êµ: Faster R-CNN and RetinaNet

## Ablations

## DETR for Panoptic Segmentation

****
# ê²°ê³¼ âœ”
## DETR ì¥ì 
- DETR ì •í™•ë„ SOTA R-CNN ëª¨ë¸ ëŠ¥ê°€
- ìœ ì—°í•œ Transformer ì•„í‚¤í…ì³ --> Panoptic segmentation ì„±ëŠ¥ â†‘
- êµ¬í˜„ì´ ì‰½ë‹¤
- ëŒ€ìš©ëŸ‰ datasetì— ëŒ€í•œ ì„±ëŠ¥ â†‘
    - global information performed by the self-attention
    - enabled by the non-local computations of the transformer
- ì»¤ìŠ¤í…€ layers í•„ìš” ì—†ìŒ --> better reproduction
    - ë•Œë¬¸ì— DETRì€ ResNet, Transformer í”„ë ˆì„ ì›Œí¬ì—ì„œë„ ì¬ì‚¬ìš© ê°€ëŠ¥ 

## DETR í•œê³„
- Training ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
- ìµœì í™” ë¬¸ì œ
- small datasetì— ëŒ€í•œ í•™ìŠµ ì„±ëŠ¥ â†“

****
# Reference ğŸ‘“
[**GitHub Repository**](https://github.com/hchoi256/carla-research-project)

[*UW Madison CARLA Research Team*](https://cavh.cee.wisc.edu/carla-simulation-project/)

[*CARLA Simulator*](https://carla.readthedocs.io/en/latest/)