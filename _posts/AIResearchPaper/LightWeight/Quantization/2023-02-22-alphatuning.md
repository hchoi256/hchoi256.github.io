---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"
categories: AIPaperLightWeight
tag: [Model Compression, AlphaTuning, Quantization, Pruning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2210.03858.pdf)

****
# í•œì¤„ìš”ì•½ âœ”

****
# Background ğŸ±
Self-supervised Learningì˜ ë“±ì¥ìœ¼ë¡œ, ë°ì´í„° í™•ë³´ê°€ ì‰¬ì›Œì§ì— ë”°ë¼ ì¦ê°€í•œ ë°ì´í„° ê°œìˆ˜ë§Œí¼ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆ(parameter ê°œìˆ˜) ë˜í•œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

í˜„ ì‹œì ì—ì„œ Transformerì˜ ë§‰ê°•í•œ powerì„ ë•íƒì— AI ì „ë°˜, íŠ¹íˆ NLP ë¶„ì•¼ì— ë§ì€ ë°œì „ì´ ì‡ë”°ë¥´ê³ , ê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ì˜€ë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ ë˜í•œ ì–¸ì–´ëª¨ë¸ ì••ì¶•ì„ targetìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.

ëª¨ë¸ì••ì¶• ì™¸ì—, zero/few-shot learning ê¸°ë°˜ ê¸°í•™ìŠµ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì˜ ì „ì ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê¸°í•™ìŠµ ëª¨ë¸ training ì´í›„ fine-tuningì´ë¼ëŠ” ìƒˆë¡œìš´ adaptation ê³¼ì •ì€ downstream task ì„±ëŠ¥ í–¥ìƒì— ìˆì–´ì„œ í•„ìˆ˜ì ì´ë‹¤.

> ê° downstream taskë§ˆë‹¤ ë…ë¦½ì ì¸ ê°ê°ì˜ adaptation ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.

í•˜ì—¬ ë‹¤ì–‘í•œ downstream taskì— ë²”ìš©ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” parametersë§Œì„ fine-tuning ëŒ€ìƒìœ¼ë¡œ ì‚¼ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤ (`parameter-efficient adaptation techniques`).
- Adapter modules, low-rank adapatino, prefix-tuning, etc.

í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì€ trainiable parameter ê°œìˆ˜ ìì²´ëŠ” ìƒê¸° ë°©ì‹ë“¤ë¡œ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë‚˜, ê·¸ë“¤ì´ ê¸°í•™ìŠµ ëª¨ë¸ì— ë¹„í•´ fine-tuned modelì˜ inference ì„±ëŠ¥ì´ ê·¸ë‹¤ì§€ ì¢‹ì§€ ì•Šë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤.

ë˜í•œ, í˜„ì¡´í•˜ëŠ” ëª¨ë¸ì••ì¶• ê¸°ìˆ ë“¤ì€ ë°˜ëŒ€ë¡œ parameter-efficient adapation ê¸°ë²•ì„ í™œìš©í•˜ì§€ ì•Šë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤.

ê°€ë ¹, ê¸°ì¡´ ì••ì¶• ê¸°ë²•ì¸ QATì˜ ê²½ìš°, fine-tuningê³¼ ëª¨ë¸ì••ì¶•ì„ í•¨ê»˜ ì‚¬ìš©í•œ ì ‘ê·¼ë²•ì´ì§€ë§Œ, ê¸°í•™ìŠµ ëª¨ë¸ ë§Œí¼ì˜ memory storageë¥¼ í•„ìš”ë¡œ í•œë‹¤.

****
# Introduction ğŸ™Œ
ìµœê·¼ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ê´€ì‹¬ë„ê°€ ë‚˜ë‚ ì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ ì†ì—ì„œ, ì–´ë–»ê²Œ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ì •í™•ë„ë¥¼ í•´ì¹˜ì§€ ì•Šìœ¼ë©´ì„œ ë†’ì¼ ìˆ˜ ìˆëŠ” ì§€ì— ëŒ€í•œ ë§ì€ ì—°êµ¬ê°€ ì´ë¤„ì§€ê³  ìˆë‹¤.

ì´ëŸ¬í•œ ê´€ì‹¬ì—ë„ ë¶ˆêµ¬í•˜ê³ , í•´ë‹¹ ë¶„ì•¼ëŠ” ì•„ì§ ë°í˜€ì§€ì§€ ì•Šì€ ê²ƒë“¤ì´ ë„ˆë¬´ ë§ë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ `model compression`ê³¼ `parameter-efficient adaptation` ê¸°ìˆ ì„ ê²°í•©í•œ ìƒˆë¡œìš´ ë°©ì‹ì¸ compression-aware parameter-efficient ê¸°ë²•ì¸ **AlphaTuning**ì„ ì œì‹œí•œë‹¤.

> `Parameter-efficient tuning`
>
>> ë” ì ì€ íŒŒë¼ë¯¸í„°ë§Œì„ í•™ìŠµí•˜ì—¬ downstream taskì— ëŒ€í•´ì„œ fine-tuningê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì´ë‹¤.

í•´ë‹¹ ê¸°ìˆ ì€ ê¸°í•™ìŠµ ëª¨ë¸ì— ëŒ€í•œ post-training quantizationê³¼ ì–‘ìí™”ëœ ëª‡ëª‡ì˜ parametersë“¤ì— ëŒ€í•´ì„œë§Œ target taskì— fine-tuningì‹œí‚¨ë‹¤.

AlphaTuningì€ binary-coding quantizationì„ ìˆ˜í–‰í•˜ë©°, adaptation ë‹¨ê³„ì—ì„œëŠ” binary ê°’ë“¤ì„ freezeí•˜ê³  scaling factorsë“¤ë§Œ fine-tuningì„ ì§„í–‰í•œë‹¤.

> `Binarization` ê°œë…ì´ ì–´ìƒ‰í•˜ë‹¤ë©´, [ì—¬ê¸°](https://hchoi256.github.io/aipaperlightweight/xnor-net/)ë¥¼ ì°¸ì¡°í•˜ê¸¸ ë°”ë€ë‹¤.

ë³¸ ë…¼ë¬¸ì´ ì œì‹œí•œ AlphaTuning ê¸°ë²•ì€ GPT-2ì™€ OPT ì ìš© ì‹œ, 4-bit quantization í™œìš© 10ë°° ì••ì¶• ë° 1000ë°°ì˜ parameter ê°œìˆ˜ ê°ì†Œ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

> GPTëŠ” ë‹¤ë“¤ ì•Œê³  ê³„ì‹¤í…Œê³ , OPTëŠ” Metaì—ì„œ ì„ ë³´ì¸ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ë¡œ ê°€ì¥ ìµœì‹  ë²„ì „ì˜ í•´ë‹¹ ëª¨ë¸ì€ 1750ì–µê°œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/220860698-05776ad1-d788-422f-a4e0-5cf52b53fcef.png)

ëª¨ë¸ì••ì¶•ê³¼ parameter-efficient adaptationì„ ìˆ˜í–‰í•˜ëŠ” ì ‘ê·¼ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤.

ìƒê¸° ì´ë¯¸ì§€ì—ì„œ, ê¸°ì¡´ ì ‘ê·¼ë²•ì€ $$A \rightarrow C \rightarrow D$$ ìˆœì„œì— í•´ë‹¹í•˜ë©°, ë§ì€ trainable parameters ë° PTQ ì‹œ downstream tasks ì„±ëŠ¥ ê°ì†Œê°€ í•œê³„ë¡œ ê¼½íŒë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ì ‘ê·¼ë²• $$A\rightarrow B \rightarrow D$$ì„ ì œì‹œí•˜ê³ , í•´ë‹¹ ì ‘ê·¼ë²•ì„ **AlphaTuning**ì´ë¼ ëª…ì¹­í•œë‹¤.

AlphaTuningì€ (1)ì£¼ì–´ì§„ parametersì„ `binary values`ì™€ `scaling factors`ë¡œ factorizationì„ ìˆ˜í–‰í•œë‹¤.
- scaling factorsëŠ” quantization formatsì—ì„œ ì•„ì£¼ ì‘ì€ ë¶€ë¶„ë§Œì„ ì°¨ì§€í•œë‹¤.

ì´í›„, (2)binary valuesëŠ” freezeí•˜ê³ , ì‘ì€ ë©”ëª¨ë¦¬ ë¶€ë¶„ë§Œì„ ì°¨ì§€í•˜ëŠ” scaling factorsì— ëŒ€í•´ì„œë§Œ fine-tuningì„ ì§„í–‰í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ accelerateí•œë‹¤.

A $$\rightarrow$$ B ê³¼ì •ì€ QAT ëŒ€ì‹  PTQë¥¼ ìˆ˜í–‰í•œë‹¤; QATëŠ” ë°©ëŒ€í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ í›ˆë ¨ ì‹œ computational overheadê°€ ì—„ì²­ë‚˜ë‹¤.

<span style="color:yellow"> QAT ê²½ìš° overhead ì¤„ì¼ ìˆ˜ë§Œ ìˆë‹¤ë©´, PTQë¥¼ ëŒ€ì²´í•´ë„ ì¢‹ì„ê¹Œ?</span>

****
# Related Work ğŸ˜‰
## Large-Scale Language Models and Quantization
ê¸°í•™ìŠµ Transformer ì–¸ì–´ëª¨ë¸ì€ ê¸°ì¡´ì˜ NLP ë””ìì¸ ë° ë°°í¬ ë°©ì‹ì„ ì „ë©´ì ìœ¼ë¡œ ë³€í™”ì‹œì¼°ë‹¤.

ìµœê·¼ì—, ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì— ëŒ€í•œ í™•ì¥ëœ ì ‘ê·¼ì„±ì€ ìƒˆë¡œìš´ ìì—°ì–´ ì²˜ë¦¬ì˜ ì‹œëŒ€ë¥¼ ì—´ì—ˆê³ , few-shot learningê³¼ parameter-efficient adapation ê°™ì€ ê¸°ìˆ ì˜ ì¬ë°œê²¬ì„ ëŒì–´ë‚¸ë‹¤.

Quantizationì€ ê·¼ë³¸ì ì¸ ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì— ëŒ€í•œ ê³µê°„ ë° ê³„ì‚° ì‹œê°„ íš¨ìœ¨ í•´ê²°ì±…ìœ¼ë¡œ ì†ê¼½íˆê³  ìˆì§€ë§Œ, ê¸°ì¡´ ë°©ë²•ë“¤ì€ ì–‘ìí™”ëœ ìƒíƒœì—ì„œ ì œí•œëœ ì˜ì—­ê³¼ task adapationabilityë¥¼ ì œê³µí•˜ëŠ” í•œê³„ë¥¼ ì§€ë‹ˆê³  ìˆì—ˆë‹¤.

## Parameter-Efficient Adaptation of LMs
ì´ˆê±°ëŒ€ ëª¨ë¸ë“¤ì´ íŒì„ ì¹˜ëŠ” í˜„ ì‹œì ì—ì„œ ì–¸ì–´ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ downstream taskì— adaptingí•˜ëŠ” ê²ƒì€ ì´ ì‚¬íšŒì˜ ìµœê³  ê´€ì‹¬ì‚¬ì´ë‹¤.

í•œ ê°€ì§€ ì „ë„ìœ ë§í•œ ì ‘ê·¼ë°©ì‹ì€ (1)`in-context learning (ICL)`ë¼ëŠ” ê²ƒì¸ë°, ì´ê²ƒì€ ì£¼ì–´ì§„ prompt ëŒ€í•œ íŒ¨í„´ë“¤ë¡œ ë¶€í„° ë°°ìš°ê³  ì˜ˆì¸¡í•˜ëŠ” ì–¸ì–´ëª¨ë¸ì´ë‹¤.

í•´ë‹¹ ê¸°ë²•ì€ ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ë“¤ì— ëŒ€í•˜ì—¬ parameter-tuning ì—†ì´ í•©ë¦¬ì ì¸ few-shot ì„±ëŠ¥ì„ ëŒë‚´ê³ , ìˆ˜ë§ì€ í™•ì¥ ì—°êµ¬ë“¤ì´ íƒ„ìƒí•´ì™”ë‹¤.

ë˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œëŠ” (2)parameter-efficient LM adapationì„ ìœ„í•´ ì™¸ë¶€ í˜¹ì€ ë¶€ë¶„ì ìœ¼ë¡œ ë‚´ë¶€ parameters (i.e., continuous prompt embeddings)ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì¸ë°, ì´ê²ƒì€ íŠ¹ì • prompt prefixesê°€ ë” ë‚˜ì€ íŠ¹ì • LM í–‰ë™ ë°©ì‹ì— ê´€ì—¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì°©ì•ˆí•œë‹¤.

> `Continuous/soft prompts`
>
>> Additional learnable parameters injected into the model

ê³¼ê±° ì—°êµ¬ë“¤ì€ **discrete** prompt token spaceë¥¼ ì¡°ì‚¬í–ˆì§€ë§Œ, ì´í›„ **continuous** work embedding spaceë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ë‹¤.

> Prompt tuningê³¼ ê´€ë ¨í•œ ë” ìì„¸í•œ ë‚´ìš©ì€ [P-tuning](https://velog.io/@seopbo/GPT-Understands-Too) ê¸°ë²•ì„ ì°¹ì¡°í•˜ê¸° ë°”ë€ë‹¤.
>
>> `P-tuning`ì€ ê¸°í•™ìŠµ ì–¸ì–´ëª¨ë¸ì˜ ëª¨ë“  weightë¥¼ fine-tuningí•˜ì§€ ì•Šê³ , `continuous prompt embeddings`ë§Œ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤.

(3)ë˜ ë‹¤ë¥¸ ì—°êµ¬ë¡œëŠ” ìƒˆë¡œìš´ parametersë“¤ì„ Transformer blocksì´ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œ ê¸°ì¡´ parametersì„ í›ˆë ¨ì‹œí‚¤ê¸°ë„ í•˜ë©°, (4)ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” parameter-efficient fine-tuning ë°©ì‹ê³¼ ê´€ë ¨ëœ ëª¨ë“  ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ë“¤ì„ í†µí•©í•˜ê¸°ë„ í–ˆë‹¤.

****
# Problem Definition âœ
                Given a large pre-trained language model

                Return a fine-tuned model after quantizing the PLM

                Such that it outperforms the performance of the quantized model after the adaptation in terms of inference time while retaining accuracy.

****
# Challenges and Main IdeağŸ’£
**C1)** Accelerating a large LM using binarization is accompanied by a non-trivial reduction in accuracy.

**C2)** How can we wisely remove many redundant parameters in the adaptation phase?

**C3)** What is the ace in the hole for the combination of model compression and parameter-efficient techniques without sacrificing memory storage?

**Idea)** Freezes all binarized parameters while just fine-tuning its scaling factor.

****
# Proposed Method ğŸ§¿
## Quantization for AlphaTuning
ë³¸ ë…¼ë¬¸ì€ ëŠë¦¬ê³ , ë¹„ì‹¼ QAT ê¸°ë²• ëŒ€ì‹ , PTQ ê¸°ë²• ì¤‘ `Binary Coding Quantization (BCQ)`ë¼ ë¶ˆë¦¬ëŠ” binarization ê¸°ë²•ì„ í™œìš©í•œë‹¤.

Binary ì–‘ìí™”ëŠ” ê·¹ë‹¨ì ì¸ lower precisionì„ ì·¨í•¨ìœ¼ë¡œì¨, ê·¹ê°•ì˜ ì••ì¶œë¥ ì„ ë‹¬ì„±í•  ìˆ˜ ìˆì§€ë§Œ, ì •í™•ì„±ì„ ë§ì´ ìƒê¸° ë§ˆë ¨ì´ë‹¤.

### BCQ Format
![image](https://user-images.githubusercontent.com/39285147/221071718-3299a693-964b-4baa-8a7c-6cfb6fd5f507.png)

                q ì¦ê°€í• ìˆ˜ë¡, ì •í™•ë„ ìƒìŠ¹ | g ì¦ê°€í• ìˆ˜ë¡ ì••ì¶•ë¥  ì†í•´

- Weight vectors: $$w \in \mathbb{R}^g \approx \Sigma^{q}_{i=1}\alpha_i b_i$$.
    - 1 $$q$$: the number of quantization bits.
    - 2 $$\alpha \in \mathbb{R}$$ a scaling factor to be shared by $$g$$ weights.
    - 3 $$b \in \{-1,+1\}^g$$: a binary vector.
    - 4 $$g$$: (hyper-parameter) a group size or the number of weights sharing a common sacling factor.

ì—¬ê¸°ì„œ $$\alpha,\ B_i$$ëŠ” í•˜ê¸°ì˜ ê°„ë‹¨í•œ ë¯¸ë¶„ì„ í†µí•œ ìˆ˜ì‹ ì—°ì‚°ìœ¼ë¡œ ë„ì¶œí•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/39285147/221073463-80dccef8-ae3a-4227-8dd2-72bc9e82cf7f.png)

í•˜ì—¬ $$q=1$$ì˜ ê²½ìš° ìƒê¸° ì´ë¯¸ì§€ì²˜ëŸ¼ ì†ì‰¬ìš´ ì—°ì‚°ìœ¼ë¡œ $$\alpha,\ B_1$$ë¥¼ ë„ì¶œ ê°€ëŠ¥í•˜ê³ , ë‚˜ë¨¸ì§€ ê²½ìš°ëŠ” `greedy approximation` ê°™ì€ heuristic methodsë¥¼ í†µí•´ ì „ê°œí•œë‹¤.
- `Greedy approximation`: ìƒê¸° ì‹ì—ì„œ $$q>1$$ì˜ ê²½ìš°, $$q=1$$ ê²½ìš°ì˜ $$\alpha,\ B_1$$ ê°’ ë¨¼ì € êµ¬í•˜ê³  ë‚˜ì„œ, $$q=2$$ ê²½ìš°ì˜ í”¼ë¼ë¯¸í„° êµ¬í•˜ëŠ” ì‹ìœ¼ë¡œ ë‹¨ê³„ë³„ ì „ê°œ.

#### Row-wise Quantization
![image](https://user-images.githubusercontent.com/39285147/221074535-d2ede1e0-4c23-40b6-9732-efbb4adc2db6.png)

For $$W \in \mathbb{R}^{h_out \times h_in},\ g=h_in$$.

<span style="color:yellow"> CNNì—ì„œ Depth-wise convolutionì²˜ëŸ¼ row-wise ëŒ€ì‹  group-wiseì²˜ëŸ¼ ë‹¬ë¦¬í•˜ë©´ ì—°ì‚°ëŸ‰ì´ ë” ê°ì¶•ë˜ì§€ ì•Šì„ê¹Œ? </span>

Binarization: $$W \approx \Sigma^{q}_{i=1}diag(\alpha_i)*B_i$$.

![image](https://user-images.githubusercontent.com/39285147/221074822-86f3287b-2aa1-4f82-a6f8-3f73392bcbb6.png)

- Input XëŠ” ì–‘ìí™” X: $$B$$ê°€ BCQ ì ìš©ë˜ì—ˆê¸°ì—, Inputì€ êµ³ì´ ì–‘ìí™”í•˜ì§€ ì•Šì•„ë„ ì´ì§„í™” ì „ìš© XNOR ì—°ì‚°ìœ¼ë¡œ ë³µì¡í•œ FP ì—°ì‚°ì„ í”¼í•˜ê¸° ê°€ëŠ¥.
- Activationì€ ì–‘ìí™” ì§„í–‰ X 
    - <span style="color:yellow"> ë” ë‚˜ì€ quantization ìˆ˜ì¤€ì„ ìœ„í•´ activationì€ ì–‘ìí™”ì—ì„œ ì œì™¸í•œë‹¤ í•˜ì˜€ìœ¼ë‚˜, ì‚¬ì‹¤ binaryê°€ gradientë¥¼ í‘œí˜„í•˜ì§€ ëª»í•´ì„œê°€ ì•„ë‹ê¹Œ? </span>

> [XNOR ì—°ì‚°](https://hchoi256.github.io/aipaperlightweight/xnor-net/)

### Transformer Quantization
[*Medium-sized GPT-2 model withhidden size($$h$$) of 1024*]

![image](https://user-images.githubusercontent.com/39285147/221075842-1f66a2bc-4460-4703-a745-bc8ba9d551df.png)

ìƒê¸° ì´ë¯¸ì§€ì—ì„œ Transformerì˜ weightsê°€ ìƒë‹¹í•œ memory footprintë¥¼ ì°¨ì§€í•˜ëŠ” ëª¨ìŠµì´ë‹¤.

í•˜ì—¬ weightsì— ëŒ€í•´ì„œ ì–‘ìí™” ë° fine-tuningì„ ì§„í–‰í•˜ê³ ì í•œë‹¤.

ACDë¡œ ì´ì–´ì§€ëŠ” AlphaTuning êµ¬ì¡°ì—ì„œ scaling factorë§Œ ê°ê¸° ë‹¤ë¥¸ downstream taskì— fine-tuning ë˜ëŠ” ëª¨ìŠµì´ë‹¤.

ì—¬ê¸°ì„œ, $$h=1024$$ì„ì„ ê°ì•ˆí•˜ë©´, scaling factorì˜ rowì˜ í¬ê¸°ëŠ” $$q=[1~4]$$ì¸ë° ì´ê²ƒë§Œ fine-tuningí•˜ê²Œ ë˜ë©´ inference ì‹œ í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì—ì„œ ìƒë‹¹í•œ ì••ì¶• íš¨ê³¼ë¥¼ ë³´ì¼ ê²ƒì´ë‹¤.

## AlphaTuning: Efficient Fine-Tuning of Quantized Models
### AlphaTuning Principles
- **Fine-tunes** scaling factor(= affine parameter)
- **Freezes** biases, binary values B, and those of the normalization layer and embedding layer

#### Training Algorithm
![image](https://user-images.githubusercontent.com/39285147/221127362-968e9b8b-771b-46e7-89cd-8858d622d964.png)

- .$$\mathbb{I}$$: $$h_{out}$$-long all-ones vector.
- .$$g_L$$: group size.

ìˆœ/ì—­ì „íŒŒ ëª¨ë‘ quantized values ê¸°ë°˜ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.

`(3)` ì—­ì „íŒŒ ê³¼ì •ì—ì„œ Chain Rule ê¸°ë°˜ í¸ë¯¸ë¶„ìœ¼ë¡œ í”¼ë¼ë¯¸í„°ë“¤ì˜ ë¯¸ë¶„ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆê³ , `(4)` $$g_L$$ë¡œ $$\alpha$$ updatesê°€ í¬ê²Œ ë³€ë™í•˜ëŠ” í˜„ìƒì„ ìµœì†Œí™”í•˜ì—¬ ì„±ëŠ¥ì˜ ì•ˆì •ì„±ë¥¼ ë„ëª¨í•œë‹¤.

í•˜ì—¬ scaling factorì— ëŒ€í•œ fine-tuningì„ downstream taskì— ëŒ€í•´ ì§„í–‰í•˜ê²Œ ëœë‹¤.

### AlphaTuning for GPT-2
[*GPT-2 medium and larnge on WebNLG dataset*]

![image](https://user-images.githubusercontent.com/39285147/221097697-1b331cbd-c140-4be0-a548-4b48013aef57.png)

![image](https://user-images.githubusercontent.com/39285147/221127662-b7ccffe5-883f-436e-b7ac-3685df2a7040.png)

![image](https://user-images.githubusercontent.com/39285147/221128028-ca00202d-dad8-4dbe-b34b-8276084241b8.png)

![image](https://user-images.githubusercontent.com/39285147/221129587-7228534d-7689-4856-912a-dd3aac2c6f2e.png)

ìƒê¸° ì´ë¯¸ì§€ë“¤ì€ GPT-2 M, L ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œë¡œì¨, AlphaTuningì´ Figure 1) $$A \rightarrow C \rightarrow D$$ ë°©ì‹ì¸ LoRA ë° ë‹¨ìˆœ FTë³´ë‹¤ BLEU ì •í™•ì„±ì€ ê±°ì§„ ê·¼ì‚¬í•˜ë©´ì„œ ì••ì¶•ë¥ ì„ í¬ê²Œ ë‚®ì¶˜ ëª¨ìŠµì´ë‹¤.
- í•™ìŠµê°€ëŠ¥í•œ parameters í¬ê¸° ê°ì†Œ.
- BLUE ì§€í‘œ ê²½ìŸ ëª¨ë¸ë“¤ì— ê·¼ì‚¬

ê²°ê³¼ì ìœ¼ë¡œ AlphaTuning with the 3-bit quantizationê°€ ê°€ì¥ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/221132720-3477e84b-c2d8-4c7b-bfa0-961a2059e05b.png)

- `Learning rate`ì™€ `weight decay factor`ëŠ” ë³¸ ë…¼ë¬¸ì—ì„œ ì§ì ‘ best ê°’ì„ ì°¾ì•„ ê³ ì •ê°’ìœ¼ë¡œ ì´ìš©í•˜ì˜€ê³ , ë‚˜ë¨¸ì§€ ëª¨ë“  hyper-parametersëŠ” **(Hu et al., 2022) for WebNLG** ë…¼ë¬¸ì—ì„œ ì§€ì •í•œ ê°’ì„ ê³ ì •ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

#### Hyper-Parameter Selection
![image](https://user-images.githubusercontent.com/39285147/221125934-7b4d747f-07d2-44ee-8b01-80314d84ecad.png)

ëª¨ë¸ í•™ìŠµì— í™œìš©ë˜ëŠ” ë‹¤ë¥¸ hyper-parameter êµ¬ì„±ì— ëŒ€í•œ ì‹¤í—˜ë„ ì§„í–‰ë˜ì—ˆë‹¤. 

ì•ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒì²˜ëŸ¼, $$\alpha_i$$ëŠ” greedy methodsë¥¼ í†µí•´ ìˆœì°¨ì ìœ¼ë¡œ êµ¬í•´ì§„ë‹¤.
- Linear decay learning rate w/o dropout.

<span style="color:yellow"> scaling factorsì— ëŒ€í•´ì„œë„ thresholdë¥¼ ë¶€ì—¬í•˜ì—¬ ê¸°ì¤€ì¹˜ ë¯¸ë‹¬ nodeë“¤ì€ dropout ì²˜ë¦¬í•´ë„ ë˜ì§€ì•Šì„ê¹Œ? </span>

> ëª¨ë“  $$\alpha$$ë¥¼ í•œ ë²ˆì— í•™ìŠµí•˜ëŠ” ê²ƒì€ **Table 2**ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ marginal gainsë§Œì„ ì–»ìœ¼ë‹ˆ, greedy methods ì¨ë„ ë¬´ë°©í•˜ë‹¤ëŠ” ì£¼ì¥ì¸ ê²ƒ ê°™ë‹¤.

<span style="color:yellow"> Alternating vs. Greedy; ì ˆëŒ€ì ìœ¼ë¡œ Greedyê°€ ë” ì¢‹ë‹¤ê³  ë§í•  ìˆ˜ ìˆë‚˜? </span>

ê° ì‹œë„ë§ˆë‹¤ 5ë²ˆ ì§¸ epochì—ì„œ test scoresì„ ê¸°ë¡í•˜ê³ , random seedë¥¼ ë°”ê¿”ì„œ ë§ˆì¹˜ cross validationì²˜ëŸ¼ ì´ 5ë²ˆì˜ ì‹œë„ì—ì„œ ì–»ì€ test scoresë“¤ì˜ ê¸°ëŒ€ê°’ì„ êµ¬í•œë‹¤.
- ê° seedëŠ” ì‚¬ì „ì— ê³ ì •ë˜ì—ˆë‹¤.

<span style="color:yellow"> 5ë²ˆ epochë¡œ ì¶©ë¶„í•œê°€? </span>

<span style="color:yellow"> Hyper-Parameter ì„¸íŒ…ì´ ë‹¬ë¼ì§€ë©´ AlphaTuningì˜ ì„±ëŠ¥ì´ ì—­ì „ë ìˆ˜ë„ ìˆì§€ ì•Šì„ê¹Œ? </span>

****
# Experiment ğŸ‘€
## GPT-2 Models on DART and E2E
![image](https://user-images.githubusercontent.com/39285147/221133940-8f8a3722-0892-43d2-8459-bd9c42d4ce89.png)

## OPT Models on MNLI and SAMSum
![image](https://user-images.githubusercontent.com/39285147/221133978-a3f60d71-3b5b-46d8-be44-031f13b7c397.png)

****
# Open Reivew ğŸ’—
TBD

****
# Major Takeaways ğŸ˜ƒ
- First successful compression-aware parameter-efficient adaptation method
- Only scaling factors (0.1% of the model size) are enough for successful adaptations
- High scores even under 4-bit quantization throughout various LMs and downstream tasks

****
# Conclusion âœ¨
## Strength
- Stable performance on **various downstream tasks**
- **Significant infernece boost** with a binary neural network

## Weakness
- GPT-2, 1.3B OPTë³´ë‹¤ ë” í° ì´ˆê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì˜ ê²½ìš°ì—ëŠ” ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ (ì‹¤í—˜í™˜ê²½ í•œê³„).
    - ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ í´ìˆ˜ë¡ ì••ì¶•ë¥ ì´ í¬ê³  ë° ì •í™•ë„ ê°ì†Œë¥ ì´ ì ë‹¤ëŠ” ë¯¿ìŒì— ì˜ì§€í•œë‹¤. 
- AlphaTuningì€ full FT ê¸°ë²•ë³´ë‹¤ ì¶”ë¡  ì†ë„ê°€ ëŠë¦¬ê²Œ ë‚˜íƒ€ë‚œë‹¤.
    - ë³¸ ë…¼ë¬¸ì€ ì´ í•œê³„ë¥¼ AlphaTuning í•™ìŠµ ë°©ë²•ë¡ ì— ëŒ€í•œ ì •ë³´ ë¶€ì¡±ì„ í•œê³„ë¡œ ê¼½ëŠ”ë‹¤.
- ê·¸ ì™¸ í¬ìŠ¤íŠ¸ì—ì„œ <span style="color:yellow"> ë…¸ë€ìƒ‰ </span>ìœ¼ë¡œ í‘œì‹œëœ ìê°€ì§ˆë¬¸ë“¤ ë˜í•œ ì•½ì ì´ ë  ìˆ˜ë„ ìˆë‹¤.

****
# Reference
[P-tuning](https://velog.io/@seopbo/GPT-Understands-Too)

[BCQ](https://arxiv.org/pdf/2206.09557.pdf)

[XNOR ì—°ì‚°](https://hchoi256.github.io/aipaperlightweight/xnor-net/)