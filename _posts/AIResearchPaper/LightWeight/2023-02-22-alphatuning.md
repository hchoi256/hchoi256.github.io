---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"
categories: Others
tag: [Model Compression, AlphaTuning, Quantization, Pruning]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬](https://arxiv.org/pdf/2210.03858.pdf)

****
# í•œì¤„ìš”ì•½ âœ”

****
# Background ğŸ±
Self-supervised Learningì˜ ë“±ì¥ìœ¼ë¡œ, ë°ì´í„° í™•ë³´ê°€ ì‰¬ì›Œì§ì— ë”°ë¼ ì¦ê°€í•œ ë°ì´í„° ê°œìˆ˜ë§Œí¼ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆ(parameter ê°œìˆ˜) ë˜í•œ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

í˜„ ì‹œì ì—ì„œ Transformerì˜ ë§‰ê°•í•œ powerì„ ë•íƒì— AI ì „ë°˜, íŠ¹íˆ NLP ë¶„ì•¼ì— ë§ì€ ë°œì „ì´ ì‡ë”°ë¥´ê³ , ê±°ëŒ€ ì–¸ì–´ëª¨ë¸ì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ì˜€ë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ ë˜í•œ ì–¸ì–´ëª¨ë¸ ì••ì¶•ì„ targetìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.

ëª¨ë¸ì••ì¶• ì™¸ì—, zero/few-shot learning ê¸°ë°˜ ê¸°í•™ìŠµ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì˜ ì „ì ì—ë„ ë¶ˆêµ¬í•˜ê³ , ê¸°í•™ìŠµ ëª¨ë¸ training ì´í›„ fine-tuningì´ë¼ëŠ” ìƒˆë¡œìš´ adaptation ê³¼ì •ì€ downstream task ì„±ëŠ¥ í–¥ìƒì— ìˆì–´ì„œ í•„ìˆ˜ì ì´ë‹¤.

> ê° downstream taskë§ˆë‹¤ ë…ë¦½ì ì¸ ê°ê°ì˜ adaptation ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.

í•˜ì—¬ ë‹¤ì–‘í•œ downstream taskì— ë²”ìš©ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” parametersë§Œì„ fine-tuning ëŒ€ìƒìœ¼ë¡œ ì‚¼ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤ (`parameter-efficient adaptation techniques`).
- Adapter modules, low-rank adapatino, prefix-tuning, etc.

í•˜ì§€ë§Œ, ë³¸ ë…¼ë¬¸ì€ trainiable parameter ê°œìˆ˜ ìì²´ëŠ” ìƒê¸° ë°©ì‹ë“¤ë¡œ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë‚˜, ê·¸ë“¤ì´ ê¸°í•™ìŠµ ëª¨ë¸ì— ë¹„í•´ fine-tuned modelì˜ inference ì„±ëŠ¥ì´ ê·¸ë‹¤ì§€ ì¢‹ì§€ ì•Šë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤.

ë˜í•œ, í˜„ì¡´í•˜ëŠ” ëª¨ë¸ì••ì¶• ê¸°ìˆ ë“¤ì€ ë°˜ëŒ€ë¡œ parameter-efficient adapation ê¸°ë²•ì„ í™œìš©í•˜ì§€ ì•Šë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤.

ê°€ë ¹, ê¸°ì¡´ ì••ì¶• ê¸°ë²•ì¸ QATì˜ ê²½ìš°, fine-tuningê³¼ ëª¨ë¸ì••ì¶•ì„ í•¨ê»˜ ì‚¬ìš©í•œ ì ‘ê·¼ë²•ì´ì§€ë§Œ, ê¸°í•™ìŠµ ëª¨ë¸ ë§Œí¼ì˜ memory storageë¥¼ í•„ìš”ë¡œ í•œë‹¤.

****
# Introduction ğŸ™Œ
ìµœê·¼ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ê´€ì‹¬ë„ê°€ ë‚˜ë‚ ì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ ì†ì—ì„œ, ì–´ë–»ê²Œ ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ë¥¼ ì •í™•ë„ë¥¼ í•´ì¹˜ì§€ ì•Šìœ¼ë©´ì„œ ë†’ì¼ ìˆ˜ ìˆëŠ” ì§€ì— ëŒ€í•œ ë§ì€ ì—°êµ¬ê°€ ì´ë¤„ì§€ê³  ìˆë‹¤.

ì´ëŸ¬í•œ ê´€ì‹¬ì—ë„ ë¶ˆêµ¬í•˜ê³ , í•´ë‹¹ ë¶„ì•¼ëŠ” ì•„ì§ ë°í˜€ì§€ì§€ ì•Šì€ ê²ƒë“¤ì´ ë„ˆë¬´ ë§ë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ `model compression`ê³¼ `parameter-efficient adaptation` ê¸°ìˆ ì„ ê²°í•©í•œ ìƒˆë¡œìš´ ë°©ì‹ì¸ compression-aware parameter-efficient ê¸°ë²•ì¸ **AlphaTuning**ì„ ì œì‹œí•œë‹¤.

> `Parameter-efficient tuning`
>
>> ë” ì ì€ íŒŒë¼ë¯¸í„°ë§Œì„ í•™ìŠµí•˜ì—¬ downstream taskì— ëŒ€í•´ì„œ fine-tuningê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì´ë‹¤.

í•´ë‹¹ ê¸°ìˆ ì€ ê¸°í•™ìŠµ ëª¨ë¸ì— ëŒ€í•œ post-training quantizationê³¼ ì–‘ìí™”ëœ ëª‡ëª‡ì˜ parametersë“¤ì— ëŒ€í•´ì„œë§Œ target taskì— fine-tuningì‹œí‚¨ë‹¤.

AlphaTuningì€ binary-coding quantizationì„ ìˆ˜í–‰í•˜ë©°, adaptation ë‹¨ê³„ì—ì„œëŠ” binary ê°’ë“¤ì„ freezeí•˜ê³  scaling factorsë“¤ë§Œ fine-tuningì„ ì§„í–‰í•œë‹¤.

> `Binarization` ê°œë…ì´ ì–´ìƒ‰í•˜ë‹¤ë©´, [ì—¬ê¸°](https://hchoi256.github.io/aipaperlightweight/xnor-net/)ë¥¼ ì°¸ì¡°í•˜ê¸¸ ë°”ë€ë‹¤.

ë³¸ ë…¼ë¬¸ì´ ì œì‹œí•œ AlphaTuning ê¸°ë²•ì€ GPT-2ì™€ OPT ì ìš© ì‹œ, 4-bit quantization í™œìš© 10ë°° ì••ì¶• ë° 1000ë°°ì˜ parameter ê°œìˆ˜ ê°ì†Œ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

> GPTëŠ” ë‹¤ë“¤ ì•Œê³  ê³„ì‹¤í…Œê³ , OPTëŠ” Metaì—ì„œ ì„ ë³´ì¸ ì´ˆê±°ëŒ€ ì–¸ì–´ ëª¨ë¸ë¡œ ê°€ì¥ ìµœì‹  ë²„ì „ì˜ í•´ë‹¹ ëª¨ë¸ì€ 1750ì–µê°œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/220860698-05776ad1-d788-422f-a4e0-5cf52b53fcef.png)

ëª¨ë¸ì••ì¶•ê³¼ parameter-efficient adaptationì„ ìˆ˜í–‰í•˜ëŠ” ì ‘ê·¼ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤.

ìƒê¸° ì´ë¯¸ì§€ì—ì„œ, ê¸°ì¡´ ì ‘ê·¼ë²•ì€ $$A \rightarrow C \rightarrow D$$ ìˆœì„œì— í•´ë‹¹í•˜ë©°, ë§ì€ trainable parameters ë° PTQ ì‹œ downstream tasks ì„±ëŠ¥ ê°ì†Œê°€ í•œê³„ë¡œ ê¼½íŒë‹¤.

í•˜ì—¬ ë³¸ ë…¼ë¬¸ì€ ìƒˆë¡œìš´ ì ‘ê·¼ë²• $$A\rightarrow B \rightarrow D$$ì„ ì œì‹œí•˜ê³ , í•´ë‹¹ ì ‘ê·¼ë²•ì„ **AlphaTuning**ì´ë¼ ëª…ì¹­í•œë‹¤.

AlphaTuningì€ (1)ì£¼ì–´ì§„ parametersì„ `binary values`ì™€ `scaling factors`ë¡œ factorizationì„ ìˆ˜í–‰í•œë‹¤.
- scaling factorsëŠ” quantization formatsì—ì„œ ì•„ì£¼ ì‘ì€ ë¶€ë¶„ë§Œì„ ì°¨ì§€í•œë‹¤.

ì´í›„, (2)binary valuesëŠ” freezeí•˜ê³ , ì‘ì€ ë©”ëª¨ë¦¬ ë¶€ë¶„ë§Œì„ ì°¨ì§€í•˜ëŠ” scaling factorsì— ëŒ€í•´ì„œë§Œ fine-tuningì„ ì§„í–‰í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ accelerateí•œë‹¤.

$$A \rightarrow B$$ ê³¼ì •ì€ QAT ëŒ€ì‹  PTQë¥¼ ìˆ˜í–‰í•œë‹¤; QATëŠ” ë°©ëŒ€í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ í›ˆë ¨ ì‹œ computational overheadê°€ ì—„ì²­ë‚˜ë‹¤.

<span style="color:red">

QAT ê²½ìš° overhead ì¤„ì¼ ìˆ˜ë§Œ ìˆë‹¤ë©´, PTQë¥¼ ëŒ€ì²´í•´ë„ ì¢‹ì„ê¹Œ?

</span>

****
# Problem Definition âœ
                Given a large pre-trained language model

                Return a quantized model

                Such that it outperforms the performance of the original model in terms of inference time while retaining accuracy.

****
# Challenges and Main IdeağŸ’£
## C1
- Accelerating a large LM using binarization is accompanied by a non-trivial reduction in accuracy.

## C2
- How can we wisely remove many redundant parameters in the adaptation phase?

## C3
- What is the ace in the hole for the combination of model compression and parameter-efficient techniques without sacrificing memory storage?

## Idea
- Freezes all binarized parameters while just fine-tuning its scaling factor.

****
# Proposed Method ğŸ§¿


****
# Major Takeaways ğŸ˜ƒ

****
# Open Reivew ğŸ’—

****
# Experiment ğŸ‘€

****
# Conclusion âœ¨

****
# Reference