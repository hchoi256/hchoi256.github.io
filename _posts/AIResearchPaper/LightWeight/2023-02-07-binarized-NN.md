---
layout: single
title: "[ë…¼ë¬¸ë¶„ì„] Binarized Neural Networks"
categories: LightWeight
tag: [Model Compression, Light-weight, Binarization]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[ë…¼ë¬¸ë§í¬: Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/pdf/1602.02830.pdf)

****
# í•œì¤„ìš”ì•½ âœ”
- **Bit-wise Operation**: Binarizes only weigths and activations with -1 or 1 at run-time *except for the first layer*
- Input value of the first layer does not need binarization
    - Data representation << Model internal representation
        - i.e., Input: ì±„ë„ 3ê°œ (RGB), Model: 512 å¤š

****
# Introduction ğŸ™Œ
í˜„ëŒ€ì— ì´ë¥´ëŸ¬ ê¹Šì€ ì‹ ê²½ë§ì„ ìŒ“ëŠ” Deep Neural Network (DNN)ê°€ í­ë°œì ìœ¼ë¡œ ì¦ê°€í•¨ì— ë”°ë¼, ë†’ì€ ì‚¬ì–‘ì˜ GPUì— ëŒ€í•œ ìˆ˜ìš” ë˜í•œ ê¸‰ì¦í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ large modelì€ mobel devices ê°™ì€ low-power devices ì—ì„œëŠ” ì´ìš© ë¶ˆê°€ëŠ¥í•˜ì—¬ ëŒ€ì¤‘ë“¤ì€ **ê²½ëŸ‰í™”** ê¸°ë²•ì— ëŒ€í•´ ê´€ì‹¬ì„ ê°–ê¸° ì‹œì‘í•œë‹¤. Quantization (ì–‘ìí™”) ë°©ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆì§€ë§Œ, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” **binarization** ì´ì§„í™” ê¸°ë²•ì„ í†µí•œ ì–‘ìí™”ë¥¼ êµ¬í˜„í•œë‹¤.

ì´ì§„í™”ë€ ê°’ì„ ì°¸/ê±°ì§“, -1/+1 ì²˜ëŸ¼ ë‘ ê°œì¤‘ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ì´ë¶„ë²•ì ì¸ ì ‘ê·¼ ë°©ì‹ì´ë‹¤. í•˜ì—¬ ê·¸ëŸ¬í•œ ê²°ê³¼ê°’ë“¤ì´ í•„ìš”ë¡œ í•˜ëŠ” bit ê°œìˆ˜ëŠ” ì˜¤ë¡œì§€ 1ê°œì´ê¸° ë•Œë¬¸ì—, ê·¹ë‹¨ì ìœ¼ë¡œ bit ê°œìˆ˜ë¥¼ ì¤„ì„ìœ¼ë¡œì¨ inference ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.

****
# Definition âœ
            `Given` a pre-trained FP32 model
            `Returns` a (mixed-)binarized model
            `Preserving` the accuracy of the original model with higher inference speed

****
# Proposed Method ğŸ§¿
## Binarization Functions
Binarizationì„ êµ¬í˜„í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤: **(1) Deterministic binarization**ê³¼ **(2) Stochastic binarization**.

### Deterministic Binarization
![image](https://user-images.githubusercontent.com/39285147/217169809-8a51f70e-07a2-48b2-bf7e-97eb83b5bc10.png)

ìš°ë¦¬ê°€ í”íˆ ì•„ëŠ” `Sign()` ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ì´ë‹¤. Inputì´ ì–‘ìˆ˜ì´ë©´ +1, ìŒìˆ˜ì´ë©´ -1ì„ ë°°ì¶œí•œë‹¤. í•´ë‹¹ í•¨ìˆ˜ë¥¼ hidden unitsìœ¼ë¡œ í™œìš©í•˜ë©° networkì˜ ë¹„ì„ í˜•ì„±ì„ ìœ ì§€í•œë‹¤.

### Stochastic Binarization
![image](https://user-images.githubusercontent.com/39285147/217170358-1ff28116-8b51-4c4f-b03a-dd74eb37803a.png)

![image](https://user-images.githubusercontent.com/39285147/217170433-b50c4038-0312-456d-8669-e1e01dce02c3.png)

`(2)`ì˜ ë°©ë²•ì€ ì´ë¦„ ê·¸ëŒ€ë¡œ stochastic, ì¦‰ randomìœ¼ë¡œ ì–‘ìí™”ë¥¼ ìœ„í•œ bit ê°œìˆ˜ë¥¼ ìƒì„±í•œë‹¤. $$\sigma(x)$$ í•¨ìˆ˜ëŠ” ëœë¤ìœ¼ë¡œ $$[0, 1]$$ ë²”ìœ„ì˜ í™•ë¥ ë¥¼ ë°°ì¶œí•œë‹¤. ê·¸ëŸ¬í•œ í™•ë¥ ê°’ì„ ì „ë‹¬ë°›ì•„ -1/+1 outputì„ ê²°ì •í•˜ëŠ” H/Wë¥¼ ë”°ë¡œ í•„ìš”ë¡œ í•œë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í•˜ì—¬, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” S/W ì°¨ì›ì—ì„œ ì ‘ê·¼ì´ ë” ìˆ˜ì›”í•œ `(1)` ë°©ë²•ìœ¼ë¡œ í’€ì´í•œë‹¤.

## Differentiable Deterministic Quantization
![image](https://user-images.githubusercontent.com/39285147/217172970-1941f2ea-8b5b-431c-abf8-ae1623b184eb.png)

            Binarize()
            - Weight: Sign function
            - Activation: Htanh function
            - Input: N/A

`(1)` Deterministic ì ‘ê·¼ë²•ì—ì„œ ì†Œê°œëœ $$Sign()$$ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ì¡°ì‚¬í•´ë³´ë©´, ëŒ€ë¶€ë¶„ì˜ ì˜ì—­ì—ì„œ ê·¸ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì—­ì „íŒŒ ê³¼ì •ì—ì„œ Activationì˜ ë¯¸ë¶„ê°’ì„ ìš°ë¦¬ê°€ í™œìš©í•˜ëŠ”ë°, $$Sign()$$ í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì´ ëŒ€ë¶€ë¶„ 0ì´ë‹¤. ì–´ë–»ê²Œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê³ , Deterministic í•˜ê²Œ ë³€í™˜í•  ìˆ˜ ìˆì„ê¹Œ?

![image](https://user-images.githubusercontent.com/39285147/217171474-a027ac93-5baf-4950-95a1-25ef372c433e.png)

í•´ë‹¹ ë…¼ë¬¸ì€ *Hard Tanh* ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ $$1_{\|r\|<=1}$$ë¥¼ ì‚¬ìš©í•˜ì—¬ **STE(Straight-Through Estimator)**ë¥¼ êµ¬í˜„í•˜ì—¬ ìƒê¸° ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

$$g_q=Sign(r)$$

$$g_r=g_q1_{|r|<=1}$$

STEëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì— threshold operationì´ ìˆì„ ë•Œ gradient ì „ë‹¬ì„ íš¨ê³¼ì ìœ¼ë¡œ í•´ì£¼ëŠ” ë°©ë²•ìœ¼ë¡œ, ê°€ë ¹ ìƒê¸° ë¬¸ì œì— ëŒ€í•´ì„œëŠ” $$-1< x < 1$$ ì¸ ê¸°ìš¸ê¸° ê°’ì— ëŒ€í•´ì„œë§Œ BackPropagationì´ ì¼ì–´ë‚˜ê²Œ í•˜ê³ , ê·¸ ì™¸ì˜ ê°’ë“¤ì€ ì „ë¶€ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤. ì´ë¥¼ **Saturation**ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Performance ì„±ëŠ¥ ì•…í™”ì— ê¸°ì—¬í•˜ëŠ” ë¶€ë¶„ì„ Saturation ìƒíƒœë¼ ê°„ì£¼í•˜ì—¬ í•™ìŠµì—ì„œ ì œì™¸í•œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/217173042-b2f0e912-5977-4f6e-b2d9-89a1cdc03afa.png)

ìƒê¸° ì´ë¯¸ì§€ëŠ” Network ë™ì‘ ì›ë¦¬ë¥¼ í•œ ëˆˆì— íŒŒì•…í•˜ê¸° ì¢‹ì€ ì‹œê° ìë£Œì´ë‹¤. ìˆœì „íŒŒ ê³¼ì •ì—ì„œì˜ Binarize() í•¨ìˆ˜ë¥¼ ë³´ë©´, Activationì— ëŒ€í•´ì„œë§Œ Htanh í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤. ê·¸ ì´ìœ ëŠ” Activationì˜ ë¯¸ë¶„ê°’ì´ ì—­ì „íŒŒì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì •ì—ì„œ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì—¬ $$Sign()$$ í•¨ìˆ˜ëŠ” ê·¸ ë¯¸ë¶„ê°’ì´ ëŒ€ë¶€ë¶„ì˜ ì§€ì—­ì—ì„œ 0ì´ ëœë‹¤ëŠ” ì ì—ì„œ, Activation Binarizationì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì“°ì„ì´ ì•„ë‹ ê²ƒì´ë‹¤.

## SBN(Shift based batch normalization)
BN ê³¼ì •ì€ ë¶„ì‚°ê°’ êµ¬í•˜ëŠ” ê³¼ì •ì„ ìˆ˜ë°˜í•˜ëŠ”ë°, ì—¬ê¸°ì„œ í–‰ë ¬ ê³±ì…ˆ(ì œê³±) ì—°ì‚°ì´ ì‚¬ìš©ëœë‹¤. ê±°ì§„ ëª¨ë“  í”¼ë¼ë¯¸í„°ê°€ bit-wise arithmaticìœ¼ë¡œ ì „ê°œë˜ëŠ” Binarized NN ëª¨ë¸ì—ì„œëŠ” í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì´ ë¶ˆí•„ìš”í•˜ë‹¤. ëŒ€ì‹ , ìš°ë¦¬ëŠ” ì†ì‰½ê²Œ bit-wise ì—°ì‚°ìë¥¼ í™œìš©í•˜ì—¬ ì£¼ì–´ì§„ inputì— ëŒ€í•œ ê±°ë“­ì œê³± ê°’ì„ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

> 100 << 2 $$\rightarrow$$ 10000

![image](https://user-images.githubusercontent.com/39285147/217182568-da564c2a-ec58-4894-8179-37248504a1f7.png)

í•´ë‹¹ ë…¼ë¬¸ì€ $$AP2()$$ í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ì œì‹œí•˜ì—¬, ê¸°ì¡´ BNì˜ varianceì™€ normalize ë¶€ë¶„ì˜ í–‰ë ¬ ê³±ì…ˆ ê³¼ì •ì„ bit-wise ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.

![image](https://user-images.githubusercontent.com/39285147/217181518-9df0de76-9163-4c03-b48f-bd1402247011.png)

Bit shiftë¥¼ í†µí•´ 2ì˜ ê±°ë“­ì œê³±ì„ ì‰½ê²Œ ì—°ì‚° ê°€ëŠ¥í•˜ë‹¤. ì´ëŠ” í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜ë°˜í•˜ëŠ” ê¸°ì¡´ BNì˜ ê·¼ì‚¿ê°’ì„ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤ëŠ” ë§ì´ë‹¤.

![image](https://user-images.githubusercontent.com/39285147/217181700-5c8829ec-52f9-402c-9151-5bfe7310536d.png)

ì˜ˆë¥¼ ë“¤ì–´, ì‹­ì§„ìˆ˜ 4ëŠ” ì´ì§„ìˆ˜ë¡œ 100ì´ë‹¤. ìƒê¸° ì´ë¯¸ì§€ì—ì„œ, $$+1 \times 2^2$$ì€ bit-wise ì—°ì‚°ìœ¼ë¡œ $$100 << 1$$ê³¼ ë™ì¹˜ì´ë‹¤.

ë”°ë¼ì„œ, $$AP2()$$ í•¨ìˆ˜ê°€ ë°°ì¶œí•˜ëŠ” ì •ë³´ë¥¼ ë°›ì•„ $$<<>>$$ ì—°ì‚°ìì™€ ì–´ìš°ëŸ¬ì ¸ ì£¼ì–´ì§„ Inputì— ëŒ€í•œ ì œê³±ì„ êµ¬í•œë‹¤.

## SAM(Shift based AdaMax)
![image](https://user-images.githubusercontent.com/39285147/217182903-f8de2a68-7008-4953-8d40-88a87f62c50a.png)

í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ìµœì í™” í•¨ìˆ˜ë¡œ ì±„íƒí•˜ê³  ìˆëŠ” ADAM OptimizerëŠ” Matrix Multiplicationë¥¼ ë§ì´ í•„ìš”ë¡œ í•œë‹¤.

í•˜ì—¬ bit-wise ì—°ì‚°ì„ í†µí•´ í–‰ë ¬ ê³±ì…ˆì„ ìµœì†Œí™”í•˜ê³ ì **Shift based AdaMax**ë¥¼ ë„ì…í•œë‹¤.

ì›ë¦¬ëŠ” SBNê³¼ ë™ì¼í•˜ë©°, ì •í™•ë„ ì†ì‹¤ì—†ì´ í–‰ë ¬ ê³±ì„¼ì„ bit-wise ì—°ì‚° ê°€ëŠ¥í•˜ë‹¤.

## First Layer
![image](https://user-images.githubusercontent.com/39285147/217183231-d15d4b68-beb5-42dd-9bcc-ded8721fbef2.png)

## First Layer: Binary Operation ë¬´ì‹œ
í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì²« ë²ˆì§¸ layerì— ë“¤ì–´ì˜¤ëŠ” inputë“¤ì€ ê±°ì§„ bit operationì´ ì ìš©ë˜ì§€ ì•ŠëŠ” non-binary imagesë¼ëŠ” ì ì„ ì‹œì‚¬í•œë‹¤.

ì´ë¯¸ì§€ ë°ì´í„°ëŠ” ê³ ì‘ 3ê°œì˜ channel (RGB)ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ì§ê´€ì ìœ¼ë¡œ í•˜ê¸° ê´€ê³„ì‹ì„ ë„ì¶œí•  ìˆ˜ ìˆë‹¤.

                Input data representation(3) << Model representation(å¤š)

ì²« ë²ˆì§¸ Layerì˜ outputì„ binary í˜•íƒœë¡œ ë°°ì¶œí•˜ì—¬ ë‚˜ë¨¸ì§€ layersë“¤ì€ ëª¨ë‘ binary inputë¥¼ ë°›ëŠ”ë‹¤.

í•˜ì—¬ ì²« ë²ˆì§¸ LayerëŠ” binary operationë¥¼ ë¬´ì‹œí•˜ê±°ë‚˜, í˜¹ì€ íŠ¹ìˆ˜í•œ ë°ì´í„° ê°€ê³µ ì²˜ë¦¬(8-bit ê³ ì •ì†Œìˆ˜ì )ë¥¼ ì·¨í•œë‹¤.

## First Layer: XNOR Operation
[*Bitwise XNOR Convolution*]

![image](https://user-images.githubusercontent.com/39285147/217767638-f1b2bc36-084a-4bea-bdb6-3340b53076f4.png)

$$input = 8-bit <XNOR> Binarized Weight$$

ì´ë¯¸ì§€ ë°ì´í„°ë“¤ì— ëŒ€í•´ì„œ ìš°ë¦¬ëŠ” Convolution(í•©ì„±ê³±)ì„ í™œìš©í•˜ì—¬ í–‰ë ¬ ê³±ì…ˆ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.

ì´ ë•Œ, Convolution ì—°ì‚°ì„ bit-wiseí•˜ê²Œ ë³€í™˜í•œ ìˆ˜ì‹ì´ ë°”ë¡œ **XNOR bit operation**ì´ë‹¤.

****
# Experiment ğŸ‘€
![image](https://user-images.githubusercontent.com/39285147/217183021-554b5794-555c-453b-9edd-f186fdb364dc.png)

- *ë°ì´í„°ì…‹*: MNIST, CIFAR-10, SVHN
- *ì‚¬ìš©ëª¨ë¸*: BNN(Torch7, Theano)

****
# Conclusion âœ¨
## Strengths
- Drastically **reduce memory size** during forward pass
- **Power efficiency**
    - Accesses and replaces most arithmetic operations with bit-wise operations
- Available on-line

## Weaknesses
- `Training` Requires **more epoches** than the original NN in terms of the same accuracy
    - ëŒ€ì‹ , í•˜ë‚˜ì˜ operationì— ëŒ€í•œ ì—°ì‚° ì†ë„ ë° memory sizeê°€ ì ë‹¤.
- Activation function ìœ¼ë¡œ ì‚¬ìš©ëœ í•¨ìˆ˜ì¸ htahnì˜ í˜•íƒœì  íŠ¹ì„±(ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ëŠ” ë¶€ë¶„ ì¡´ì¬) ë•Œë¬¸ì— **Gradient Vanishing** í˜„ìƒ ì—¬ì „íˆ ì”ì¬

****
# Reference ğŸ’•