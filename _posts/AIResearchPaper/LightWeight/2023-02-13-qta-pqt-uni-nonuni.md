---
layout: single
title: "Quantization ê¸°ë³¸ê¸°"
categories: LightWeight
tag: [Quantization, Uniform, Non-uniform, QAT, PTQ]
toc: true
toc_sticky: true
toc_label: "ì­ŒìŠ¤log"
#author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

****
# Uniform vs Non-uniform ğŸ™Œ
![image](https://user-images.githubusercontent.com/39285147/218532636-6bfaf954-949f-4ff8-8530-051745fbec47.png)

## Uniform(ê· ì¼ ì–‘ìí™”)
![image](https://user-images.githubusercontent.com/39285147/218531666-6fe2cb58-736c-4fad-b449-2138d85f9ccc.png)

- FP Input, ê°€ì¤‘ì¹˜ë¥¼ INT, ê³ ì •ì†Œìˆ˜ì  í˜•íƒœë¡œ ë³€í™˜
- step size ê· ì¼
- ëŒ€ì¹­ì  ì–‘ìˆ˜/ìŒìˆ˜ êµ¬ì¡°
- `Round-off Error`: ì–‘ìí™” ìŠ¤í… í¬ê¸° ì ˆë°˜
    - ì§ìˆ˜ or í™€ìˆ˜ step ì–‘ìí™”

### (1) Affine Transfrom
![image](https://user-images.githubusercontent.com/39285147/218533664-d0255d17-a412-43c3-aa51-e9405c16ab99.png)

- ë¹„ëŒ€ì¹­(Asymmetric)
- `(2)` ë³´ë‹¤ ì •í™•ë„ â†‘, Cost íš¨ìœ¨ â†“


$$f(x)=s\times x+z \\\\ s=\frac{2^b-1}{\alpha-\beta} \\\\ z=-round(\beta \times s)-2^{b-1}$$

$$z$$: zero-point, ë³€í™˜ ì „ 0ì˜ ìœ„ì¹˜ê°€ ë³€í™˜ í›„ ì–´ëŠ ì ìœ¼ë¡œ ëŒ€ì‘ë˜ëŠ”ì§€ í‘œí˜„
$$s$$: scaling factor

$$quantize(x,b,s,z)=clip(round(s\times x+z),-2^{b-1},2^{b-1}-1) \\\\  dequantize(x_q,s,z)=\frac{x_q-z}{s}$$

> `Fake Image/Quantization`: ì–‘ìí™” ë³€í™˜ í›„ ê°’ë“¤ì´ë‹¤.

### (2) Scale Transform
![image](https://user-images.githubusercontent.com/39285147/218533713-ffa45cd5-e3e4-43e9-8074-7e2d231e4f3d.png)

- ëŒ€ì¹­(Symmetric)
- `(1)` ë³´ë‹¤ ì •í™•ë„ â†“, Cost íš¨ìœ¨ â†‘

$$f(x)=s\times x \\\\ s=\frac{2^{b-1}-1}{\alpha}$$

$$quantize(x,b,s)=clip(round(s\times x),-2^{b-1}+1,2^{b-1}-1) \\\\ dequantize(x_q,s)=\frac{x_q}{s}$$

## Non-uniform(ë¹„ê· ì¼ ì–‘ìí™”)
- step size ë¹„ê· ì¼
    - ì…ë ¥ ì‹ í˜¸ ë ˆë²¨ â†“ $$\rightarrow$$ ì–‘ìí™” ê³„ë‹¨ ê°„ê²© â†“
    - `Code Book`: ë§µí•‘ ë°©ì‹ì„ ê²°ì •í•˜ëŠ” ì‚¬ìš©ìê°€ ì •í•´ë†“ì€ Rule 

                ìŒìˆ˜: 0
                0~1: 1
                >= 1: 2

****
# Quantization Aware Training ğŸ’œ


****
# Post Training Quantization âœ

****
# Reference
## Quantization Granularity
- **Activation**: tensor quantization.
- **Weights**: tensor or chennel quantization.
