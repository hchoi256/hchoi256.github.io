---
layout: single
title: "[논문분석] Teaching Assistant Knowledge Distillation"
categories: LightWeight
tag: [Model Compression, Light-weight, Knowledge Distillation]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[논문링크: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)

****
# 한줄요약 ✔
- XNOR Net이란 **Weight과 input에 대한 binarization 수행을 통해 Convolutions 연산을 binary operations으로 대체**하여 58배 빠른 합성곱 연산 속도와 32배 memory saving 효과를 거둔 네트워크이다.
- 최초로 ImageNet 분류 평가 진행
- CPU 사용
- Binarized Neural Network 2016과 비교해서 input도 binary 표현한다는 점에서 차이

****
# Introduction 🙌
![image](https://user-images.githubusercontent.com/39285147/218665627-4e7bdfd9-c7b4-42a2-ae73-a87442b56267.png)

BinaryNet은 binarized weights와 binarized activations을 활용한 `BinaryConnect`를 출시한다.

하지만, `BinaryConnect`는 small dataset에 대해서만 SOTA 성능을 달성하고 large dataset에는 낮은 정확성을 보여줬다.

해당 논문에서 제안된 XNOR NET은 네트워크 구조 및 binarization method 자체를 변경하여 대규모 데이터셋에도 좋은 성능을 보인다.

****
# Definition ✏
            `Given` a pre-trained FP32 model
            `Returns` a (mixed-)binarized model
            `Preserving` the accuracy of the original model with higher inference speed

****
# Proposed Method 🧿
## XNOR 연산
![image](https://user-images.githubusercontent.com/39285147/218667767-97aa66a1-83a9-4266-8c2a-048f823de249.png)

Binary 값들로 표현된 행렬 간의 합성곱 연산은 간단하게 **XNOR 연산**으로 구할 수 있다.

XNOR 연산에는 행렬 곱셈이 수반되지 않아서 Cost가 훨씬 절약된다.

## XNOR Net
XNOR-Net은 input과 weights에 대하여 binarization을 수행하는 네트워크이다.

$$X^TW\approx \beta H^T \alpha B$$

$$(H=binarized input_X,\ B = binarized W,\ \alpha= scale factor for W,\ \beta=scale factor for X)$$

상기 식에서 $$X^TW$$는 hidden nodes를 구하는 순전파 과정을 나타낸다.

$$\alpha^*,H^*,\beta^*,B^*=argmin_{\alpha,H,\beta,B}||X\odot W-\beta \alpha H \odot B||$$.

`argmin`을 취하여 실제값($$X\odot W$$)과 예측값($$\beta \alpha H \odot B$$)의 차이가 최소가 되는 피라미터들을 구할 수 있을 것이다.

> 실제 계산은 $$||\cdot||$$을 미분한 값을 0이 되게 만드는 피라미터들을 구하면 된다.

![image](https://user-images.githubusercontent.com/39285147/218670838-6a159169-6fe1-4750-8771-549d877e60ef.png)

근데 합성곱은 겹쳐지는 부분에서 중복 연산이 많이 발생하기 때문에, channel wise 정보들의 평균값을 scale factors로 활용한다.

$$\alpha^* \beta^*=(\frac{||X||_{l1}}{n})(\frac{||W||_{l1}}{n})$$.

$$H^*\odot B^*=sign(X) \odot sign(W)$$.

****
# Experiment 👀

****
# Conclusion ✨

****
# Reference 💕