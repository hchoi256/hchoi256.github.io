---
layout: single
title: "[논문분석] Teaching Assistant Knowledge Distillation"
categories: AIPaperLightWeight
tag: [Model Compression, Light-weight, Knowledge Distillation]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/qntzn.png
sidebar:
    nav: "docs"
---

[논문링크: Improved Knowledge Distillation via Teacher Assistant](https://arxiv.org/abs/1902.03393)

****
# 한줄요약 ✔
기존 지식증류 방식의 문제점은 Teacher~Student 사이 차이가 심할 경우, 정확도가 떨어지는 경향이 있는데, 제안된 방법 (TAKD)는 Teacher과 Student 사이 `Teaching Assistant`라는 새로운 중간자를 추가한다.

T~TA 그리고 TA~S, 총 두 번의 지식증류를 통하여 기존 Teacher과 Student 사이 간극을 좁힌다 (T: Teacher, TA: Teaching Assistant, S: Student).

****
# Introduction 🙌
최근 딥러닝 모델 크기가 커지면서 정확도 향상이 잇달았으나, 말단 장비들에서 해당 모델을 적용시키기 어려워 모델 압축에 대한 관심도가 높아졌다.

모델 압축에는 여러 가지 방법이 있으나, 이 글에서는 **향상된 지식 증류** 방법을 소개한다.

## Knowledge Distillation (KD)
![image](https://user-images.githubusercontent.com/39285147/218952582-3a96a0f5-9164-4944-bfed-213e2a5d218a.png)

![image](https://user-images.githubusercontent.com/39285147/218954499-dad9bb52-ee05-4a89-9f72-ea1005130014.png)

거대 모델 (Teacher)에서 작은 모델 (Student)로 지식(가중치)를 전달하여, Student가 거대 모델의 예측을 모방(mimic)하도록 한다.

> **전이 학습**과의 차이점은 전이 학습은 기학습 모델이 다른 task를 수행하여 얻은 가중치를 전달하지만, 지식증류는 같은 task를 수행한 거대 기학습 모델을 이용한다.

### Student Network Loss
![image](https://user-images.githubusercontent.com/39285147/218953667-0fc3b4ed-d509-4249-b729-10f840ccf3aa.png)

$$H(): Cross\ entropy$$

$$a_s: input$$

$$y_r: ground-truth label$$

### KL Divergence
![image](https://user-images.githubusercontent.com/39285147/218953966-9f88bd20-acb7-4309-ac8c-f275ad5a3cf4.png)

$$y_s=softmax(\frac{a_s}{\tau})$$: Softened ouput of Student

$$y_r=softmax(\frac{a_t}{\tau})$$: Softened ouput of Teacher

$$\tau$$: Temperature parameter, 증류할 지식량 결정  

### KL Loss
![image](https://user-images.githubusercontent.com/39285147/218954379-f99fb037-23c1-43ab-bb84-0d1cea5aafb1.png)

**Soft** Student predictions ~ **soft** Teacher labels 사이의 예측 간극 좁히기 위함.

****
# Definition ✏
                `Given` a pre-trained model as Teacher
                `Find` the parameters of Student
                `Such that` the parameters maximize the accuracy of Student

****
# Proposed Method 🧿
## TAKD
![image](https://user-images.githubusercontent.com/39285147/218954794-ab46c33e-d314-448e-ad5d-61b6beb7fb3a.png)

- Student 성능은 Teacher 크기에 따라 변한다
- KD 효과는 Student 크기에 따라 변한다.

## Best TA Size
![image](https://user-images.githubusercontent.com/39285147/218955761-a7cd5a33-75e5-421b-a236-fc8886afb25f.png)

![image](https://user-images.githubusercontent.com/39285147/218955902-4767f8c2-e33e-45c6-919d-57f56fe22666.png)

## Theoretical Analysis
![image](https://user-images.githubusercontent.com/39285147/218956915-13a798f9-2ec0-49f5-892e-c668c2314c43.png)

![image](https://user-images.githubusercontent.com/39285147/218957010-204c4ca3-3e4a-44cb-b7f0-1187fd164450.png)

![image](https://user-images.githubusercontent.com/39285147/218957071-e9fbd9ab-c56a-48c7-9264-14838b9015a6.png)

TAKD는 타 KD 방법들에 비해 Local minima 지점에서 더 납작한 표면을 형성한다 (= 더 잘 수렴한다 = 학습 완료가 더 잘된다).

****
# Experiment 👀
![image](https://user-images.githubusercontent.com/39285147/218955621-c4901e88-7e0d-4659-bfc0-d29c25eee5be.png)

![image](https://user-images.githubusercontent.com/39285147/218955694-13b68973-156a-4185-a1b0-17e71d063ef4.png)

![image](https://user-images.githubusercontent.com/39285147/218956041-e9ce64d0-3c4b-4ff3-a96e-a4e307ea3bd6.png)


****
# Conclusion ✨
## Strength
- Student ~ Teacher 사이 간극 줄여서 더 높은 정확도 끌어냄
- TAKD는 타 KD 방법들에 비해 높은 성능

****
# Reference 💕