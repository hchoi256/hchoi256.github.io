---
layout: single
title: "PART 2: 언어모델, Seq2Seq, ConvS2S, Attention, Self-Attention"
categories: BERT
tag: [NLP Language Model, Seq2Seq, Attention]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# 언어 모델 (Language Model, LM)
'자연어'의 법칙을 컴퓨터로 모사하는 모델로, 다음에 등장할 단어 예측을 수행한다(= '맥락'을 고려한다).

## Markov 확률 모델
![image](https://user-images.githubusercontent.com/39285147/183536291-32897298-797e-4fd8-aac9-4bcc9ef3459e.png)

이전 단어의 형태를 통하여 확률적으로 다음에 나올 단어를 예측한다

가령, 'like' 다음에 'rabbit'이라는 단어가 나타날 확률은 주어진 학습 데이터에 기반하여 **33%**로 나타난다.

이러한 과정을 ***Language Modeling***이라 일컫는다.

****
# RNN (Recurrent Neural Network) 모델
![image](https://user-images.githubusercontent.com/39285147/183536890-b8d596a2-c3c0-4c90-8193-ac96f8b8cdb0.png)

Markov 체인 모델을 **딥러닝**에 접목하여 확장한 모델이 바로 RNN이다.

현재 state를 결정하기 위하여 이전 state의 hidden layer 정보를 인풋으로 받는다.

이를 통해 **앞선 문맥을 고려한** 최종 출력 vector(Context vector)를 만든다.

하나의 RNN을 '인코더', 다른 하나의 RNN을 '디코더'라는 모듈로 명명하고, 두 개의 RNN을 연결해서 사용하는 **인코더-디코더 구조**이다.
- 인코더-디코더 구조: 주로 입력 문장과 출력 문장의 길이가 다를 경우에 사용하는데, 대표적인 분야가 번역기나 텍스트 요약과 같은 경우이다.

> 보다 자세한 내용은 [여기](https://github.com/hchoi256/ai-terms/blob/main/README.md)를 참조하자.

## Seq2Seq
![image](https://user-images.githubusercontent.com/39285147/183537292-5cfe7c3f-d380-4e0c-aa20-266341ae5d9a.png)
![image](https://user-images.githubusercontent.com/39285147/185516796-59b5f330-c2b1-40c0-9bcb-14c43a31af03.png)

> LSTM에 관한 자세한 내용은 [여기](https://github.com/hchoi256/ai-terms/blob/main/README.md)를 참조하자.

- **Encoder layer**: Context vector 획득
- **Decoder layer**: Context vecotr 해독/해석

인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 컨텍스트 벡터(context vector)라고 지칭한다.

입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송하고, 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력한다.

**번역**에 자주 사용되어, 자연어를 인식(encoding)하고 알맞은 언어로 번역(decoding)하여 출력한다.

그 외에도, 챗봇(Chatbot), 기계 번역(Machine Translation), 내용 요약(Text Summarization), STT(Speech to Text) 분야에서 활용되고 있다.

하지만, sequence가 길어진다면, **초기 단어에 대한 정보를 점차 소실**되는 구조적 한계점, **병렬화 불가능**, **중요하지 않은 token의 정보**가 최종 출력에 영향을 주는 단점이 존재한다.

또한, RNN의 고질적인 문제인 **기울기 소실(vanishing gradient) 문제**가 존재한다.

## Convolution Seq2Seq (ConvS2S)
![image](https://user-images.githubusercontent.com/39285147/186263776-3dfac63c-5ea1-4736-8e19-52dbbd40809a.png)

- *Residual Connection*: 입력 정보가 다음 레이어에 전달될 벡터에 더해져서 최종 출력에 영향을 줄 수 있도록 하기 위함.

기존 RNN의 '직렬화'라는 한계점을 타파하고자 RNN 구조를 제거하고 'Convolution'으로 대체한 경우이다.

![image](https://user-images.githubusercontent.com/39285147/186263248-1acb5a60-ae13-4df1-a1ba-cbe0d47fe9cc.png)

순환 신경망의 연결선을 제거하여 이전 출력을 기다릴 필요없이, 가령 상기 사진에서는 3개의 단어 토큰 단위로 새로운 벡터들을 만드는 식으로 동작한다.

이러한 아키텍쳐를 구현함으로써 질적으로도 뛰어날 뿐 아니라, 계산의 **병렬화**를 가능하게 하여 **훈련 시간의 단축**이라는 쾌거를 이루어낼 수 있었다.

## Attention 모델
![image](https://user-images.githubusercontent.com/39285147/183538147-9eb2a2cf-b06c-4994-9a3b-11a4013a6fc8.png)

모든 token을 고려하는 Seq2Seq의 한계점을 보완하기 위하여, **특정 중요 단어/입력에 대한 집중도**를 부여한다.

**RNN의 셀 각각이 도출하는 Output을 모두 활용**하여 더 dynamic하게 Context Vector를 생성한다.

![image](https://user-images.githubusercontent.com/39285147/185517327-2f41fb45-1eb5-4ce6-ba0c-af45220f8114.png)

- **Query**: 단어에 대한 가중치
- **Key**: 단어 ~ Query와의 연관성을 나타내는 가중치
- **Value**: 의미에 대한 가중치 (집중할 단어) 

> ![image](https://user-images.githubusercontent.com/39285147/185522397-1fafee32-76bc-4cac-9d6c-cee081e341b1.png)

Query는 Decoder Cell(FCL)에서 도출되고, Key와 Value는 Encoder Cell에서 도출된다.

하나의 디코더에서 예측 단어를 출력하기 위한 과정:
- 1. RNN 셀들과 앞선 단어 문맥을 고려한 출력값에 대한 **score를** 계산한다
- 2. Softmax 활성화 함수를 사용해서 **Attention Weight**를 도출한다
- 3. 각 셀에 대한 결과값과 attention weight을 곱한 합계로 context vector를 구한다.
- 4. Context Vector를 decoder에 넣는다.
- 5. 최종 output의 분류 결과가 안 좋다면, score를 조정하여 **decoder가 해석하기에 용이한 attention weight를 재계산한다**.
  - Input과 output 사이의 연관성을 가늠해볼 수 있다.

인코더에서의 전체 입력 문장을 다시 한 번 참고하여 각 디코더에 대하여 상기 과정을 반복한다.

> 흔히, 딥러닝 모델이 내놓은 결과의 원인을 설명할 수 없는([Blackbox](https://github.com/hchoi256/ai-terms/blob/main/README.md)) 경우가 많음에도 불구하고, Attention 모델은 attention weight 덕분에 해당 설명이 가능하다.

하지만, RNN 신경망 구조를 기반으로 하기 때문에 이전 state의 결과를 기다려야 한다는 점에서 **연산 속도가 느리다**.

****
# Self-attention 모델
![image](https://user-images.githubusercontent.com/39285147/185520452-e27a130d-510f-4d3a-a12d-adea5378a164.png)

RNN의 시계열 학습에서 벗어나 Attention 구조만으로 학습에 임한다.

Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 **input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식**으로 동작한다.

![image](https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png)

**인코더 내부에서 인코더 한 토큰을 재구성**하는 개념이다.
- **Query, Key, Value가 동일한 시퀀스**를 가진다.

![image](https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png)

- Score = Query와 Key 내적

최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.

이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.

![image](https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png)

> 여러 번의 Self-attention을 **병렬처리**로 수행하여 **빠른 속도**를 끌어낸 것이 바로 **multi-head attention**이다.

이제 [**Trnasformer, 그리고 BERT 언어 모델**](https://hchoi256.github.io/bert/bert-3/)에 대해 알아보자.

# Reference
[Deep Learning for NLP - KAIST 주재걸 교수님](https://www.youtube.com/watch?v=JqkfT1s60cI&list=PLep-kTP3NkcOjOS1a30UNW-tH2FSoGYfg&index=1)