---
layout: single
title: "PART 2: 언어모델, Seq2Seq, Attention Models"
categories: BERT
tag: [NLP Language Model, Seq2Seq, Attention]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# 언어 모델 (Language Model, LM)
'자연어'의 법칙을 컴퓨터로 모사하는 모델로, 다음에 등장할 단어 예측을 수행한다(= '맥락'을 고려한다).

## Markov 확률 모델
![image](https://user-images.githubusercontent.com/39285147/183536291-32897298-797e-4fd8-aac9-4bcc9ef3459e.png)

이전 단어의 형태를 통하여 확률적으로 다음에 나올 단어를 예측한다

가령, 'like' 다음에 'rabbit'이라는 단어가 나타날 확률은 주어진 학습 데이터에 기반하여 **33%**로 나타난다.

이러한 과정을 ***Language Modeling***이라 일컫는다.

****
# RNN (Recurrent Neural Network) 모델
![image](https://user-images.githubusercontent.com/39285147/183536890-b8d596a2-c3c0-4c90-8193-ac96f8b8cdb0.png)

Markov 체인 모델을 **딥러닝**에 접목하여 확장한 모델이 바로 RNN이다.

현재 state를 결정하기 위하여 이전 state의 hidden layer 정보를 인풋으로 받는다.

이를 통해 **앞선 문맥을 고려한** 최종 출력 vector(Context vector)를 만든다.

하나의 RNN을 '인코더', 다른 하나의 RNN을 '디코더'라는 모듈로 명명하고, 두 개의 RNN을 연결해서 사용하는 **인코더-디코더 구조**이다.
- 인코더-디코더 구조: 주로 입력 문장과 출력 문장의 길이가 다를 경우에 사용하는데, 대표적인 분야가 번역기나 텍스트 요약과 같은 경우이다.

> 보다 자세한 내용은 [여기](https://github.com/hchoi256/ai-terms/blob/main/README.md)를 참조하자.

## Seq2Seq
![image](https://user-images.githubusercontent.com/39285147/183537292-5cfe7c3f-d380-4e0c-aa20-266341ae5d9a.png)
![image](https://user-images.githubusercontent.com/39285147/185516796-59b5f330-c2b1-40c0-9bcb-14c43a31af03.png)

> LSTM에 관한 자세한 내용은 [여기](https://github.com/hchoi256/ai-terms/blob/main/README.md)를 참조하자.

- **Encoder layer**: Context vector 획득
- **Decoder layer**: Context vecotr 해독/해석

인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 이를 **컨텍스트 벡터(context vector)**라고 지칭한다.

입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송하고, 각 디코더는 컨텍스트 벡터를 받아서 Output Layer를 거쳐서 도출된 번역된 단어를 한 개씩 순차적으로 출력한다.

이 때, 다음 디코더 레이어로 이전 디코더의 출력이 입력으로써 전달된다.

**번역**에 자주 사용되어, 자연어를 인식(encoding)하고 알맞은 언어로 번역(decoding)하여 출력한다.

그 외에도, 챗봇(Chatbot), 기계 번역(Machine Translation), 내용 요약(Text Summarization), STT(Speech to Text) 분야에서 활용되고 있다.

하지만, sequence가 길어진다면, **초기 단어에 대한 정보를 점차 소실**되는 구조적 한계점, **병렬화 불가능**, **중요하지 않은 token의 정보**가 최종 출력에 영향을 주는 단점이 존재한다.

또한, RNN의 고질적인 문제인 **기울기 소실(vanishing gradient) 문제**가 존재한다.

# Attention Models (RNN)
![image](https://user-images.githubusercontent.com/39285147/185522397-1fafee32-76bc-4cac-9d6c-cee081e341b1.png)

입력 시퀀스의 마지막 시점의 벡터에 모든 정보를 모두 담기가 어려우므로, **모든 입력 시퀀스의 정보를 조합하여 각 출력 단어**를 생성한다.

모든 token을 고려하는 기존 Seq2Seq의 한계점을 보완하기 위하여, **특정 중요 단어/입력에 대한 집중도(attention/alignmnet weight)**를 부여한다.

> 흔히, 딥러닝 모델이 내놓은 결과의 원인을 설명할 수 없는([Blackbox](https://github.com/hchoi256/ai-terms/blob/main/README.md)) 경우가 많음에도 불구하고, Attention 모델은 attention weight 덕분에 해당 설명이 가능하다.

하지만, RNN 신경망 구조를 기반으로 하기 때문에 이전 state의 결과를 기다려야 한다는 점에서 여전히 **연산 속도가 느리다**.

## Seq2Seq with Attention
![image](https://user-images.githubusercontent.com/39285147/183538147-9eb2a2cf-b06c-4994-9a3b-11a4013a6fc8.png)

**RNN의 셀 각각이 도출하는 Output을 모두 활용**하여 더 dynamic하게 Context Vector를 생성한다.

![image](https://user-images.githubusercontent.com/39285147/186281646-1f620a26-7a61-4e9a-81de-9dd9b88b75e2.png)

- **Q = Query** : t 시점의 디코더 셀에서의 은닉 상태
- **K = Keys** : 모든 시점의 인코더 셀의 은닉 상태들
- **V = Values** : 모든 시점의 인코더 셀의 은닉 상태들

Query는 Decoder Cell(FCL)에서 도출되고, Key와 Value는 Encoder Cell에서 도출된다.

여기서, Attention Weight은 **해당 Key에 대응되는 Query에 대한 유사도**이다.
- 유사도 계산 함수: dot product, splice, detector, etc.

> Attention(Q, K, V) = Attention Value

하나의 디코더에서 예측 단어를 출력하기 위한 과정:
1. RNN 셀들과 앞선 단어 문맥을 고려한 출력값에 대한 **similarity score를** 내적으로 계산한다
2. Softmax 활성화 함수를 사용해서 **Attention Weight**를 도출한다 (Decoder).
3. 각 셀에 대한 결과값과 업데이트된 attention weight을 곱한 합계로 context vector를 구한다.
4. 이렇게 구한 Context Vector를 다음 decoder 레이어에서 concat하여 하나의 벡터를 만든다.
5. 이 벡터는 인코더로부터 얻은 정보를 활용하여 더 나은 예측을 하기위한 예측 연산의 입력으로 사용된다.
5. 최종 output의 분류 결과가 여전히 안 좋다면, 다시 score를 조정하여 **decoder가 해석하기에 용이한 attention weight를 재계산한다**.

인코더에서의 전체 입력 문장을 다시 한 번 참고하여 각 디코더에 대하여 상기 과정을 반복한다.

> 다양한 종류의 Attention
>
> ![image](https://user-images.githubusercontent.com/39285147/186280401-2314828d-877c-4056-b578-872f229e6b52.png)

### 기계번역 사례
![image](https://user-images.githubusercontent.com/39285147/186273750-3ecf29f3-289c-44aa-a345-afe3a325a265.png)

상기 모형은, 가독성 좋게 heatmap으로 단어 토큰별 기계번역(영어 --> 프랑스어) 과정을 표현한다.

밝은 픽셀값을 가진 부분들은, 가령 'The'라는 영단어는 'L'이라는 프랑스어로 번역될 가중치가 가장 높게 나타났다는 의미이다.

중간에 'zone'이라는 프랑스어가 영어의 'Area'와 매칭되는 것으로 프랑스어어의 어순에 맞게 잘 번역하는 모습을 볼 수 있다.

![image](https://user-images.githubusercontent.com/39285147/186274146-b9fedbd5-7b6e-40a0-ba57-9f1599e82001.png)

또한, 'change'는 프랑스어로 'va'와 'changer' 두 개의 단어에 영향을 주는 모습이다.

이것은 가령 어떤 단어는 관사가 필요로 해서, 다수 단어로 기계번역되어야 하는 경우들이라고 생각하면 좋다.

## Image Captioning with Attention
![image](https://user-images.githubusercontent.com/39285147/186275873-843f6307-9463-44c9-aab6-9c36fb97b0fb.png)

어떤 이미지가 주어졌을 때, **이미지에 대한 설명을 caption**으로 제공하는 것을 목표로 한다.

![image](https://user-images.githubusercontent.com/39285147/186276848-50615f64-ba96-491f-8848-7dea5a9f986f.png)

상기 사진에서 **CNN을 활용해서 이미지 특성을 인풋**으로 가져온다.
- 이미지 특성의 spatial localization을 유지한다

이후 과정은 앞서 언급된 Attention 모델과 동일한 매커니즘을 따른다.

![image](https://user-images.githubusercontent.com/39285147/186277826-5554b458-4ec2-49ad-8a88-338e2bf7c159.png)

- **Soft**: Rough하게 포인트를 잡는다.
- **Hard**: 날카롭게 포인트를 잡는다.

![image](https://user-images.githubusercontent.com/39285147/186278089-53f57436-b083-4e52-a259-532a5220aee7.png)

이미지 영역에서 'bird'를 잘 나타내는 영역은 높은 가중치를 띄고있을 것이다.

### Visual-Question Answering
![image](https://user-images.githubusercontent.com/39285147/186278538-5cc4ab6b-4deb-4821-a81d-a0dacf81207f.png)

![image](https://user-images.githubusercontent.com/39285147/186278341-8640156d-70f2-45d2-ae1c-110e1b6dbf21.png)

인풋으로 질문 텍스트와 사진이 주어지면 AI가 그 텍스트 질문에 답을 하는 task이다.

이제 [**ConvS2S, Self-Attention**](https://hchoi256.github.io/bert/bert-3/)에 대해 알아보자.

# Reference
[Deep Learning for NLP - KAIST 주재걸 교수님](https://www.youtube.com/watch?v=JqkfT1s60cI&list=PLep-kTP3NkcOjOS1a30UNW-tH2FSoGYfg&index=1)

[딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22893)