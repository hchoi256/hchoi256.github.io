---
layout: single
title: "PART 1: 자연어와 Word Embedding"
categories: BERT
tag: [NLP, Word Embedding, Word2Vec, One Hot Encoding]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# 자연어 처리(Natural Language Processing, NLP)
## 자연어란?
![image](https://user-images.githubusercontent.com/39285147/183526289-0d7a43a8-f329-4ed1-9e0b-31259a32fe6f.png)

- 부호화(Encoding)
- 해독(Decoding)

자연어 처리는 상기 도표에서 컴퓨터가 **텍스트를 해독하는 과정**을 의미한다.

일상에서 사용하는 모든 인간의 언어로, 한국어, 영어와 같은 것들이 예시이다.

> 인공언어: 프로그래밍 언어, etc.

## 자연어 처리의 두 종류
1. **규칙 기반 접근법 (Symbolic approach)**

2. **확률 기반 접근법 (Statistical approach)**
- *TF-IDF*
  - TF(Term frequency): 단어가 문서에 등장한 개수 (TF ↑, 중요단어확률 ↑)
  - DF(Document frequency): 해당 단어가 등장한 문서의 개수 (DF ↑, 중요단어확률 ↓)

## 자연어 처리의 단계
① **전처리**

- 개행문자, 특수문자, 공백, 중복 표현, 이메일 및 링크, 불용어, 조사 제거
- 띄어쓰기, 문장분리 보정
- 어간추출

② **Tokenizing**

![image](https://user-images.githubusercontent.com/39285147/183527388-2369aaca-6791-42d0-821b-1e09460d713f.png)

- 어절 tokenizing
- 형태소 tokenizing
- n-gram tokenizing
- **WordPiece tokenizing** (BERT)

③ **Lexical analysis**

- 어휘, 형태소 분석
- 개체명 인식
- 상호 참조

④ **Syntactic analysis**

- 구문 분석

⑤ **Semantic analysis**

- 의미 분석

## NLP 활용 분야
- 의미 분석 (i.e., 최초의 컴퓨터는 무엇 --> '질문')
- 구문 분석 (i.e., 문법 구문 분석)
- 감성 분석
- 형태소 분석
- 개체명 인식 (i.e., 네이버는 어떤 회사 --> '기관')

****
# Word Embedding
'자연어'에서 특징을 추출하는 방법이다.

텍스트를 컴퓨터가 이해하고, 효율적으로 처리하게 하기 위해서는 컴퓨터가 이해할 수 있도록 텍스트를 적절히 숫자로 변환하여 좌표평면/벡터공간 위에 표현할 필요가 있다.

그러한 공간에서, 서로 다른 두 포인트/벡터 간의 거리를 유추하는 **유사도** 측정이 가능하다.

## 1. **One-hot encoding** (sparse representation)
![image](https://user-images.githubusercontent.com/39285147/183528088-8343c972-2c27-4f45-812b-3259e39e2151.png)

- 각 단어가 가지는 **의미 파악이 불가능하다**
- 단어 간 **유사도 비교가 불가능하다**.
- [**차원의 저주**](https://github.com/hchoi256/ai-terms)에 빠질 위험성 多

### 유사도
![image](https://user-images.githubusercontent.com/39285147/186249354-87d4c3d8-797c-494a-ad1f-befc687e48c4.png)
![image](https://user-images.githubusercontent.com/39285147/186250196-79d14e07-c6b9-4771-a8ca-99b7e21993f7.png)

여기서 유사성을 측정하는 여러 가지 척도가 존재한다: Euclidean, Cosin, 내적, etc.

상기 모형에서, 초록색은 Cosin 거리, 빨간색은 Euclidean 거리이다.

Cosin 거리는 각도가 좁아질 수록, 두 벡터간의 유사도가 높다는 지표이다.

원핫 인코딩은 좌표평면에 단어를 표현하기 때문에, 상기 모형처럼 각도가 90도로 모두 동일한 모습이다.

따라서, Cosin Simliarity를 구하면 서로 모두 동일한 유사도를 가지기 때문에 비교가 불가능하다.

이러한, 한계점을 타파한 것이 벡터공간에 단어의 의미를 담아 표현한 Word2Vec이다.

## 2. Word2Vec (dense representation)
![image](https://user-images.githubusercontent.com/39285147/183535690-05358c7b-a9ba-4893-8959-53e36b521513.png)

- 자연어의 **의미를 '벡터 공간'에 임베딩**하여, one-hot encoding의 한계점을 해결한다.
  - i.e., *한국 - 서울 + 도쿄 = 일본*
- 한 단어의 **주변 단어들을 통해** 비지도학습으로 그 단어의 의미 파악이 가능하다 (유사성 有).
- **한정된 자원**으로 표현이 가능하다.

One-hot 벡터들을 인풋으로 받고, 2개의 간단한 hidden layer를 가진 신경망으로 구성되있다.

주변부 단어를 예측하는 방식으로 학습하여 인풋 단어를 올바르게 유츄해낸다.

단어 벡터의 유사도과 semantic 혹은 syntactic analogy를 통하여 Word2Vec 성능을 검증한다.

하지만, 단어의 **subword information(i,e., 서울 vs 서울시)을 무시**하고 **학습에 사용될 충분한 vocabulary가 주어져야 한다**는 한계가 존재한다.

## 3. FastText
![image](https://user-images.githubusercontent.com/39285147/183534835-d42db067-905f-483d-a738-e437e4dc4e78.png)

subword information을 무시하는 Word2Vec 한계를 **n-gram 기법**을 통해 subword를 직접 학습하면서 극복한다 (i.e., assumption에 2-gram을 적용하면 as, ss, su,..).

FastText는 단어를 n-gram으로 분리한 후, 모든 n-gram vector를 합산한 평균을 통해 단어 벡터를 획득한다.

**오탈자, OOV (Out of Vocabulary), 그리고 빈도수가 적은 단어**에 대한 학습이 용이하다.

## Word embedding 한계점
**동형어, 다의어**(*account: 계좌, 차지하다, etc.*)와 같은 단어에 대한 embedding 성능이 좋지 않다.

~~주변 단어를 예측하는 방식으로 학습하기 때문에 **'문맥'을 고려할 수 없다**.~~

이러한 한계점을 타파하는 [**언어 모델(Language Model)**](https://hchoi256.github.io/bert/bert-2/)에 대해 다음 시간에 알아보자.