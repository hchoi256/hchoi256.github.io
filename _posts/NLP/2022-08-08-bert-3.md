---
layout: single
title: "PART 3: BERT Language Model"
categories: BERT
tag: [NLP, BERT, BPE, WordPiece, KorBERT]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

# BERT
Bi-directional transformer로 이루어진 언어모델로, 양방향으로 encoder를 발산하여 전체 단어를 학습에 활용한다 (전체 맥락을 이해한다).

BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행한다.

> BERT: Transformer 의 Encoder (self attention)
>
> GPT: Transformer 의 Decoder (Multi-head attention)

# BPE (Byte Pair Encoding)
![image](https://user-images.githubusercontent.com/39285147/183617390-94429c10-1868-4967-a534-f56199dfeba3.png)

빈도수에 기반하여 토큰을 병합하는 **BPE (Byte Pair Encoding)** 알고리즘으로, **OOV 문제 해결 가능하다.**

BPE는 언어 집합의 크기는 억제하면서 시퀀스의 길이를 **압축하는 알고리즘**이다.

아래 예제를 통하여 간단히 이해해보자.

        aaabdaaabac

가령, 상기와 같은 문자열에 BPE을 수행한다면, 가장 자주 등장하는 바이트(i.e., 'a') 쌍은 'aa'이다.

이것을 하나의 바이트인 'Z'로 치환한다.

        ZabdZabac
        Z=aa

다음으로 가장 많이 등장하는 바이트 쌍은 'ab'이고, 이것을 'Y'로 치환한다.

        ZYdZYac
        Y=ab
        Z=aa

다음은 'ZY'가 가장 많이 등장할 것이다.

        XdXac
        X=ZY
        Y=ab
        Z=aa

더 이상 병합할 바이트의 쌍은 없으므로 BPE는 위의 결과를 최종 결과로 하여 종료된다.

> 보다 자세한 BPE 시행 과정은 [여기](https://wikidocs.net/22592) 참조 요망.

# WordPiece tokenizing
BPE의 확장 버전으로, WordPiece는 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합한다 (= 두 문자가 같이 오는 문자 단위를 중요시한다).

> *우도*: 전체 글자 중 각 단어가 따로 등장한 것을 '분모'로, 같이 등장한 빈도수를 '분자'로 삼는다.

        수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake

        WordPiece Tokenizer를 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake

상기 결과에서 **띄어쓰기는 서브 워드(subwords)들을 구분하는 역할**, **언더바 _는 문장 복원을 위한 장치** (*교착어 방지 효과*).이다.

> WordPiece Tokenizer이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법은 현재 있는 모든 띄어쓰기를 전부 제거하고, 언더바를 띄어쓰기로 바꾸면 된다.

# ETRI KorBERT
한국어는 영어와 다르게 **조사의 쓰임에 따라 형태소가 변하는 문법적 특성**나 **교착어** 등 NLP 처리에 까다로운 tasks를 수반하는 언어 중 하나이다.

영어 BERT 모델과 다르게, 형태소 단위로 corpus를 분절하고(= *Mecab tokenzier*) Word2Vec으로 1차 tokenizing 이전에 형태소 태그를 단어마다 붙여놓는 **labeling 작업**이 수반된다.

이 작업은 텍스트가 더 쪼개질 수 있도록 도와준다
- 가령, '이'라는 단어가 어떤 것을 가르키는 이것의 의미인지 숫자 2의 의미인지 구분해준다.

**KorBERT Morphology**와 **KorBERT WordPiece** 두 가지 세부 모델이 존재한다.

## KorBERT 성능에 영향을 미치는 요인
- Corpus 사이즈
- Corpus 도메인
- Corpus tokenizing (어절, BPE, 형태소)
- Vocab 사이즈 小
- **데이터 전처리**

# KorBERT 실습 과정
0. 원하는 **corpus** 텍스트(가령, 위키 기사) 불러오기

1. ***SentenceTokenizer***/WordTokenizer 1차 분절

2. 'Mecab' tokenizer 기반 **형태소 단위 분절**
- 'Mecab'이 가장 성능이 좋다.

3. **교착어 방지 token sequences 형성** ('*[BertWordPieceTokenizer](https://wikidocs.net/99893)*')
- GPT 모델은 '*ByteLevelBPETokenizer*'를 사용한다/

4. 데이터 전처리 (i.e., 불용어 등 제거)

5. BERT 학습

6. **KorQuAD* 데이터셋으로 학습된 BERT 모델 평가
- *KorQuAD*: 자연어 이해(NLU, Natural Language Understanding) 학습용 한국어 질의응답 표준 데이터셋이다.

7. [*BERT 네이버 영화 리뷰 데이터 감성분석*](https://github.com/e9t/nsmc)

8. [*BERT 관계 추출*](https://github.com/machinereading/kor-re-gold)

9. **유튜브 댓글 데이터 감성 분석** (데이터 수집: '*썸트랜드*')

# References
[*딥 러닝을 이용한 자연어 처리 입문*](https://wikidocs.net/22592)

[*KorQuAD*](https://www.slideshare.net/qksksk657/korquad-v10)

[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805)