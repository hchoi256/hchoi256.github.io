---
layout: single
title: "ConvS2S, Self-Attention"
categories: NLP
tag: [NLP, ConvS2S, Self-Attention, Multi-head Attention]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

## Convolution Seq2Seq (ConvS2S)
![image](https://user-images.githubusercontent.com/39285147/187002178-642ad27b-b738-48c5-9eb0-1ddab54cc921.png)

### Convolutional block structure for encoder
![image](https://user-images.githubusercontent.com/39285147/186263776-3dfac63c-5ea1-4736-8e19-52dbbd40809a.png)

- *Residual Connection*: 입력 정보가 가중치 합(weight sum) 연산에 적용되고자 지름길 route를 만든다. 다음 레이어에 전이시켜 최종 출력에 영향을 줄 수 있도록 하기 위함.

> [ResNet](https://github.com/hchoi256/ai-terms/blob/main/README.md) 알아보기.

기존 RNN의 '직렬화'라는 한계점을 타파하고자 **RNN 구조를 제거**하고 몇 개의 단어 토큰으로 'Convolution'을 적용한 경우이다.

![image](https://user-images.githubusercontent.com/39285147/186263248-1acb5a60-ae13-4df1-a1ba-cbe0d47fe9cc.png)

- Convolution Filter 개수: 순환 신경망의 레이어 간 연결선을 제거하여 **이전 출력을 기다릴 필요없이**, 상기 사진에서는 가령 3개의 단어 토큰 단위로 새로운 단어 벡터를 인풋으로 활용한다.
- 빨간색 평행사변형은 'go', 'to', 'the school' 세 단어가 하나의 새로운 벡터로 변환되는 모습을 보여준다.

![image](https://user-images.githubusercontent.com/39285147/187803235-9c85f564-987d-489d-a40c-901600767c9c.png)

이러한 아키텍쳐를 구현함으로써 질적으로도 뛰어날 뿐 아니라, 계산의 **병렬화**를 가능하게 하여 **훈련 시간의 단축**이라는 쾌거를 이루어낼 수 있었다.
- 기존 직렬화 방식은 'I' --> 'go' --> 'to' --> 'the school' 식으로 학습하면서 최종적으로 이전 단어 맥락을 전부 고려하는 context vector를 만든다.

![image](https://user-images.githubusercontent.com/39285147/187803537-dee60d19-98e6-4caa-979f-f3a9be6d62e2.png)

상기 사진에서, ①은 Attention Weight를 구할 때 사용되는 부분으로, Softmax 이후 Attention Weights가 도출되는 모습이다.

또한, ②은 ①에서 얻은 Attention Weights에 각 단어 토큰에 대한 지름길 route를 통하여 가중합을 구할 때 사용된다.

****
# Self-attention 모델
![image](https://user-images.githubusercontent.com/39285147/185520452-e27a130d-510f-4d3a-a12d-adea5378a164.png)

RNN의 시계열 학습에서 벗어나 Attention 구조만으로 학습에 임한다.

Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 **input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식**으로 동작한다.

![image](https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png)

**인코더 내부에서 인코더 한 토큰을 재구성**하는 개념이다.
- **Query, Key, Value가 동일한 시퀀스**를 가진다.

![image](https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png)

- Score = Query와 Key 내적

최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.

이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.

![image](https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png)

****
# Multi-head Attention 모델
하나의 Attention 모델은 각 단어 토큰의 가중치만을 도출하기에, 다른 단어와 어떻게 연관되어있는지 잘 훈련이 안될수도 있다.

따라서 여러개의 어텐션, 즉 Multi-head Attention을 이용하여 이를 하나로 이어붙여서 연관성을 탐구한다.

여러 번의 Self-attention을 **병렬처리**로 수행하여 **빠른 속도**를 끌어낸 것이 바로 **multi-head attention**이다.


이제 [**Transformer**](https://hchoi256.github.io/bert/bert-4/)에 대해 알아보자.

# Reference
[Deep Learning for NLP - KAIST 주재걸 교수님](https://www.youtube.com/watch?v=JqkfT1s60cI&list=PLep-kTP3NkcOjOS1a30UNW-tH2FSoGYfg&index=1)