---
layout: single
title: "PART 3: Self-Attention, Transformer, BERT, BPE, WordPiece"
categories: BERT
tag: [NLP, Transformer, BERT, BPE, WordPiece]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# Self-attention 모델
![image](https://user-images.githubusercontent.com/39285147/185520452-e27a130d-510f-4d3a-a12d-adea5378a164.png)

RNN의 시계열 학습에서 벗어나 Attention 구조만으로 학습에 임한다.

Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 **input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식**으로 동작한다.

![image](https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png)

**인코더 내부에서 인코더 한 토큰을 재구성**하는 개념이다.
- **Query, Key, Value가 동일한 시퀀스**를 가진다.

![image](https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png)

- Score = Query와 Key 내적

최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.

이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.

![image](https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png)

> 여러 번의 Self-attention을 **병렬처리**로 수행하여 **빠른 속도**를 끌어낸 것이 바로 **multi-head attention**이다.


****
# Transformer 모델
Multi-head attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행하며, 이것이 바로 BERT에 탑재된 기술이다.

seq2seq의 구조인 '**인코더-디코더**'를 따르면서도, 어텐션(Attention)만으로 구현한 모델이기 때문에 **RNN을 사용하지 않는다**.

또한, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여준다.

****
# BERT
Bi-directional transformer로 이루어진 언어모델로, 양방향으로 encoder를 발산하여 전체 단어를 학습에 활용한다 (전체 맥락을 이해한다).

BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행한다.

> BERT: Transformer 의 Encoder (self attention)
>
> GPT: Transformer 의 Decoder (Multi-head attention)

****
# BPE (Byte Pair Encoding)
![image](https://user-images.githubusercontent.com/39285147/183617390-94429c10-1868-4967-a534-f56199dfeba3.png)

빈도수에 기반하여 토큰을 병합하는 **BPE (Byte Pair Encoding)** 알고리즘으로, **OOV 문제 해결 가능하다.**

BPE는 언어 집합의 크기는 억제하면서 시퀀스의 길이를 **압축하는 알고리즘**이다.

아래 예제를 통하여 간단히 이해해보자.

        aaabdaaabac

가령, 상기와 같은 문자열에 BPE을 수행한다면, 가장 자주 등장하는 바이트(i.e., 'a') 쌍은 'aa'이다.

이것을 하나의 바이트인 'Z'로 치환한다.

        ZabdZabac
        Z=aa

다음으로 가장 많이 등장하는 바이트 쌍은 'ab'이고, 이것을 'Y'로 치환한다.

        ZYdZYac
        Y=ab
        Z=aa

다음은 'ZY'가 가장 많이 등장할 것이다.

        XdXac
        X=ZY
        Y=ab
        Z=aa

더 이상 병합할 바이트의 쌍은 없으므로 BPE는 위의 결과를 최종 결과로 하여 종료된다.

> 보다 자세한 BPE 시행 과정은 [여기](https://wikidocs.net/22592) 참조 요망.

****
# WordPiece Tokenizing
BPE의 확장 버전으로, WordPiece는 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합한다 (= 두 문자가 같이 오는 문자 단위를 중요시한다).

> *우도*: 전체 글자 중 각 단어가 따로 등장한 것을 '분모'로, 같이 등장한 빈도수를 '분자'로 삼는다.

        수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake

        WordPiece Tokenizer를 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake

상기 결과에서 **띄어쓰기는 서브 워드(subwords)들을 구분하는 역할**, **언더바 _는 문장 복원을 위한 장치** (*교착어 방지 효과*).이다.

> WordPiece Tokenizer이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 방법은 현재 있는 모든 띄어쓰기를 전부 제거하고, 언더바를 띄어쓰기로 바꾸면 된다.

이제, [KoBERT 기반 감성분석 및 키워드 추출 실습]()을 수행해보자.