---
layout: single
title: "Transformer"
categories: NLP
tag: [NLP, Transformer]
toc: true
toc_sticky: true
toc_label: "쭌스log"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

****
# Transformer 모델
Multi-head attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행하며, 이것이 바로 BERT에 탑재된 기술이다.

seq2seq의 구조인 '**인코더-디코더**'를 따르면서도, 어텐션(Attention)만으로 구현한 모델이기 때문에 **RNN을 사용하지 않는다**.

또한, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여준다.

이제, [BERT와 KoBERT 실습](https://hchoi256.github.io/bert/bert-5/)을 수행해보자.

# Reference
[Deep Learning for NLP - KAIST 주재걸 교수님](https://www.youtube.com/watch?v=JqkfT1s60cI&list=PLep-kTP3NkcOjOS1a30UNW-tH2FSoGYfg&index=1)