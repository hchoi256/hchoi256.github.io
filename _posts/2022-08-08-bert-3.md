---
layout: single
title: "BERT Language Model - Part 3"
categories: NLP
tag: [NLP, python, BERT, Language Model]
toc: true
toc_sticky: true
toc_label: "GITHUB BLOG JJUNS"
#author_profile: false
header:
    teaser: /assets/images/posts/bert-thumbnail.png
sidebar:
    nav: "docs"
---

# BERT
Bi-directional transformer로 이루어진 언어모델로, BERT 언어모델 위에 1개의 classification layer만 부착하여 다양한 NLP task를 수행한다.

# WordPiece tokenizing
입력 문장을 toeknizing하고, 그 token들로 'token sequence'를 만들어 학습에 사용한다.

## BPE (Byte Pair Encoding)
![image](https://user-images.githubusercontent.com/39285147/183617390-94429c10-1868-4967-a534-f56199dfeba3.png)

이러한 작업을 위해 **BPE (Byte Pair Encoding)** 알고리즘을 이용한다.

Word2Vec으로 어절 단위로 1차 tokenizing을 진핸한 후, iteration을 통해 나타난 token sequence 빈도수에 기반하여 **의미 있는 패턴(subword)**으로 잘라서 tokenizing한다.

가령, BERT 모델은 '고양경찰서'와 '종로경찰서'에서 '##경찰서'라는 token sequence를 형성한다.

# ETRI KorBERT
기존 영어 BERT 모델과 다르게, Word2Vec으로 1차 tokenizing 이전에 형태소 태그를 단어마다 붙여놓는 **labeling 작업**이 수반된다.

이 작업은 텍스트가 더 쪼개질 수 있도록 도와준다
- 가령, '이'라는 단어가 어떤 것을 가르키는 이것의 의미인지 숫자 2의 의미인지 구분해준다.

**KorBERT Morphology**와 **KorBERT WordPiece** 두 가지 세부 모델이 존재한다.

## KorBERT 성능에 영향을 미치는 요인
- Corpus 사이즈
- Corpus 도메인
- Corpus tokenizing (어절, BPE, 형태소)
- Vocab 사이즈 小
- **데이터 전처리**

# KorBERT 실습 과정
1. BERT 학습을 위한 vocab 형성 ('WordPiece')
- 원하는 corpus 텍스트(가령, 위키 기사)에 대하여 vocab을 형성한다.

2. 데이터 전처리

3. BERT 학습

4. 학습된 BERT 모델로 **KorQuAD* 학습

> KorQuAD: 자연어 이해(NLU, Natural Language Understanding) 학습용 한국어 질의응답 표준 데이터셋이다.

5. 학습된 KorQuAD 평가

6. [*BERT 네이버 영화 리뷰 데이터 감성분석*](https://github.com/e9t/nsmc)

7. [*BERT 관계 추출*](https://github.com/machinereading/kor-re-gold)


# References
[*KorQuAD*](https://www.slideshare.net/qksksk657/korquad-v10)

[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805)