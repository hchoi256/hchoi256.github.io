I"ğ^<h1 id="code">Code</h1>
<p class="notice--danger"><strong>[Notice]</strong> <a href="https://github.com/hchoi256/machine-learning-development">download here</a></p>

<h1 id="learning-goals">Learning Goals</h1>
<p>Artificial Neural Network (ANN)ì„ ì´ìš©í•œ íšŒê·€ ì‘ì—… ì²˜ë¦¬ë¥¼ ì´í•´í•œë‹¤. <span style="color: blue">Understanding of how ANN solves regression tasks.</span></p>

<p>ìˆœë°©í–¥/ì—­ì „íŒŒë¥¼ ë™ë°˜í•˜ëŠ” ê°€ì¤‘ì¹˜ í•™ìŠµì˜ ê³¼ì •ì— ëŒ€í•´ ë³´ë‹¤ ë‚˜ì€ ì´í•´ë¥¼ ë„ëª¨í•œë‹¤. <span style="color: blue">Better understanding of deep learning through forward/backward propagation</span></p>

<h1 id="description">Description</h1>
<p>ì—¬ëŸ¬ë¶„ì´ ìë™ì°¨ ë”œëŸ¬ í˜¹ì€ ì°¨ëŸ‰ íŒë§¤ì›ì´ë¼ ê°€ì •í•´ë³´ì. <span style="color: blue">Now, you are a car seller.</span></p>

<p>ìƒê¸° ê³ ê°ë“¤ì˜ íŠ¹ì • ë°ì´í„°(ë‚˜ì´, ì—°ë´‰, etc.)ë¥¼ ì°¸ê³ í•˜ì—¬ ê³ ê°ë“¤ì´ ì°¨ëŸ‰ êµ¬ë§¤ì— ì‚¬ìš©í•  ê¸ˆì•¡ì„ ì˜ˆì¸¡í•˜ì—¬ íŠ¹ì • ì§‘ë‹¨ì— ëŒ€í•œ íƒ€ê¹ƒ ë§ˆì¼€íŒ…ì„ ì´ë£¨ê³ ì í•œë‹¤. <span style="color: blue">Your job is to analyze the customer dataset, then predict how much money a new customer would like to spend.</span></p>

<h2 id="dataset">Dataset</h2>
<table border="0" cellpadding="0" cellspacing="0" id="sheet0" class="sheet0 gridlines">
    <col class="col0" />
    <col class="col1" />
    <col class="col2" />
    <col class="col3" />
    <col class="col4" />
    <col class="col5" />
    <col class="col6" />
    <col class="col7" />
    <col class="col8" />
    <tbody>
        <tr class="row0">
        <td class="column0 style0 s">Customer Name</td>
        <td class="column1 style0 s">Customer e-mail</td>
        <td class="column2 style0 s">Country</td>
        <td class="column3 style0 s">Gender</td>
        <td class="column4 style0 s">Age</td>
        <td class="column5 style0 s">Annual Salary</td>
        <td class="column6 style0 s">Credit Card Debt</td>
        <td class="column7 style0 s">Net Worth</td>
        <td class="column8 style0 s">Car Purchase Amount</td>
        </tr>
        <tr class="row1">
        <td class="column0 style0 s">Martina Avila</td>
        <td class="column1 style0 s">cubilia.Curae.Phasellus@quisaccumsanconvallis.edu</td>
        <td class="column2 style0 s">Bulgaria</td>
        <td class="column3 style0 n">0</td>
        <td class="column4 style0 n">41.8517198</td>
        <td class="column5 style0 n">62812.09301</td>
        <td class="column6 style0 n">11609.38091</td>
        <td class="column7 style0 n">238961.2505</td>
        <td class="column8 style0 n">35321.45877</td>
        </tr>
        <tr class="row2">
        <td class="column0 style0 s">Harlan Barnes</td>
        <td class="column1 style0 s">eu.dolor@diam.co.uk</td>
        <td class="column2 style0 s">Belize</td>
        <td class="column3 style0 n">0</td>
        <td class="column4 style0 n">40.87062335</td>
        <td class="column5 style0 n">66646.89292</td>
        <td class="column6 style0 n">9572.957136</td>
        <td class="column7 style0 n">530973.9078</td>
        <td class="column8 style0 n">45115.52566</td>
        </tr>
        <tr class="row3">
        <td class="column0 style0 s">Naomi Rodriquez</td>
        <td class="column1 style0 s">vulputate.mauris.sagittis@ametconsectetueradipiscing.co.uk</td>
        <td class="column2 style0 s">Algeria</td>
        <td class="column3 style0 n">1</td>
        <td class="column4 style0 n">43.15289747</td>
        <td class="column5 style0 n">53798.55112</td>
        <td class="column6 style0 n">11160.35506</td>
        <td class="column7 style0 n">638467.1773</td>
        <td class="column8 style0 n">42925.70921</td>
        </tr>
        <tr class="row4">
        <td class="column0 style0 s">Jade Cunningham</td>
        <td class="column1 style0 s">malesuada@dignissim.com</td>
        <td class="column2 style0 s">Cook Islands</td>
        <td class="column3 style0 n">1</td>
        <td class="column4 style0 n">58.27136945</td>
        <td class="column5 style0 n">79370.03798</td>
        <td class="column6 style0 n">14426.16485</td>
        <td class="column7 style0 n">548599.0524</td>
        <td class="column8 style0 n">67422.36313</td>
        </tr>
        <tr class="row5">
        <td class="column0 style0 s">Cedric Leach</td>
        <td class="column1 style0 s">felis.ullamcorper.viverra@egetmollislectus.net</td>
        <td class="column2 style0 s">Brazil</td>
        <td class="column3 style0 n">1</td>
        <td class="column4 style0 n">57.31374945</td>
        <td class="column5 style0 n">59729.1513</td>
        <td class="column6 style0 n">5358.712177</td>
        <td class="column7 style0 n">560304.0671</td>
        <td class="column8 style0 n">55915.46248</td>
        </tr>
    </tbody>
</table>

<p><strong>ë…ë¦½ë³€ìˆ˜(Independent Variables)</strong></p>
<ul>
  <li>Customer Name</li>
  <li>Customer e-mail</li>
  <li>Country</li>
  <li>Gender</li>
  <li>Age</li>
  <li>Annual Salary</li>
  <li>Credit Card Debt.</li>
  <li>Net Worth</li>
</ul>

<p><strong>ì¢…ì†ë³€ìˆ˜(Dependent Variables)</strong></p>
<ul>
  <li>Car Purchase Amount</li>
</ul>

<h1 id="loading-the-dataset">Loading the dataset</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span> <span class="c1"># data frame
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># numbers
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span> <span class="c1"># data visualization
</span>
<span class="n">car_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'Car_Purchasing_Data.csv'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'ISO-8859-1'</span><span class="p">)</span> <span class="c1"># ë°ì´í„°ì…‹ì´ '@'ì™€ ê°™ì€ íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— ìƒê¸° ì¸ì½”ë”© ì„¤ì •ì„ í•´ì¤˜ì•¼í•œë‹¤. Encoding process (i.e., remove '@')
</span></code></pre></div></div>

<h1 id="data-visualization">Data Visualization</h1>

<h2 id="seaborn">Seaborn</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">car_df</span><span class="p">)</span> <span class="c1"># ì”¨ë³¸ ë•ë¶„ì— ë¶„ì„ ì‘ì—…ì„ ì—¬ëŸ¬ ë²ˆ í•  í•„ìš”ì—†ì´ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì‹œê°í™”ë¥¼ ë³´ì—¬ì¤€ë‹¤ Easily produce data visualization using seaborn
</span></code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/180380848-d3772ba0-a21b-416b-8139-534c7a3aa721.JPG" alt="screensht" /></p>

<p>ë°ì´í„° ë¶„í¬ì—ì„œ ë§¨ ì•„ë˜ ìœ„ì¹˜í•œ í–‰ì€ â€˜Car Purchase Amountâ€™ì´ê³ , ê° ì—´ì€ ìˆœì„œëŒ€ë¡œ Gender, Age, Annual Salary, Credit Car Debt, Net Worth, Car Purchase Amountì´ë‹¤.<span style="color: blue">The bottom row in the data distribution is â€˜Car Purchase Amountâ€™, and each column is Gender, Age, Annual Salary, Credit Car Debt, Net Worth, and Car Purchase Amount in order.</span></p>

<p>ë”°ë¼ì„œ, ë‚˜ì´ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì°¨ëŸ‰ êµ¬ë§¤ ì˜ˆìƒ ê¸ˆì•¡ì´ ì¦ê°€í•˜ëŠ” ì„ í˜•ì  í˜•íƒœì˜ ë°ì´í„° ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ê³ , ë°˜ëŒ€ë¡œ Credit Card Debtì€ ì¢…ì†ë³€ìˆ˜ì™€ ëšœë ·í•œ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ê´€ì°°ëœë‹¤.<span style="color: blue">Therefore, it is observed that the data distribution in a linear form shows that the expected amount of vehicle purchase increases with age, and on the contrary, Credit Card Debt does not show a clear correlation with the dependent variable.</span></p>

<h1 id="data-preprocessing">Data Preprocessing</h1>
<h1 id="remove-unnecessary-variables">Remove Unnecessary Variables</h1>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">car_df</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'Customer Name'</span><span class="p">,</span> <span class="s">'Customer e-mail'</span><span class="p">,</span> <span class="s">'Country'</span><span class="p">,</span> <span class="s">'Car Purchase Amount'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠëŠ” ë¶ˆí•„ìš”í•œ ì…ë ¥í”¼ì²˜ë¥¼ ì œê±°í•œë‹¤. remove unncessary columns not affecting 'y'
</span><span class="n">y</span> <span class="o">=</span> <span class="n">car_df</span><span class="p">[</span><span class="s">'Car Purchase Amount'</span><span class="p">]</span> <span class="c1"># Dependent Variable
</span><span class="n">X</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/180381916-2051d577-51ec-4ff8-be80-0685754f456b.png" alt="image" /></p>

<h2 id="data-scaling">Data Scaling</h2>
<p>ë‚˜ì´ì™€ ì—°ë´‰ê³¼ ê°™ì€ ì…ë ¥í”¼ì²˜ì˜ ìˆ˜ì¹˜ê°€ ì°¨ì´ê°€ ì»¤ì„œ, íŠ¹ì • í”¼ì²˜ì— ê³¼ì¤‘í™”ëœ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ [0, 1] ê°’ìœ¼ë¡œ ì •ê·œí™”í•˜ëŠ” ìŠ¤ì¼€ì¼ë§(Scailing)ì„ ì ìš©í•´ì•¼ í•œë‹¤. <span style="color: blue">Since there is a gap between age and salary (maybe overfitting), we should apply scaling that normalizes data into the range [0, 1]</span></p>

<p>ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ, ìš°ë¦¬ëŠ” <strong>MinMaxScaler</strong>ë¥¼ ì‚¬ìš©í•œë‹¤. <span style="color: blue">We are using <strong>MinMaxScaler</strong></span></p>

<p>ê¸°ì¡´ StandardScalerì™€ MinMaxScalerì˜ ì°¨ì´ì ì€ ë°ì´í„°ê°€ <strong>ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í˜¹ì€ ë”°ë¼ì•¼ í•˜ëŠ”ì§€</strong>ì— ë‹¬ë ¤ìˆë‹¤. <span style="color: blue">The main difference between MinMaxScaler and StandardScaler relies on normal distribution.</span></p>

<p><a href="https://velog.io/@ljs7463/%ED%94%BC%EC%B2%98-%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81StandardScalerMinMaxScaler">reference</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

</code></pre></div></div>

<h1 id="model-training">Model Training</h1>

<p><strong>Dense</strong></p>
<ul>
  <li><em>ì²«ë²ˆì§¸ ì¸ì</em> : ì¶œë ¥ ë‰´ëŸ°ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. <span style="color: blue"># output neurons</span></li>
  <li><em>input_dim</em> : ì…ë ¥ ë‰´ëŸ°ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.<span style="color: blue"> # input neurons</span></li>
  <li><em>init</em> : ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë°©ë²• ì„¤ì •í•©ë‹ˆë‹¤. <span style="color: blue">method to init weights</span>
    <ul>
      <li>â€˜uniformâ€™ : ê· ì¼ ë¶„í¬</li>
      <li>â€˜normalâ€™ : ê°€ìš°ì‹œì•ˆ ë¶„í¬</li>
    </ul>
  </li>
  <li><em>activation</em> : í™œì„±í™” í•¨ìˆ˜ ì„¤ì •í•©ë‹ˆë‹¤.
    <ul>
      <li>â€˜linearâ€™ : ë””í´íŠ¸ ê°’, ì…ë ¥ë‰´ëŸ°ê³¼ ê°€ì¤‘ì¹˜ë¡œ ê³„ì‚°ëœ ê²°ê³¼ê°’ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜µë‹ˆë‹¤.<span style="color: blue"> default setting</span></li>
      <li>â€˜reluâ€™ : rectifier í•¨ìˆ˜, ì€ë‹‰ì¸µì— ì£¼ë¡œ ì“°ì…ë‹ˆë‹¤. <span style="color: blue">used in hidden layers</span></li>
      <li>â€˜sigmoidâ€™ : ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì— ì£¼ë¡œ ì“°ì…ë‹ˆë‹¤. <span style="color: blue">used to solve binary classification tasks</span></li>
      <li>â€˜softmaxâ€™ : ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì— ì£¼ë¡œ ì“°ì…ë‹ˆë‹¤. <span style="color: blue">used in output layers to solve multinomial classification tasks</span></li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y_scaled</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span> <span class="c1"># Create training and test set
</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span> <span class="c1"># ì‹ ê²½ë§ì„ ìˆœì°¨ì  í˜•íƒœë¡œ ì„¤ê³„ sequential network
</span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span> <span class="c1"># ë‰´ëŸ°ì˜ ì…ì¶œë ¥ì„ ì—°ê²°í•´ì£¼ëŠ” ì™„ì „ ì—°ê²° ì‹ ê²½ë§ ìƒì„± fully connected layers
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span> <span class="c1"># ìˆœì°¨ì  ë§ì´ê¸° ë•Œë¬¸ì— 'input_dim'ì€ ë‹¤ì‹œ ì“°ì§€ ì•Šì•„ë„ ëœë‹¤. since it is a sequential network, we don't need to set 'input_dim' here again
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span> <span class="c1"># output
</span><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/180387209-a42f1385-dacc-45a3-a844-0ea2c3384262.png" alt="image" /></p>

<p><strong>ì…ë ¥ê°’ ê°œìˆ˜(# input)</strong>: 5 (age, etc.)</p>

<p><strong>ë‰´ëŸ° ê°œìˆ˜(# neurons</strong>: 25</p>

<blockquote>
  <p>ë‰´ëŸ° ê°œìˆ˜ê°€ ì ì„ìˆ˜ë¡ ì†ì‹¤ì´ í¬ê²Œ ë°œìƒí•œë‹¤ (ì—í¬í¬ ìˆ˜ë¥¼ ëŠ˜ë¦¼ìœ¼ë¡œì¨ ë³´ì™„ ê°€ëŠ¥í•˜ë‹¤). <span style="color: blue">As # neurons decreases, loss increases (can be alleviated by increasing # epoches)</span></p>
</blockquote>

<p><strong>bias</strong>: ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ì— ë§ê²Œ í• ë‹¹ëœë‹¤ (i.e., ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ 25ê°œ â€“&gt; bias ì—­ì‹œ 25ê°œê°€ ì¡´ì¬í•œë‹¤). <span style="color: blue">has the same number of neurons of the previous hidden layer</span></p>

<p><strong>í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(learnable parameters)</strong>: ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì˜ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ í”¼ë¼ë¯¸í„° ì—…ë°ì´íŠ¸ì˜ ëŒ€ìƒì´ ë˜ëŠ” ê°€ì¤‘ì¹˜ì™€ biasë¥¼ ë§í•œë‹¤. <span style="color: blue">i.e., weights and bias</span></p>

<ol>
  <li>
    <p>ì´ˆê¸° ì…ë ¥ê°’ì—ì„œ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µê¹Œì§€(<span style="color: blue">from initial input to first hidden layer</span>), <em>*# learnable parameters</em> = 5(# input) * 25(# neurons of first hidden layer) + 25(bias) = 150</p>
  </li>
  <li>
    <p>ì´ˆê¸° ì…ë ¥ê°’ì—ì„œ ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µê¹Œì§€(<span style="color: blue">from initial input to second hidden layer</span>), <em>*# learnable parameters</em> =  25(# neurons of first hidden layer) * 25(# neurons of second hidden layer) + 25(bias) = 650</p>
  </li>
  <li>
    <p>Ouput, <em>*# learnable parameters</em> = 25(neurons of second hidden layer) * 1(output) + 1(bias) = 150</p>
  </li>
</ol>

<blockquote>
  <p>Tot. params: 826</p>
  <blockquote>
    <p>ì…ì¶œë ¥ ê°’ì— ëŒ€í•œ ìµœì„ ì˜ ìƒê´€ê´€ê³„ ë„ì¶œì„ ìœ„í•´ í›ˆë ¨ë˜ê±°ë‚˜ ì¡°ì •ë˜ëŠ” í”¼ë¼ë¯¸í„° ì´ ê°œìˆ˜ì´ë‹¤. <span style="color: blue"># parameters that have been adjusted for drawing better correlation between input and output</span></p>
  </blockquote>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">)</span> <span class="c1"># present how to tarin model
</span>
</code></pre></div></div>
<p><img src="https://user-images.githubusercontent.com/39285147/180389686-0fd6c3e2-8ee8-4e0f-999c-7686f8f89d41.png" alt="image" /></p>

<blockquote>
  <p>Optimizer</p>
  <blockquote>
    <p>ëª¨ë¸ì´ í•™ìŠµê³¼ì •ì—ì„œ ì–´ë–»ê²Œ ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ì´ë¤„ë‚´ëŠ”ì§€ì— ëŒ€í•œ ë°©ë²•ì„ ì œì‹œí•œë‹¤. <span style="color: blue">presenting how to achieve weight optimization in the process of training model</span></p>

    <p><a href="https://github.com/hchoi256/lg-ai-auto-driving-radar-sensor/blob/main/supervised-learning/gradient-discent.md">adam</a></p>
  </blockquote>
</blockquote>

<blockquote>
  <p>loss (ì†ì‹¤í•¨ìˆ˜)</p>
  <blockquote>
    <p>ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ íŒë‹¨í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤. <span style="color: blue">method to determine modelâ€™s accuracy</span></p>

    <p>mean_squared_error (í‰ê· ì œê³±ì˜¤ì°¨)</p>
    <blockquote>
      <blockquote>

        <p>ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë„ë¡œ, ê·¸ ê°’ì´ ì‘ì„ìˆ˜ë¡ ì‹¤ì œê°’ê³¼ ìœ ì‚¬í•˜ì—¬ ì •í™•í•œ ì˜ˆì¸¡ì„ í•´ëƒˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. <span style="color: blue">difference between estimate and actual value (if low value, then better precdiction)</span></p>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs_hist</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>  <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/180390662-b659a0d9-6f49-46cd-bb01-acd9cf2bd4b5.png" alt="image" /></p>

<p>ëª¨ë¸ì´ í•™ìŠµí•˜ë©´ì„œ epochë¥¼ ê±°ë“­í•¨ì— ë”°ë¼ loss(ì—¬ê¸°ì„œëŠ” í‰ê· ì œê³±ì˜¤ì°¨ ë°©ë²•ì„ ì‚¬ìš©)ì˜ ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. <span style="color: blue">Loss is getting smaller as epoches go by.</span></p>

<ul>
  <li>epoch: ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ì˜ í•˜ë‚˜ì˜ í•™ìŠµì„ ëª‡ë²ˆ ì‹œí–‰í• ì§€ ê²°ì •í•œë‹¤. ê·¸ í¬ê¸°ê°€ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ìµœëŒ€ ì„ê³„ì¹˜ì— ê°€ê¹Œì›Œ ì§ˆìˆ˜ë¡ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•´ë‚¼ ìˆ˜ ìˆë‹¤. <span style="color: blue">Train the batch size of data in one epoch</span></li>
  <li>batch_size: í•œ ë²ˆì— í•™ìŠµí•  í›ˆë ¨ ë°ì´í„° ê°œìˆ˜ <span style="color: blue"># training data at one epoch</span></li>
  <li>verbose: ë””í´íŠ¸ 0. 1ë¡œ ì§€ì •í•˜ë©´ Epochì˜ ìƒí™©ê³¼, lossì˜ ê°’ì´ outputì— ë³´ì—¬ì¤€ë‹¤. <span style="color: blue">1: show epoch and loss in the output</span></li>
  <li><a href="https://github.com/hchoi256/ai-terms/blob/main/README.md">validation_split</a></li>
</ul>

<h1 id="model-evaluation">Model Evaluation</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_hist</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_hist</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model Loss Progression During Training/Validation'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Training and Validation Losses'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch Number'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Training Loss'</span><span class="p">,</span> <span class="s">'Validation Loss'</span><span class="p">])</span>

</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/180432355-41591eba-81ac-4625-a7a9-52b8dcba0c7a.png" alt="image" /></p>

<p>ìœ„ ê·¸ë˜í”„ì—ì„œ ì†ì‹¤í•¨ìˆ˜ì˜ ë¶„í¬ì˜ ì—í¬í¬ê°€ [0, 4] ì‚¬ì´ì˜ ì–´ëŠ ì„ê³„ì¹˜ì—ì„œë¶€í„° í¬ê²Œ ì¤„ì–´ë“¤ì§€ ì•ŠëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. <span style="color: blue">In the graph above, you can see epoch at around 3 has stopped plummeting.</span></p>

<p>ì´ë¥¼ í†µí•˜ì—¬ ìš°ë¦¬ëŠ” ì ë‹¹í•œ ì—í¬í¬ ê°œìˆ˜ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. <span style="color: blue">This gives us information about the best number of epoch to train the model. </span></p>

<h1 id="model-prediction">Model Prediction</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_predict</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">600000</span><span class="p">]]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Expected Purchase Amount='</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Expected Purchase Amount= [35656.47]
</code></pre></div></div>
:ET