I"<h1 id="언어-모델-language-model-lm">언어 모델 (Language Model, LM)</h1>
<p>‘자연어’의 법칙을 컴퓨터로 모사하는 모델로, 다음에 등장할 단어 예측을 수행한다(= ‘맥락’을 고려한다).</p>

<h2 id="markov-확률-모델">Markov 확률 모델</h2>
<p><img src="https://user-images.githubusercontent.com/39285147/183536291-32897298-797e-4fd8-aac9-4bcc9ef3459e.png" alt="image" /></p>

<p>이전 단어의 형태를 통하여 확률적으로 다음에 나올 단어를 예측한다</p>

<p>가령, ‘like’ 다음에 ‘rabbit’이라는 단어가 나타날 확률은 주어진 학습 데이터에 기반하여 <strong>33%</strong>로 나타난다.</p>

<h2 id="rnn-recurrent-neural-network-모델">RNN (Recurrent Neural Network) 모델</h2>
<p><img src="https://user-images.githubusercontent.com/39285147/183536890-b8d596a2-c3c0-4c90-8193-ac96f8b8cdb0.png" alt="image" /></p>

<p>Markov 체인 모델을 <strong>딥러닝</strong>에 접목하여 확장한 모델이 바로 RNN이다.</p>

<p>현재 state를 결정하기 위하여 이전 state의 hidden layer 정보를 인풋으로 받는다.</p>

<p>이를 통해 <strong>앞선 문맥을 고려한</strong> 최종 출력 vector(Context vector)를 만든다.</p>

<p>보다 자세한 내용은 <a href="https://github.com/hchoi256/ai-terms/blob/main/README.md">여기</a>를 참조하자.</p>

<h3 id="seq2seq">Seq2Seq</h3>
<p><img src="https://user-images.githubusercontent.com/39285147/183537292-5cfe7c3f-d380-4e0c-aa20-266341ae5d9a.png" alt="image" /></p>

<p><strong>Encoder layer</strong>: Context vector 획득</p>

<p><strong>Decoder layer</strong>: Context vecotr 해독/해석</p>

<p><strong>번역</strong>에 자주 사용되어, 자연어를 인식(encoding)하고 알맞은 언어로 번역(decoding)하여 출력한다.</p>

<p>하지만 sequence가 길어진다면, <strong>초기 단어에 대한 정보를 점차 소실</strong>되는 구조적 한계점과 <strong>중요하지 않은 token의 정보</strong> 또한 최종 출력에 영향을 주는 단점이 존재한다.</p>

<h3 id="attention-모델">Attention 모델</h3>
<p><img src="https://user-images.githubusercontent.com/39285147/183538147-9eb2a2cf-b06c-4994-9a3b-11a4013a6fc8.png" alt="image" /></p>

<p>모든 token을 고려하는 Seq2Seq의 한계점을 보완한다.</p>

<p>이를 위해서 <strong>특정 중요 단어에 대한 집중도</strong>를 부여한다.</p>

<p><strong>RNN의 셀 각각이 도출하는 output을 모두 활용</strong>하여 denamic하게 context vector를 생성한다.</p>
<ul>
  <li>FCL(Fully connected layer)에서 RNN 셀들과 앞선 단어 문맥을 고려한 출력값에 대한 <strong>score를</strong> 계산한다</li>
  <li>softmax 활성화 함수를 사용해서 <strong>attention weight</strong>를 도출한다</li>
  <li>각 셀에 대한 결과값과 attention weight을 곱한 합계로 context vector를 구한다.</li>
  <li>Context vector를 decoder에 넣는다.</li>
  <li>최종 output의 분류 결과가 안 좋다면, FCL에서 score를 조정하여 <strong>decoder가 해석하기에 용이한 attention weight를 재계산한다</strong>.
    <ul>
      <li>Input과 output 사이의 연관성을 가늠해볼 수 있다.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>흔히, 딥러닝 모델이 내놓은 결과의 원인을 설명할 수 없는(<a href="https://github.com/hchoi256/ai-terms/blob/main/README.md">Blackbox</a>) 경우가 많음에도 불구하고, Attention 모델은 attention weight 덕분에 해당 설명이 가능하다.</p>
</blockquote>

<p>하지만, RNN 신경망 구조를 기반으로 하기 때문에 이전 state의 결과를 기다려야 한다는 점에서 <strong>연산 속도가 느리다</strong>는 단점이 존재한다.</p>

<h2 id="self-attention-모델">Self-attention 모델</h2>
<p>RNN의 시계열 학습에서 벗어나 attention 구조만으로 학습에 임한다.</p>

<p>Decoder 해석에 용이한 방향으로 가중치 업데이트를 하는 것이 아닌 <strong>input 그 자체를 가장 잘 표현하기 위한 방향으로 학습하는 방식</strong>으로 동작한다.</p>

<p><img src="https://user-images.githubusercontent.com/39285147/183540623-c662b029-b65d-493c-8501-6edbcf8139c8.png" alt="image" /></p>

<ul>
  <li><strong>Query</strong>: 단어에 대한 가중치</li>
  <li><strong>Key</strong>: 단어가 Query와의 연관성을 나타내는 가중치</li>
  <li><strong>Value</strong>: 의미에 대한 가중치 (집중할 단어)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/39285147/183540697-a5e884be-56b5-4c34-9b87-95f8f4eacf7f.png" alt="image" /></p>

<p>최종적으로, softmax와 value를 곱한 값의 합계를 도출하여 해당 단어가 가진 전체적인 의미를 설명한다.</p>

<p>이러한 과정을 단어마다 각각 수행한 것이 바로 self-attention 모델로, 하기 도표를 통해 직관적으로 이해해보자.</p>

<p><img src="https://user-images.githubusercontent.com/39285147/183540713-da495ca0-9f6e-4584-a701-b6c402576c87.png" alt="image" /></p>

<blockquote>
  <p>RNN보다 더 복잡한 상기 과정을 <strong>병렬처리</strong>로 수행하여 <strong>빠른 속도</strong>를 끌어낸 것이 바로 <strong>multi-head attention</strong>이다.</p>
</blockquote>

<h2 id="transformer-모델">Transformer 모델</h2>
<p>Multi-head attention으로 이루어진 encoder를 여러 층 쌓아서 encoding을 수행하며, 이것이 바로 BERT에 탑재된 기술이다.</p>

<p>seq2seq의 구조인 ‘인코더-디코더’를 따르면서도, 어텐션(Attention)만으로 구현한 모델이기 때문에 RNN을 사용하지 않는다.</p>

<p>또한, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 우수한 성능을 보여주었습니다.</p>

<blockquote>
  <p>self attention –&gt; multi-head attention –&gt; 하나의 encoder</p>
</blockquote>

<p>Transformer 개념을 숙지했으니, 이제 <a href="https://hchoi256.github.io/nlp/bert-3/"><strong>BERT 언어 모델</strong></a>에 대해 알아보자.</p>
:ET