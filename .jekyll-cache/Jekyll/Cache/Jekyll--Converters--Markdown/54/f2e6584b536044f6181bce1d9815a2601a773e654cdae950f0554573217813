I"‚<h1 id="code">Code</h1>
<p class="notice--danger"><strong>[Notice]</strong> <a href="https://github.com/hchoi256/ai-workspace/blob/main/codes/MNIST-with-tensorflow.ipynb">download here</a></p>

<h1 id="observing-the-dataset">Observing the dataset</h1>
<p>MNIST ë°ì´í„°ì…‹ì—ëŠ” 0ë¶€í„° 9ê¹Œì§€ 10ì¢…ë¥˜ì˜ ìˆ«ìê°€ ì—¬ëŸ¬ 2ì°¨ì› ì´ë¯¸ì§€ ë°ì´í„° í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, RGB ì°¨ì›ì€ ë”°ë¡œ ì—†ëŠ” í‘ë°±ì‚¬ì§„ì´ë‹¤. <span style="color: blue">In the MNIST dataset, 10 types of numbers from 0 to 9 are stored in the form of several two-dimensional image data, and it is a black-and-white photograph without 3 RGB dimensions. </span></p>

<p>ê° ì´ë¯¸ì§€ í•´ìƒë„ëŠ” 28x28 í”½ì…€ ë‹¨ìœ„ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, í•™ìŠµ ë°ì´í„° 6ë§Œì¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° 1ë§Œì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. <span style="color: blue"> Each image resolution consists of 28x28 pixel units, and consists of 60,000 training data sheets and 10,000 test data sheets. </span></p>

<p>í•´ë‹¹ ë°ì´íŠ¸ì…‹ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤! <span style="color: blue"> It aims to train a model that performs image classification using the data set!</span></p>

<h1 id="loading-the-dataset">Loading the dataset</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="p">((</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="c1"># normalization
</span>
<span class="c1"># reshape for image generator
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># print(X_train.shape, X_test.shape)
# print(y_train.shape, y_test.shape)
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    [5 0 4 ... 5 6 8]
</code></pre></div></div>

<p>ìƒê¸° ì¢…ì†ë³€ìˆ˜ ê²°ê³¼ëŠ” ê° í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. <span style="color: blue"> The result of the dependent variable indicates an index corresponding to each class.</span></p>

<p>ê°€ë ¹, ë§¨ ì•ì˜ 5ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•˜ì—¬ ìˆ«ì 5ì¼ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²°ê³¼ê°’ì¼ ê²ƒì´ë‹¤. <span style="color: blue"> For example, the first 5 will be the result value that the model predicts to be the number 5 for new data. </span></p>

<p>ë˜í•œ, ìš°ë¦¬ëŠ” ê° í”½ì…€ë§ˆë‹¤ [0, 255] ìƒ‰ìƒê°’ì´ í• ë‹¹ëœ ì´ë¯¸ì§€ ë°ì´í„°ì— ëŒ€í•˜ì—¬ <strong>ì •ê·œí™”</strong>(normalization)ë¥¼ ìˆ˜í–‰í•˜ì—¬ <em>í•™ìŠµ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ</em> í•œë‹¤. <span style="color: blue"> In addition, we perform <strong>normalization</strong> on the image data to which the [0, 255] color value is assigned to each pixel to <em>fasten the learning speed</em>. </span></p>

<p>ê·¸ë¦¬ê³ , ìš°ë¦¬ëŠ” <strong>ì´ë¯¸ì§€ ì¦ê°•(image augmentation)</strong> ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ <em>í•™ìŠµ ë°ì´í„° ìˆ˜ë¥¼ ëŠ˜ë ¤</em> ë”ìš± ì •í™•í•œ ëª¨ë¸ ì„±ëŠ¥ì„ ëŒì–´ë‚´ê³ ì í•œë‹¤. <span style="color: blue"> And, we use <strong>image augmentation</strong> to <em>increase the number of training data</em> to get more accurate model performance. </span></p>

<p>ì´ë¥¼ ìœ„í•´, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì“°ì„ì— ë§ê²Œ reshape í•œë‹¤. <span style="color: blue"> To do this, the given data is reshaped for use. </span></p>

<p>ì´ê²ƒì€ RGB ì—´ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…ìœ¼ë¡œ, í‘ë°±ì‚¬ì§„ì´ê¸° ë•Œë¬¸ì— 3ì´ ì•„ë‹Œ 1ì„ í• ë‹¹í•œ ëª¨ìŠµì´ë‹¤. <span style="color: blue"> This is an operation to add an RGB column, and since it is a black-and-white photo, 1 is assigned instead of 3. </span></p>

<h1 id="visualizing-the-dataset">Visualizing the dataset</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">figure</span><span class="p">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">axes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">figure</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">matshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/182051405-8180e3cd-c511-438e-9f85-d1cb10132788.png" alt="image" /></p>

<p>ìƒê¸° ì´ë¯¸ì§€ ê²°ê³¼í‘œëŠ” MNIST ë°ì´í„°ì…‹ì—ì„œ ëœë¤í•˜ê²Œ 5ê°€ì§€ ìˆ«ì ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. <span style="color: blue">  The image result table shows five numerical images randomly from the MNIST dataset.</span></p>

<p>ì´ëŸ¬í•œ ì´ë¯¸ì§€ì™€ ê°™ì€ ìƒˆë¡œìš´ ë°ì´í„°ë“¤ì— ëŒ€í•œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ìˆ«ì ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” CNN ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ì! <span style="color: blue"> Letâ€™s build a CNN model that performs multi-class numeric classification on new data such as images! </span></p>

<h1 id="building-the-model">Building the model</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span> 
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">AveragePooling2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">BatchNormalization</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cnn_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">strides</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s">'same'</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="c1"># 3 hidden layers 
</span>    <span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
    <span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
    <span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="n">activation</span> <span class="o">=</span> <span class="s">'relu'</span><span class="p">))</span>

<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">'softmax'</span><span class="p">))</span> <span class="c1"># class: 10, multiclass --&gt; softmax
</span></code></pre></div></div>

<p>ë³¸ ê³¼ì œì—ì„œ ìš°ë¦¬ëŠ” ìˆœì°¨ì (â€˜Sequentialâ€™) CNNì„ êµ¬ì¶•í•œë‹¤: <span style="color: blue"> </span></p>
<ul>
  <li><em>Number of kernels</em>(= filters): 10 (3x3)</li>
  <li><em>Input_shape</em>: 28x28 (RGB = 1)</li>
  <li><em>Pooling</em>: 2x2
    <ul>
      <li>ì‹¤í—˜ ê²°ê³¼ <strong>MaxPooling</strong>ì´ AveragePoolingë³´ë‹¤ ë” ì¢‹ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. <span style="color: blue">In this task, we build a â€˜Sequentialâ€™ CNN. </span></li>
    </ul>
  </li>
  <li><em>Number of FCL(= hidden layers)</em>: 3 (# neurons=100, activation=relu)</li>
  <li><em>Output Layer</em>: softmax (multiclass classification task)</li>
  <li><em>BatchNormalization()</em>: <a href="https://github.com/hchoi256/ai-terms">internal covariate shift</a> ë¬¸ì œ í•´ê²°ì„ í†µí•œ <strong>ê³¼ì í•© ê°œì„ </strong> <span style="color: blue"><strong>Improved overfitting by solving the internal covariate shift problem</strong> </span></li>
</ul>

<p>FCL ì¸µ ê°œìˆ˜, ë‰´ëŸ° ê°œìˆ˜ì™€ ê°™ì€ í”¼ë¼ë¯¸í„°ëŠ” <strong>ììœ¨ì ìœ¼ë¡œ</strong> ì‘ì„±í•œë‹¤. <span style="color: blue"> Parameters such as the number of FCL layers and the number of neurons are created <strong>autonomously</strong>. </span></p>

<p>í”¼ë¼ë¯¸í„° ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ ì†ì‹¤í•¨ìˆ˜ ì‹œê°í™”ë¥¼ ê´€ì°°í•˜ì—¬ <strong>ìœ ì˜ë¯¸í•œ ê°ì†Œ</strong>ë¥¼ ë³´ì´ì§€ ì•Šê¸° ì‹œì‘í•˜ëŠ” ì§€ì ì—ì„œì˜ ìˆ˜ì¹˜ë¡œ í”¼ë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ë‹¤. <span style="color: blue"> By observing the visualization of the model loss function according to the parameter change, it is desirable to set the parameter to a value at the point where it does not show a significant decrease. </span></p>

<p>ì´ ë¶€ë¶„ì— ëŒ€í•œ ë³´ë‹¤ ìì„¸í•œ ë‚´ìš©ì€ ë³¸ ê³¼ì œì—ì„œëŠ” ìƒëµí•œë‹¤(<strong>TBD</strong>). <span style="color: blue"> Further details on this part will be omitted from this project (<strong>TBD</strong>). </span></p>

<h1 id="training-the-model">Training the model</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cnn_model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stop</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Epoch 1/10
    1200/1200 [==============================] - 16s 12ms/step - loss: 0.1787 - accuracy: 0.9496 - val_loss: 0.0658 - val_accuracy: 0.9794
    Epoch 2/10
    1200/1200 [==============================] - 14s 11ms/step - loss: 0.0674 - accuracy: 0.9797 - val_loss: 0.0839 - val_accuracy: 0.9747
    ...
    Epoch 9/10
    1200/1200 [==============================] - 15s 12ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0481 - val_accuracy: 0.9860
    Epoch 10/10
    1200/1200 [==============================] - 14s 12ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0518 - val_accuracy: 0.9866
</code></pre></div></div>

<p>ì, <del>ê°„ì‹ íƒ€ì„</del> ëª¨ë¸ í›ˆë ¨ì˜ ì‹œê°„ì´ë‹¤.</p>

<p>ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆê¸°ì— <a href="https://github.com/hchoi256/ai-terms/blob/main/entropy.md">â€˜<strong>í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼</strong>â€™</a>ë¥¼ ì†ì‹¤í•¨ìˆ˜ë¡œ, ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” â€˜<strong>Adam</strong>â€˜ì„ ì˜µí‹°ë§ˆì´ì €ë¡œ, ê·¸ë¦¬ê³  ëª¨ë¸ ì„±ëŠ¥ ì²™ë„ëŠ” â€˜<strong>ì •í™•ë„</strong>â€˜ë¡œ ì„¤ì •í–ˆë‹¤. <span style="color: blue"> Since we are dealing with a classification problem, we set â€˜<strong>Cross Entropy</strong>â€™ as the loss function, â€˜<strong>Adam</strong>â€™ with the best performance as the optimizer, and â€˜<strong>Accuracy</strong>â€™ as the model performance measure. </span></p>
<ul>
  <li><em>verbose</em>: outputì— ì •í™•ë„, validation ë° test ì •í™•ë„ë¥¼ í‘œí˜„í•œë‹¤. <span style="color: blue"> Expresses accuracy, validation, and test accuracy in the output.</span></li>
  <li><em>epoch</em>: ëª¨ë¸ í•™ìŠµì˜ ìˆœ/ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì‹ ê²½ë§ ì „ì²´ë¥¼ ëª‡ ë²ˆ í†µê³¼í• ì§€ ê²°ì •í•œë‹¤. <span style="color: blue"> Determines how many times to pass through the entire neural network in the forward/backward propagation process of model training. </span></li>
  <li><em>batch_size</em>: í•œ epochì— ëŒ€í•˜ì—¬ í•™ìŠµí•  ë°ì´í„° í¬ê¸°, ì¦‰ mini batch í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤. <span style="color: blue"> Determines the size of the data to be trained for one epoch, that is, the mini batch size. </span></li>
  <li><em>validation_data</em>: ëª¨ë¸ ì˜ˆì¸¡ì„±ëŠ¥ í‰ê°€ì— í™œìš©í•  ê²€ì¦ ë°ì´í„°ì´ë‹¤. <span style="color: blue">  verification data to be used for model prediction performance evaluation.</span></li>
</ul>

<p>â€˜<strong>Early_stop</strong>â€˜ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµê³¼ì •ì—ì„œ ì„±ëŠ¥ ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ìë™ìœ¼ë¡œ ë©ˆì¶”ëŠ” ì²˜ë¦¬ë¥¼ ì¶”ê°€í–ˆë‹¤. <span style="color: blue"> By using â€˜<strong>Early_stop</strong>â€™, we added a process that automatically stops learning if there is no performance change in the model training process. </span></p>
<ul>
  <li><em>patience</em>: ëª‡ ë²ˆì˜ ì„±ëŠ¥ ë¬´ë³€í™”ë¥¼ ìš©ì¸í•´ì¤„ê±´ì§€ ì„¤ì •í•œë‹¤. <span style="color: blue"> Sets how many times of no performance change to tolerate. </span></li>
</ul>

<p>ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ì…‹ ì •í™•ë„ëŠ” <strong>99.32%</strong>, ê²€ì¦ ë°ì´í„°ì˜ ì •í™•ë„ëŠ” <strong>98.66%</strong>ë¡œ ë„ì¶œë˜ì—ˆë‹¤. <span style="color: blue"> In this process, the accuracy of the model (test set) was <strong>99.32%</strong>, and the accuracy of the model (validation data) was <strong>98.66%</strong>. </span></p>

<p>ì°¸ê³ ë¡œ, <strong>BatchNormalization()</strong>ì´ ì—†ì„ ê²½ìš°, ì •í™•ë„ëŠ” 99%ê°€ ë‚˜ì˜¤ì§€ ëª»í–ˆê³ , í•™ìŠµ ì†ë„ê°€ ëˆˆì— ë„ê²Œ ë”ë”˜ ê²ƒì´ í™•ì¸ë˜ì—ˆë‹¤. <span style="color: blue"> For reference, in the absence of <strong>BatchNormalization()</strong>, the accuracy did not come out to 99%, and it was confirmed that the learning speed was noticeably slow. </span></p>

<h1 id="training-the-model-with-image-augmentation">Training the model with image augmentation</h1>
<p><em>íˆë”©í¬(Hiddink)</em>, â€œIâ€™m still hungry.â€</p>

<p>ì‚¬ëŒì˜ ìš•ì‹¬ì€ ëì´ì—†ë‹¤. <span style="color: blue"> There is no end to human greed.</span></p>

<p>ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ê·¸ì¹˜ì§€ ì•Šê³  ëª¨ë¸ì˜ ì„±ëŠ¥ ë” ëŒì–´ë‚´ê³  ì‹¶ë‹¤. <span style="color: blue">We donâ€™t want to stop there, but we want to bring out the performance of the model further. </span></p>

<p>ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ, ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê¸°ì¡´ ì¸í’‹ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ë³€í™”ë¥¼ ì£¼ì–´ <strong>ë°ì´í„° ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤.</strong> <span style="color: blue"> To do so, by using image augmentation techniques, various changes are made to the existing input image to <strong>increase the number of data.</strong></span></p>

<p><strong>í•™ìŠµ ë°ì´í„°ì˜ ì¦ê°€</strong>ëŠ” ëª¨ë¸ì˜ <strong>ê³¼ì í•©ì„ ë°©ì§€</strong>í•˜ì—¬ ë” ë†’ì€ ì˜ˆì¸¡ì„±ëŠ¥ì„ ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤. <span style="color: blue"> <strong>Increasing training data</strong> can lead to higher predictive performance by avoiding overfitting the model.</span></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>

<span class="n">datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
                            <span class="n">rotation_range</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                            <span class="c1">#width_shift_range = 0.1,
</span>                            <span class="c1">#horizontal_flip = True,
</span>                            <span class="c1">#vertical_flip = True,
</span>                            <span class="c1">#brightness_range=(.9, 1)
</span>                            <span class="c1">#zoom_range = 0.7,
</span>                            <span class="c1">#height_shift_range=0
</span>                             <span class="p">)</span>

<span class="n">datagen</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Creating the image flow with the training dataset
# Loading the certain number of adjusted images
</span><span class="k">for</span> <span class="n">x_batch</span> <span class="ow">in</span> <span class="n">datagen</span><span class="p">.</span><span class="n">flow</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">n</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
     <span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">'Augmented images (rotated 90 degrees)'</span><span class="p">)</span>
     <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
     <span class="k">break</span><span class="p">;</span>

<span class="n">cnn_model</span><span class="p">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">datagen</span><span class="p">.</span><span class="n">flow</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span><span class="p">),</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># fit_generator() when using data made by image generator
</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Test accuracy'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
 
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/182052312-f21c4327-4c66-4203-b6be-15e2e1e8d67d.png" alt="image" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Epoch 1/10
    1200/1200 [==============================] - 20s 16ms/step - loss: 0.1346 - accuracy: 0.9613
    Epoch 2/10
    1200/1200 [==============================] - 18s 15ms/step - loss: 0.0799 - accuracy: 0.9756
    ...
    Epoch 9/10
    1200/1200 [==============================] - 18s 15ms/step - loss: 0.0521 - accuracy: 0.9843
    Epoch 10/10
    1200/1200 [==============================] - 18s 15ms/step - loss: 0.0481 - accuracy: 0.9850
    313/313 [==============================] - 1s 3ms/step - loss: 0.0461 - accuracy: 0.9860
    Test accuracy 0.9860000014305115
</code></pre></div></div>

<p>ìƒê¸° ì´ë¯¸ì§€ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ì— 50ë„ ì•ˆíŒì˜ íšŒì „ë„ë¥¼ ë¶€ì—¬í•˜ì—¬ ì „í˜€ ë‹¤ë¥¸ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚¸ ê²°ê³¼ì´ë‹¤. <span style="color: blue"> The above image is the result of creating a completely different new image by giving a rotation of around 50 degrees to the existing image.</span></p>

<p>ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ì¸ 4ì™€ ê°™ì´ íšŒì „ì„ ë°›ì•„ ê¸°ìš¸ì–´ì§„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëª¨ë¸ì€ ì •í™•í•œ ì˜ˆì¸¡ì„ í•´ë‚´ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤. <span style="color: blue"> When new data is rotated and tilted as in the second image 4, it may be difficult for the model to make accurate predictions.</span></p>

<p>ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë³€ë³„ë ¥ìˆëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë” ëŒì–´ë‚¸ë‹¤. <span style="color: blue">We generate these discriminative data to further drive the performance of our models. </span></p>

<p>êµ¬í˜„ ê³¼ì •ì—ì„œ í•˜ë‚˜ íŠ¹ë³„í•œ ì ì€, ì´ë¯¸ì§€ ìƒì„±ê¸°ë¥¼ í†µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•  ê²½ìš° fit()ì´ ì•„ë‹Œ <strong>fit_generator()</strong> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. <span style="color: blue"> One special thing in the implementation process is that when training a model through an image generator, use the <strong>fit_generator()</strong> function rather than fit().</span></p>

<p>ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ 0.9860000014305115, ì¦‰ <strong>98.6%</strong>ë¡œ ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤. <span style="color: blue"> Finally, the performance of the model is 0.9860000014305115, that is, <strong>98.6%</strong>, which shows a very high accuracy.</span></p>

<p>ì, ì´ì œ ì§ê´€ì ì¸ ì‹œê°í™”ë¡œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•˜ê³ , í˜¼ë™ í–‰ë ¬ë¡œ í‘œí˜„í•˜ì—¬ ëª¨ë¸ í‰ê°€ë¥¼ ì§„í–‰í•´ë³´ì. <span style="color: blue"> Now, letâ€™s compare the actual value and the predicted value with an intuitive visualization and evaluate the model by expressing it as a confusion matrix.</span></p>

<h1 id="evaluating-the-model">Evaluating the model</h1>

<h2 id="comparison-actual-vs-predicted">Comparison: Actual vs. Predicted</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">evaluation</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">predicted_classes</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">predicted_classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predicted_classes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">L</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">W</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">L</span><span class="o">*</span><span class="n">W</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Prediction = {}</span><span class="se">\n</span><span class="s"> True = {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">predicted_classes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/182052768-75e7abd5-4c6e-462d-a2dd-83cba24cb115.png" alt="image" /></p>

<p>ëœë¤í•˜ê²Œ ì„ íƒëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë³´ì—¬ì£¼ëŠ” í•´ë‹¹ ê²°ê³¼í‘œì—ì„œ â€˜ì˜¤ë¶„ë¥˜â€™ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒì„ ë°œê²¬í–ˆë‹¤. <span style="color: blue"> We found that there was no â€˜misclassificationâ€™ in the corresponding result table showing the actual and predicted values â€‹â€‹for the randomly selected test data.</span></p>

<p>ì´ê²ƒì€ 99%ì— ìœ¡ë°•í•˜ëŠ” ìš°ë¦¬ ëª¨ë¸ì˜ ê°•ë ¥í•œ ì„±ëŠ¥ì—ì„œ ë¹„ë¡¯ëœ ê²°ê³¼ì¼ ê²ƒì´ë‹¤. <span style="color: blue">This is probably the result of the strong performance of our model, which is close to 99%. </span></p>

<p>í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ì˜¤ë¶„ë¥˜ì˜ ê°œìˆ˜ ì—­ì‹œ í•œ ëˆˆì— íŒŒì•…í•˜ê³  ì‹¶ë‹¤. <span style="color: blue">However, we also want to know the number of misclassifications at a glance. </span></p>

<p>ì´ëŸ´ ë•Œ, í˜¼ë™ í–‰ë ¬ë¡œì¨ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•˜ë©´ ë§¤ìš° ìœ ìš©í•˜ë‹¤. <span style="color: blue">In this case, it is very useful to perform performance evaluation with a confusion matrix. </span></p>

<h2 id="confusion-matrix">Confusion matrix</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predicted_classes</span><span class="p">)</span>
<span class="n">cm</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/39285147/182052904-2de7452e-58dd-4de5-b76e-6138b130ced2.png" alt="image" /></p>

<p>ìš°ë¦¬ëŠ” ì†ì‰½ê²Œ í˜¼ë™í–‰ë ¬ë¥¼ êµ¬í˜„í•˜ê³ ì <strong>â€˜ì”¨ë³¸(seaborn)â€™</strong> ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œë‹¤. <span style="color: blue">We use the <strong>â€˜seabornâ€™</strong> library to easily implement the confusion matrix. </span></p>

<p>ë‹¨ ëª‡ ì¤„ë§Œìœ¼ë¡œ í˜¼ë™ í–‰ë ¬ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë‹¤. <span style="color: blue"> A confusion matrix can be implemented with just a few lines.</span></p>

<p>ë§ì€ ë°ì´í„°ê°€ í¬í•¨ë˜ìˆì„ ìˆ˜ë¡ ë°ì€ ìƒ‰ì— ê°€ê¹Œìš´ í”½ì…€ê°’ì„ ê°€ì§„ë‹¤. <span style="color: blue"> The more data is included, the closer the pixel value is to a bright color.</span></p>

<p>ì§ê´€ì ìœ¼ë¡œë„ ëŒ€ê°ì„ , ì¦‰ ì°¸ë¶„ë¥˜ (TP, TN) ê°œìˆ˜ê°€ ì••ë„ì ì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆê³ , ê·¸ ì™¸ ê²€ì •ìƒ‰ì— ê·¼ì‚¬í•œ í”½ì…€ì— ì íŒ ìˆ˜ì¹˜ë“¤ì€ ëª¨ë‘ ì˜¤ë¶„ë¥˜ì´ë‹¤. <span style="color: blue"> Intuitively, it can be seen that the diagonal, the number of true classifications (TP, TN), is overwhelming, and all other figures written in pixels that are close to black are misclassifications.</span></p>

<p>7ë§Œ ê°œì˜ ê¸°ì¡´ ì´ë¯¸ì§€ ë°ì´í„°ì™€ â€˜ì´ë¯¸ì§€ ì¦ê°•â€™ì„ í†µí•´ ì¶”ê°€ëœ ìƒˆë¡œìš´ ì´ë¯¸ì§€ 7ë§Œ ì¥, ë„í•© 14ë§Œì¥ì˜ ì¸í’‹ì„ í•™ìŠµí•œ ëª¨ë¸ì´ ë‚´ë†“ì€ ì˜¤ë¶„ë¥˜ ì¹˜ê³  êµ‰ì¥íˆ ë‚®ì€ ì˜¤ë¶„ë¥˜ ê°œìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <span style="color: blue"> You can see a very low number of misclassifications, considering the input of 140,000 in total; 70,000 existing image data and 70,000 new images added through â€˜image augmentationâ€™. </span></p>
:ET