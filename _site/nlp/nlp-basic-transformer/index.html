<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>NLP - Part 6: Transformer | 정리하여 내 것으로, AI</title>
<meta name="description" content="Transformer w/ Tensorflow 번역, 질문 답변, 텍스트 요약 등과 같은 작업을 위해 순차적으로 데이터를 처리하도록 설계된 2017년에 구글이 소개한 딥러닝 아키텍처이다.  It is a deep learning architecture introduced by Google in 2017 designed to process data sequentially for tasks such as translation, question answering, text summarization, and more.">


  <meta name="author" content="Hojun Eric Choi">
  
  <meta property="article:author" content="Hojun Eric Choi">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="정리하여 내 것으로, AI">
<meta property="og:title" content="NLP - Part 6: Transformer">
<meta property="og:url" content="http://localhost:4000/nlp/nlp-basic-transformer/">


  <meta property="og:description" content="Transformer w/ Tensorflow 번역, 질문 답변, 텍스트 요약 등과 같은 작업을 위해 순차적으로 데이터를 처리하도록 설계된 2017년에 구글이 소개한 딥러닝 아키텍처이다.  It is a deep learning architecture introduced by Google in 2017 designed to process data sequentially for tasks such as translation, question answering, text summarization, and more.">



  <meta property="og:image" content="http://localhost:4000/assets/images/posts/nlp-thumbnail.jpg">





  <meta property="article:published_time" content="2022-07-29T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/nlp/nlp-basic-transformer/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Hojun Eric Choi",
      "url": "http://localhost:4000/"
    
  }
</script>






  <meta name="naver-site-verification" content="57d020c8f9b26bf56ed7846608b8d873481e9b84">


<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="정리하여 내 것으로, AI Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/logo.ico/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/logo.ico/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/logo.ico/favicon-16x16.png">
<link rel="mask-icon" href="/assets/logo.ico/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          정리하여 내 것으로, AI
          <span class="site-subtitle">with jjuns</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tag</a>
            </li><li class="masthead__menu-item">
              <a href="/search/">Search</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/categories/#nlp" itemprop="item"><span itemprop="name">Nlp</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">NLP - Part 6: Transformer</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/echoi.jpg" alt="Hojun Eric Choi" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Hojun Eric Choi</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>I am a senior-year B.S. student in Computer Science and Data Science at the University of Wisconsin, Madison.</p>

<p>My research interests are <strong>ML</strong>, <strong>deep NLP</strong>, and <strong>computer vision</strong> for emerging application domains.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Madison</span>
        </li>
      

      
        
          
            <li><a href="https://www.linkedin.com/in/hojun-choi-2b10b11a0/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://github.com/hchoi256" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="/assets/CV.pdf" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">CV</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:hchoi256@wisc.edu" rel="me" class="u-email">
            <meta itemprop="email" content="hchoi256@wisc.edu" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">토글 메뉴</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Table of Contents</span>
        

        
        <ul>
          
            <li><a href="/categories/">Category</a></li>
          
            <li><a href="/tags/">Tag</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="NLP - Part 6: Transformer">
    <meta itemprop="description" content="Transformer w/ Tensorflow번역, 질문 답변, 텍스트 요약 등과 같은 작업을 위해 순차적으로 데이터를 처리하도록 설계된 2017년에 구글이 소개한 딥러닝 아키텍처이다.  It is a deep learning architecture introduced by Google in 2017 designed to process data sequentially for tasks such as translation, question answering, text summarization, and more.">
    <meta itemprop="datePublished" content="2022-07-29T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/nlp/nlp-basic-transformer/" class="u-url" itemprop="url">NLP - Part 6: Transformer
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-07-29T00:00:00+09:00">2022-07-29</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 분 소요
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> GITHUB BLOG JJUNS</h4></header>
              <ul class="toc__menu"><li><a href="#transformer-w-tensorflow">Transformer w/ Tensorflow</a></li><li><a href="#gpt-2-generative-pre-training-transformer-2">GPT-2 (Generative Pre-training Transformer 2)</a><ul><li><a href="#loading-the-libraries">Loading the libraries</a></li><li><a href="#encoding">Encoding</a></li><li><a href="#decoding">Decoding</a></li></ul></li><li><a href="#bert">BERT</a></li></ul>

            </nav>
          </aside>
        
        <h1 id="transformer-w-tensorflow">Transformer w/ Tensorflow</h1>
<p><strong>번역, 질문 답변, 텍스트 요약</strong> 등과 같은 작업을 위해 <strong>순차적으로</strong> 데이터를 처리하도록 설계된 2017년에 구글이 소개한 딥러닝 아키텍처이다. <span style="color: blue"> It is a deep learning architecture introduced by Google in 2017 designed to process data sequentially for tasks such as <strong>translation, question answering, text summarization</strong>, and more.</span></p>

<p>텍스트 생성기로 <em>GPT-2, GPT-3</em> 등이 있는데, <em>GPT-2</em>의 텍스트 생성 기능은 대부분이 사용할 수 있는 가장 인기 있는 transformer 아키텍처 중에 하나이다. <span style="color: blue"> As text generators, there are <em>GPT-2</em> and <em>GPT-3</em>, and the text generation function of <em>GPT-2</em> is one of the most popular transformer architectures that most of them can use. </span></p>

<blockquote>
  <p>단어들은 불연속적 단위이기 때문에 각각에 대힌 확률값을 얻을 수 있다. 여기서, ‘언어 모델’은 단어 시퀀스에 확률을 할당하는 모델입니다. <span style="color: blue"> Since words are discrete units, we can obtain a probability value for each. Here, a ‘language model’ is a model that assigns probabilities to word sequences.</span></p>
</blockquote>

<h1 id="gpt-2-generative-pre-training-transformer-2">GPT-2 (Generative Pre-training Transformer 2)</h1>
<p>WebText라 불리는 40GB 크기의 거대한 코퍼스에다가 인터넷에서 크롤링한 데이터를 합쳐서 훈련시킨 자귀 회귀 언어 모델이며, 이 모델은 디코더 스택만 사용하는 Attention를 활용한다 (BERT는 인코더 스택만 사용한다). <span style="color: blue"> It is a <em>*auto-regressive</em> language model trained by combining data crawled from the Internet in a huge 40GB corpus called WebText, and this language model utilizes Attention using only the decoder stack (BERT uses only the encoder stack).</span></p>
<ul>
  <li><strong>Generative</strong>: 한 단어(토큰)가 들어오면 다음에 올 적절한 토큰을 생성하는 언어 모델이다. <span style="color: blue"> It is a language model that, when a word (token) comes in, generates the appropriate token to follow.</span>
    <ul>
      <li>예를들어, “오늘” 이라는 단어가 GPT 모델에 Input으로 들어가면, GPT는 “날씨가”  같은 뒤에 올 적절한 단어를 Output으로 내보낸다. <span style="color: blue"> For example, if the word “today” is input to the GPT model, GPT outputs the appropriate word followed by something like “weather” as output.</span></li>
    </ul>
  </li>
  <li><strong>Pre-trained</strong>: 말뭉치 (Corpus) 만을 가지고 <em>사전 학습</em>한다. <span style="color: blue"> <em>pre-learning</em> with only the corpus.</span>
    <ul>
      <li>Encoding 과 Decoding 의 과정이 필요하지 않는다. <span style="color: blue"> Encoding and decoding processes are not required.</span></li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>자기회귀 모델(auto-regressive model)</strong>: 이전의 출력이 다음의 입력이 되는 모델을 의미한다. <span style="color: blue">  A model in which the previous output becomes the next input.</span></p>
</blockquote>

<blockquote>
  <p><a href="https://hyyoka-ling-nlp.tistory.com/9">GPT-2</a></p>
</blockquote>

<blockquote>
  <p><a href="https://hyyoka-ling-nlp.tistory.com/8">BERT vs. GPT-2</a></p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="c1"># 특정 모델을 쉽게 다운로드해서 사용할 수 있다! can easily download and use specific models!
</span></code></pre></div></div>

<h2 id="loading-the-libraries">Loading the libraries</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFGPT2LMHeadModel</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2-large"</span><span class="p">)</span>
<span class="n">GPT2</span> <span class="o">=</span> <span class="n">TFGPT2LMHeadModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2-large"</span><span class="p">)</span> <span class="c1"># 문장 생성 sentence generation
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">GPT2</span> <span class="o">=</span> <span class="n">TFGPT2LMHeadModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2-large"</span><span class="p">,</span> <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span><span class="p">)</span> <span class="c1"># EOS토큰을 PAD토큰으로 지정하여 warning이 나오지 않게 한다 Designating EOS tokens as PAD tokens to avoid warnings
</span></code></pre></div></div>

<p>상기 코드에서 보는 것처럼 GPT는 사전 훈련 기반 모델이며, fine-tuning을 거치지 않는다. <span style="color: blue"> As shown in the code above, GPT is a pre-training-based model and does not undergo fine-tuning.</span></p>

<blockquote>
  <p><strong>eos_token_id</strong>: The end of sequence token.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SEED</span> <span class="o">=</span> <span class="mi">34</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="encoding">Encoding</h2>
<p>input_sequence — (encode) —&gt; tensor — (decode) —&gt; greedy_output</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_LEN</span> <span class="o">=</span> <span class="mi">70</span> 
<span class="n">input_sequence</span> <span class="o">=</span> <span class="s">"There are times when we are really tired of people but we feel lonely too"</span> <span class="c1"># input sample
</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"tf"</span><span class="p">)</span> <span class="c1"># encoding with tensor as output
</span><span class="n">input_ids</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    &lt;tf.Tensor: shape=(1, 15), dtype=int32, numpy=
    array([[ 1858,   389,  1661,   618,   356,   389,  1107, 10032,   286,
            661,   475,   356,  1254, 21757,  1165]])&gt;
</code></pre></div></div>

<h2 id="decoding">Decoding</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_outputs</span> <span class="o">=</span> <span class="n">GPT2</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="n">MAX_LEN</span><span class="p">,</span> 
<span class="n">do_sample</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">top_k</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span> <span class="o">=</span> <span class="mf">0.85</span><span class="p">,</span> <span class="n">num_return_sequences</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>generate()</strong></p>
<ul>
  <li><em>max_length</em>: 출력 문자열이 가질 수 있는 단어의 최대 갯수 <span style="color: blue"> Max number of words the output string can have</span></li>
  <li><em>do_sample</em>: activate sampling</li>
  <li><em>top_k</em>: sampling only from the most likely k words</li>
  <li><em>top_p</em>: Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p</li>
  <li><em>num_return_sequences</em>: 출력 개수 <span style="color: blue"> # outputs</span></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Output:</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s">"-"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">beam_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_outputs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{}: {}."</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">beam_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Output:
    ----------------------------------------------------------------------------------------------------
    0: There are times when we are really tired of people but we feel lonely too. There are times when we're not sure what to do and want to leave but there is a lot of pressure. There are times when we're frustrated and don't know what to do. There are times when we don't know if we're right or wrong. There.
    1: There are times when we are really tired of people but we feel lonely too," he said.

    He has no doubt that the Chinese government will try to silence the opposition in any way possible.

    "The Chinese government is going to try to silence us with the use of the law, that's my feeling," he said.

    .
    2: There are times when we are really tired of people but we feel lonely too, which makes me think that our loneliness is a normal part of life," he said. "I think it's the fact that we live in a country where everyone can easily get into the spotlight or be in the news."

    Szewczyk said he found.
    3: There are times when we are really tired of people but we feel lonely too. In those times we can get along well together. If we are going to work, we can work together, if we are going out to do things, we can do things together. We can work together to find some peace, some happiness in our lives, and it.
    4: There are times when we are really tired of people but we feel lonely too.

    The first thing that happens is when you're in your 20s, I would think to myself, "I just want to go to the pub".

    I'll walk to the train station and I'll just think, "I want to go to the.
</code></pre></div></div>

<p>상기 결과에서 볼 수 있듯이, 하나의 문장 인풋으로 GPT-2가 유사도가 높은 여러 개의 문장을 생성해냈다. <span style="color: blue"> As can be seen from the above results, with one sentence input, GPT-2 generated several sentences with high similarity.</span></p>

<p>생성된 각 문장은 우리가 지정한 아키텍쳐과 맞아 떨어진다; 문장 길이 최대 70자 이내. <span style="color: blue"> Each generated statement matches the architecture we specified; Sentence length up to 70 characters.</span></p>

<p>우리 아키텍쳐가 지침하는대로, 이러한 문장들은 인풋 텍스트와 유사도가 가장 높은 문장으로 선별한 50개 중 최대 5개를 추출하여 보여준다. <span style="color: blue"> As our architecture guides, these sentences extract and display up to 5 out of 50 sentences with the highest similarity to the input text.</span></p>

<blockquote>
  <p><strong>Beam Search</strong>: 매번 선택하는 단어의 갯수로, 선택은 확률 값이 높은 순서대로 한다. num_beams가 2인 경우, 다음 2가지 확률값이 높은 단어에 대해서 탐색한다. 단어 생성에서 가능성이 더 높은 다음 예측 단어를 놓치는 Greedy 방식의 단점을 보완하고자 고안되었다. <span style="color: blue"> With the number of words selected each time, the selection is made in order of highest probability value. When num_beams is 2, the following two high probability words are searched for. It was designed to compensate for the disadvantage of the Greedy method, which misses the next more likely predictive word in word generation.</span></p>
</blockquote>

<h1 id="bert"><a href="https://hchoi256.github.io/nlp/bert-1/">BERT</a></h1>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a><span class="sep">, </span>
    
      <a href="/tags/#python" class="page__taxonomy-item p-category" rel="tag">python</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2022-07-29T00:00:00+09:00">2022-07-29</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=NLP+-+Part+6%3A+Transformer%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fnlp-basic-transformer%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fnlp-basic-transformer%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2Fnlp-basic-transformer%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/nlp/nlp-basic-topic-modeling/" class="pagination--pager" title="NLP - Part 5: Topic Modeling
">이전</a>
    
    
      <a href="/star/ai-dissertations/" class="pagination--pager" title="AI Research Papers
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/teeko.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml/ml-teeko-minimax/" rel="permalink">ML: Minimax 알고리즘 - Teeko Game
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-14T00:00:00+09:00">2022-08-14</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          12 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Minimax 알고리즘을 활용해서 Teeko 게임을 구현한다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/pca-image-compression.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml/ml-pca/" rel="permalink">ML: PCA - Image Compression(이미지 압축)
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-14T00:00:00+09:00">2022-08-14</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          2 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">PCA를 이용해서 이미지 압축을 진행해보자.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/hac.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml/ml-hca/" rel="permalink">ML: Hierarchical Agglomerate Clustering(HAC) - 포켓몬 군집화
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-14T00:00:00+09:00">2022-08-14</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Hierarchical Agglomerate Clustering(HAC) 이용해서 서로 다른 특성을 공유하는 Pokemon들을 군집으로 묶어보자.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/8-tile-puzzle.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml/ml-8-tile-puzzle-a-star-algorithm/" rel="permalink">ML: A* Search - 8-tile Puzzle Game
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-08-14T00:00:00+09:00">2022-08-14</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          최대 1 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A-star 알고리즘을 활용해서 8-tile Puzzle 게임을 구현한다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
          <li><a href="mailto:hchoi256@wisc.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/hojun-choi-2b10b11a0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://github.com/hchoi256" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="/assets/CV.pdf" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> CV</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Hojun Eric Choi. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/nlp/nlp-basic-transformer/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/nlp/nlp-basic-transformer"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://jjunes.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
